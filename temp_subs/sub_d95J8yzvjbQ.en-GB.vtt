WEBVTT
Kind: captions
Language: en-GB

00:00:08.042 --> 00:00:10.044
(RUMINATIVE MUSIC PLAYING)

00:00:29.105 --> 00:00:30.148
(DEVICE CHIMES)

00:00:30.231 --> 00:00:31.232
JULIETTE LOVE: Hi, Alpha.  

00:00:32.316 --> 00:00:33.317
ALPHA: &lt;i&gt;Hello.&lt;/i&gt;

00:00:33.818 --> 00:00:34.819
LOVE: Can you
help me write code?

00:00:37.113 --> 00:00:38.656
ALPHA: &lt;i&gt;I was trained&lt;/i&gt;
&lt;i&gt;to answer questions,&lt;/i&gt;

00:00:40.199 --> 00:00:41.826
&lt;i&gt;but I'm able to learn.&lt;/i&gt;

00:00:43.369 --> 00:00:44.620
LOVE: That's very
open-minded of you.

00:00:45.621 --> 00:00:47.790
ALPHA: &lt;i&gt;Thank you.&lt;/i&gt;
&lt;i&gt;I'm glad you're happy with me.&lt;/i&gt;

00:00:49.792 --> 00:00:50.793
What's this guy doing?

00:00:54.088 --> 00:00:55.173
ALPHA: &lt;i&gt;That's a developer.&lt;/i&gt;

00:00:56.215 --> 00:00:57.342
What do you think
he's working on?

00:00:59.093 --> 00:01:00.595
ALPHA:
&lt;i&gt;That's a tough question.&lt;/i&gt;

00:01:00.678 --> 00:01:02.513
&lt;i&gt;He might be working&lt;/i&gt;
&lt;i&gt;on a new feature,&lt;/i&gt;

00:01:02.597 --> 00:01:04.724
&lt;i&gt;a bug fix or something else.&lt;/i&gt;

00:01:04.807 --> 00:01:05.808
It's quite possible.

00:01:05.892 --> 00:01:06.893
ALPHA: &lt;i&gt;Yes.&lt;/i&gt;

00:01:10.104 --> 00:01:11.147
LOVE: Do you see my backpack?

00:01:13.608 --> 00:01:14.984
ALPHA:
&lt;i&gt;That's a badminton racket.&lt;/i&gt;

00:01:15.068 --> 00:01:17.320
It's a squash racket,
but that's pretty close.

00:01:19.906 --> 00:01:21.324
ALPHA:
&lt;i&gt;That's a badminton racket.&lt;/i&gt;

00:01:21.407 --> 00:01:22.533
No, but you're not
the first person

00:01:22.617 --> 00:01:23.618
to make that mistake.

00:01:24.285 --> 00:01:26.245
(UPBEAT MUSIC PLAYING)

00:01:34.253 --> 00:01:35.546
NEWSREADER 1:
&lt;i&gt;AI, the technology&lt;/i&gt;

00:01:35.630 --> 00:01:38.091
&lt;i&gt;that has been advancing&lt;/i&gt;
&lt;i&gt;at breakneck speed.&lt;/i&gt;

00:01:38.174 --> 00:01:40.343
NEWSREADER 2: &lt;i&gt;Artificial&lt;/i&gt;
&lt;i&gt;intelligence is all the rage.&lt;/i&gt;

00:01:40.426 --> 00:01:42.011
NEWSREADER 3: &lt;i&gt;Some are now&lt;/i&gt;
&lt;i&gt;raising alarm about...&lt;/i&gt;

00:01:42.095 --> 00:01:43.471
NEWSREADER 4:
&lt;i&gt;It is definitely concerning.&lt;/i&gt;

00:01:43.554 --> 00:01:44.764
NEWSREADER 5:
&lt;i&gt;This is an AI arms race.&lt;/i&gt;

00:01:44.847 --> 00:01:45.848
NEWSREADER 6: &lt;i&gt;We don't know&lt;/i&gt;

00:01:45.932 --> 00:01:47.141
&lt;i&gt;how this is all&lt;/i&gt;
&lt;i&gt;going to shake out,&lt;/i&gt;

00:01:47.225 --> 00:01:48.351
&lt;i&gt;but it's clear&lt;/i&gt;
&lt;i&gt;something is happening.&lt;/i&gt;

00:01:53.398 --> 00:01:54.565
DEMIS HASSABIS:
&lt;i&gt;I'm kind of restless.&lt;/i&gt;

00:01:56.693 --> 00:01:59.737
&lt;i&gt;Trying to build AGI&lt;/i&gt;
&lt;i&gt;is the most exciting journey,&lt;/i&gt;

00:01:59.821 --> 00:02:01.739
&lt;i&gt;in my opinion, that humans&lt;/i&gt;
&lt;i&gt;have ever embarked on.&lt;/i&gt;

00:02:04.158 --> 00:02:05.743
&lt;i&gt;If you're really going&lt;/i&gt;
&lt;i&gt;to take that seriously,&lt;/i&gt;

00:02:05.827 --> 00:02:07.161
&lt;i&gt;there isn't a lot of time.&lt;/i&gt;

00:02:08.121 --> 00:02:09.455
&lt;i&gt;Life's very short.&lt;/i&gt;

00:02:12.041 --> 00:02:14.043
&lt;i&gt;My whole life goal is to solve&lt;/i&gt;

00:02:14.127 --> 00:02:15.712
&lt;i&gt;artificial&lt;/i&gt;
&lt;i&gt;general intelligence.&lt;/i&gt;

00:02:16.379 --> 00:02:19.799
&lt;i&gt;And on the way,&lt;/i&gt;
&lt;i&gt;use AI as the ultimate tool&lt;/i&gt;

00:02:19.882 --> 00:02:21.300
&lt;i&gt;to solve all the world's&lt;/i&gt;

00:02:21.384 --> 00:02:22.760
&lt;i&gt;most complex&lt;/i&gt;
&lt;i&gt;scientific problems.&lt;/i&gt;

00:02:25.179 --> 00:02:26.723
&lt;i&gt;I think that's bigger&lt;/i&gt;
&lt;i&gt;than the Internet.&lt;/i&gt;

00:02:26.806 --> 00:02:28.016
&lt;i&gt;I think that's bigger&lt;/i&gt;
&lt;i&gt;than mobile.&lt;/i&gt;

00:02:29.434 --> 00:02:30.727
&lt;i&gt;I think it's more like&lt;/i&gt;

00:02:30.810 --> 00:02:32.311
&lt;i&gt;the advent&lt;/i&gt;
&lt;i&gt;of electricity or fire.&lt;/i&gt;

00:02:45.241 --> 00:02:46.242
ANNOUNCER: &lt;i&gt;World leaders&lt;/i&gt;

00:02:46.325 --> 00:02:47.994
&lt;i&gt;and artificial&lt;/i&gt;
&lt;i&gt;intelligence experts&lt;/i&gt;

00:02:48.077 --> 00:02:49.537
&lt;i&gt;are gathering&lt;/i&gt;
&lt;i&gt;for the first ever&lt;/i&gt;

00:02:49.620 --> 00:02:51.622
&lt;i&gt;global AI safety summit,&lt;/i&gt;

00:02:52.373 --> 00:02:54.042
&lt;i&gt;set to look at the risks&lt;/i&gt;

00:02:54.125 --> 00:02:56.252
&lt;i&gt;of the fast growing technology&lt;/i&gt;
&lt;i&gt;and also...&lt;/i&gt;

00:02:56.336 --> 00:02:57.420
HASSABIS: &lt;i&gt;I think&lt;/i&gt;
&lt;i&gt;this is a hugely&lt;/i&gt;

00:02:57.503 --> 00:02:59.547
&lt;i&gt;critical moment&lt;/i&gt;
&lt;i&gt;for all humanity.&lt;/i&gt;

00:03:00.757 --> 00:03:03.092
It feels like
we're on the cusp

00:03:03.176 --> 00:03:05.303
of some incredible things
happening.

00:03:05.386 --> 00:03:06.429
NEWSREADER:
&lt;i&gt;Let me take you through&lt;/i&gt;

00:03:06.512 --> 00:03:07.847
&lt;i&gt;some of the reactions&lt;/i&gt;
&lt;i&gt;in today's papers.&lt;/i&gt;

00:03:07.930 --> 00:03:09.599
HASSABIS: &lt;i&gt;AGI is pretty close,&lt;/i&gt;
&lt;i&gt;I think.&lt;/i&gt;

00:03:09.682 --> 00:03:12.393
&lt;i&gt;There's clearly huge interest&lt;/i&gt;
&lt;i&gt;in what it is capable of,&lt;/i&gt;

00:03:12.477 --> 00:03:13.728
&lt;i&gt;where it's taking us.&lt;/i&gt;

00:03:14.437 --> 00:03:15.521
HASSABIS: &lt;i&gt;This is the moment&lt;/i&gt;

00:03:15.605 --> 00:03:17.106
&lt;i&gt;I've been living&lt;/i&gt;
&lt;i&gt;my whole life for.&lt;/i&gt;

00:03:17.607 --> 00:03:19.609
(MID-TEMPO
ELECTRONIC MUSIC PLAYS)

00:03:22.028 --> 00:03:23.905
&lt;i&gt;I've always been fascinated&lt;/i&gt;
&lt;i&gt;by the mind.&lt;/i&gt;

00:03:24.739 --> 00:03:27.533
&lt;i&gt;So I set my heart&lt;/i&gt;
&lt;i&gt;on studying neuroscience&lt;/i&gt;

00:03:27.617 --> 00:03:29.535
&lt;i&gt;because I wanted&lt;/i&gt;
&lt;i&gt;to get inspiration&lt;/i&gt;

00:03:29.619 --> 00:03:31.162
&lt;i&gt;from the brain for AI.&lt;/i&gt;

00:03:31.245 --> 00:03:32.747
ELEANOR MAGUIRE:
&lt;i&gt;I remember asking Demis,&lt;/i&gt;

00:03:32.830 --> 00:03:34.123
&lt;i&gt;"What's the end game?"&lt;/i&gt;

00:03:34.207 --> 00:03:35.792
You know?
So you're going to come here

00:03:35.875 --> 00:03:37.835
and you're going
to study neuroscience

00:03:37.919 --> 00:03:40.963
and you're going to maybe
get a Ph.D. if you work hard.

00:03:42.632 --> 00:03:43.758
And he said,

00:03:43.841 --> 00:03:46.511
&lt;i&gt;"You know, I want&lt;/i&gt;
&lt;i&gt;to be able to solve AI.&lt;/i&gt;

00:03:46.594 --> 00:03:48.721
&lt;i&gt;"I want to be able&lt;/i&gt;
&lt;i&gt;to solve intelligence."&lt;/i&gt;

00:03:49.722 --> 00:03:51.474
HASSABIS: &lt;i&gt;The human brain&lt;/i&gt;
&lt;i&gt;is the only existent proof&lt;/i&gt;

00:03:51.557 --> 00:03:53.393
&lt;i&gt;we have, perhaps&lt;/i&gt;
&lt;i&gt;in the entire universe,&lt;/i&gt;

00:03:53.476 --> 00:03:55.478
&lt;i&gt;that general intelligence&lt;/i&gt;
&lt;i&gt;is possible at all.&lt;/i&gt;

00:03:56.187 --> 00:03:58.898
&lt;i&gt;And I thought&lt;/i&gt;
&lt;i&gt;someone in this building&lt;/i&gt;

00:03:58.981 --> 00:03:59.982
&lt;i&gt;should be interested&lt;/i&gt;

00:04:00.066 --> 00:04:01.567
&lt;i&gt;in general intelligence&lt;/i&gt;
&lt;i&gt;like I am.&lt;/i&gt;

00:04:02.568 --> 00:04:04.445
&lt;i&gt;And then Shane's name&lt;/i&gt;
&lt;i&gt;popped up.&lt;/i&gt;

00:04:04.529 --> 00:04:06.948
HOST: Our next speaker today
is Shane Legg.

00:04:07.031 --> 00:04:08.032
He's from New Zealand,

00:04:08.116 --> 00:04:11.244
where he trained in math
and classical ballet.

00:04:11.327 --> 00:04:13.663
Are machines actually
becoming more intelligent?

00:04:13.746 --> 00:04:15.998
Some people say yes,
some people say no.

00:04:16.082 --> 00:04:17.125
It's not really clear.

00:04:17.208 --> 00:04:18.376
We know they're getting
a lot faster

00:04:18.459 --> 00:04:19.794
at doing computations.

00:04:19.877 --> 00:04:21.087
But are we actually
going forwards

00:04:21.170 --> 00:04:23.339
in terms
of general intelligence?

00:04:23.423 --> 00:04:25.425
HASSABIS: &lt;i&gt;We were both&lt;/i&gt;
&lt;i&gt;obsessed with AGI,&lt;/i&gt;

00:04:25.508 --> 00:04:27.176
&lt;i&gt;artificial&lt;/i&gt;
&lt;i&gt;general intelligence.&lt;/i&gt;

00:04:27.260 --> 00:04:28.511
So today I'm going
to be talking about

00:04:29.303 --> 00:04:31.681
different approaches
to building AGI.

00:04:31.764 --> 00:04:33.391
With my colleague
Demis Hassabis,

00:04:33.474 --> 00:04:35.393
we're looking at ways
to bring in ideas

00:04:35.476 --> 00:04:37.228
from theoretical neuroscience.

00:04:37.311 --> 00:04:41.065
I felt like we were
the keepers of a secret

00:04:41.149 --> 00:04:42.734
that no one else knew.

00:04:42.817 --> 00:04:45.069
&lt;i&gt;Shane and I knew&lt;/i&gt;
&lt;i&gt;no one in academia&lt;/i&gt;

00:04:45.153 --> 00:04:47.321
&lt;i&gt;would be supportive&lt;/i&gt;
&lt;i&gt;of what we were doing.&lt;/i&gt;

00:04:47.405 --> 00:04:50.366
AI was almost
an embarrassing word

00:04:50.450 --> 00:04:52.368
to use in academic circles,
right?

00:04:52.452 --> 00:04:54.412
If you said
you were working on AI,

00:04:54.495 --> 00:04:56.914
then you clearly weren't
a serious scientist.

00:04:57.623 --> 00:04:59.667
&lt;i&gt;So I convinced Shane&lt;/i&gt;
&lt;i&gt;the right way to do it&lt;/i&gt;

00:04:59.751 --> 00:05:00.835
&lt;i&gt;would be to start a company.&lt;/i&gt;

00:05:00.918 --> 00:05:02.795
SHANE LEGG: &lt;i&gt;Okay,&lt;/i&gt;
&lt;i&gt;we're going to try to do&lt;/i&gt;

00:05:02.879 --> 00:05:04.422
&lt;i&gt;artificial&lt;/i&gt;
&lt;i&gt;general intelligence.&lt;/i&gt;

00:05:04.505 --> 00:05:06.466
It may not even be possible.

00:05:06.549 --> 00:05:08.134
We're not quite sure
how we're going to do it,

00:05:08.217 --> 00:05:10.261
but we have some ideas
or, kind of, approaches.

00:05:11.387 --> 00:05:13.765
Huge amounts of money,
huge amounts of risk,

00:05:14.640 --> 00:05:16.059
lots and lots of compute.

00:05:18.311 --> 00:05:19.937
And if we pull this off,

00:05:20.021 --> 00:05:21.981
it'll be the biggest thing
ever, right?

00:05:23.316 --> 00:05:25.735
That is a very hard thing
for a typical investor

00:05:25.818 --> 00:05:26.986
to put their money on.

00:05:27.070 --> 00:05:28.738
It's almost like
buying a lottery ticket.

00:05:29.405 --> 00:05:32.116
I'm going to be speaking about
the system of neuroscience

00:05:32.200 --> 00:05:36.204
and how it might be used
to help us build AGI.

00:05:36.287 --> 00:05:37.455
HASSABIS:
&lt;i&gt;Finding initial funding&lt;/i&gt;

00:05:37.538 --> 00:05:38.664
&lt;i&gt;for this was very hard.&lt;/i&gt;

00:05:38.748 --> 00:05:40.833
&lt;i&gt;We're going to solve&lt;/i&gt;
&lt;i&gt;all of intelligence.&lt;/i&gt;

00:05:40.917 --> 00:05:42.418
&lt;i&gt;You can imagine&lt;/i&gt;
&lt;i&gt;some of the looks I got&lt;/i&gt;

00:05:42.502 --> 00:05:44.253
&lt;i&gt;when we were&lt;/i&gt;
&lt;i&gt;pitching that around.&lt;/i&gt;

00:05:44.337 --> 00:05:47.423
So I'm a V.C.
and I look at about

00:05:47.507 --> 00:05:49.801
700 to 1,000 projects a year.

00:05:50.635 --> 00:05:54.263
And I fund
literally 1% of those.

00:05:54.347 --> 00:05:55.932
About eight projects a year.

00:05:56.766 --> 00:05:59.936
So that means 99% of the time,
you're in "No" mode.

00:06:00.019 --> 00:06:01.354
"Wait a minute.
I'm telling you,

00:06:01.437 --> 00:06:03.272
"this is the most important
thing of all time.

00:06:03.356 --> 00:06:04.732
"I'm giving you
all this build-up

00:06:04.816 --> 00:06:05.817
"about how... explain

00:06:05.900 --> 00:06:06.943
"how it connects
with the brain,

00:06:07.026 --> 00:06:09.028
"why the time's right now,
and then you're asking me,

00:06:09.112 --> 00:06:10.571
"'But what's your, how are you
going to make money?

00:06:10.655 --> 00:06:11.698
"'What's your product?'"

00:06:11.781 --> 00:06:15.368
It's like,
so prosaic a question.

00:06:15.952 --> 00:06:17.036
You know?

00:06:17.120 --> 00:06:18.746
"Have you not been listening
to what I've been saying?"

00:06:18.830 --> 00:06:20.623
LEGG: &lt;i&gt;We needed investors&lt;/i&gt;

00:06:20.707 --> 00:06:23.126
&lt;i&gt;who aren't necessarily&lt;/i&gt;
&lt;i&gt;going to invest&lt;/i&gt;

00:06:23.209 --> 00:06:24.711
&lt;i&gt;because they think&lt;/i&gt;
&lt;i&gt;it's the best&lt;/i&gt;

00:06:24.794 --> 00:06:26.170
&lt;i&gt;investment decision.&lt;/i&gt;

00:06:26.254 --> 00:06:27.338
They're probably
going to invest

00:06:27.422 --> 00:06:29.173
because they just think
it's really cool.

00:06:29.257 --> 00:06:31.134
NEWSREADER:
&lt;i&gt;He's the Silicon Valley&lt;/i&gt;

00:06:31.217 --> 00:06:33.136
&lt;i&gt;version of the man&lt;/i&gt;
&lt;i&gt;behind the curtain&lt;/i&gt;

00:06:33.219 --> 00:06:34.387
&lt;i&gt;in &lt;/i&gt;The Wizard of Oz.

00:06:34.470 --> 00:06:35.847
&lt;i&gt;He had a lot to do&lt;/i&gt;
&lt;i&gt;with giving you&lt;/i&gt;

00:06:35.930 --> 00:06:38.516
&lt;i&gt;PayPal, Facebook,&lt;/i&gt;
&lt;i&gt;YouTube and Yelp.&lt;/i&gt;

00:06:38.599 --> 00:06:40.059
LEGG: &lt;i&gt;If everyone says "X,"&lt;/i&gt;

00:06:40.143 --> 00:06:42.812
Peter Thiel suspects
that the opposite of X

00:06:42.895 --> 00:06:43.938
is quite possibly true.

00:06:44.022 --> 00:06:47.358
HASSABIS: &lt;i&gt;So Peter Thiel&lt;/i&gt;
&lt;i&gt;was our first big investor.&lt;/i&gt;

00:06:47.442 --> 00:06:49.819
But he insisted that
we come to Silicon Valley

00:06:49.902 --> 00:06:51.320
because that
was the only place we could...

00:06:51.404 --> 00:06:52.530
There would be the talent,

00:06:52.613 --> 00:06:53.865
and we could build
that kind of company.

00:06:54.824 --> 00:06:56.492
&lt;i&gt;But I was pretty adamant&lt;/i&gt;
&lt;i&gt;we should be in London&lt;/i&gt;

00:06:56.576 --> 00:06:58.578
&lt;i&gt;because I think&lt;/i&gt;
&lt;i&gt;London's an amazing city.&lt;/i&gt;

00:06:58.661 --> 00:07:01.080
&lt;i&gt;Plus, I knew there were&lt;/i&gt;
&lt;i&gt;really amazing people&lt;/i&gt;

00:07:01.164 --> 00:07:03.291
&lt;i&gt;trained at Cambridge&lt;/i&gt;
&lt;i&gt;and Oxford and UCL.&lt;/i&gt;

00:07:03.374 --> 00:07:04.459
&lt;i&gt;In Silicon Valley,&lt;/i&gt;

00:07:04.542 --> 00:07:06.085
everybody's founding
a company every year,

00:07:06.169 --> 00:07:07.170
and then if it doesn't work,

00:07:07.253 --> 00:07:09.047
you chuck it
and you start something new.

00:07:09.130 --> 00:07:10.715
That is not conducive

00:07:10.798 --> 00:07:13.634
to a long-term
research challenge.

00:07:14.510 --> 00:07:16.929
&lt;i&gt;So we were totally&lt;/i&gt;
&lt;i&gt;an outlier for him.&lt;/i&gt;

00:07:17.013 --> 00:07:19.849
Hi, everyone.
Welcome to DeepMind.

00:07:20.558 --> 00:07:22.060
So, what is our mission?

00:07:23.269 --> 00:07:24.520
We summarize it as...

00:07:25.521 --> 00:07:27.523
DeepMind's mission is to build
the world's first

00:07:27.607 --> 00:07:29.067
general learning machine.

00:07:29.150 --> 00:07:31.194
So we always stress the word
"general" and "learning" here

00:07:31.277 --> 00:07:32.362
are the key things.

00:07:32.445 --> 00:07:34.655
LEGG: &lt;i&gt;Our mission&lt;/i&gt;
&lt;i&gt;was to build an AGI,&lt;/i&gt;

00:07:34.739 --> 00:07:36.449
&lt;i&gt;an artificial&lt;/i&gt;
&lt;i&gt;general intelligence.&lt;/i&gt;

00:07:36.532 --> 00:07:39.952
And so that means that we need
a system which is general.

00:07:40.036 --> 00:07:41.746
It doesn't learn to do
one specific thing.

00:07:42.497 --> 00:07:44.582
&lt;i&gt;That's a really key part&lt;/i&gt;
&lt;i&gt;of human intelligence.&lt;/i&gt;

00:07:44.665 --> 00:07:46.334
&lt;i&gt;We can learn to do&lt;/i&gt;
&lt;i&gt;many, many things.&lt;/i&gt;

00:07:46.417 --> 00:07:48.378
It's going to, of course,
be a lot of hard work.

00:07:48.461 --> 00:07:50.588
But one of the things
that keeps me up at night

00:07:50.672 --> 00:07:52.715
is to not waste this
opportunity to, you know,

00:07:52.799 --> 00:07:54.008
to really make
a difference here,

00:07:54.092 --> 00:07:55.551
and have a big impact
on the world.

00:07:56.427 --> 00:07:57.679
LEGG: &lt;i&gt;The first people&lt;/i&gt;
&lt;i&gt;that came&lt;/i&gt;

00:07:57.762 --> 00:07:59.847
&lt;i&gt;and joined DeepMind&lt;/i&gt;
&lt;i&gt;really believed in the dream.&lt;/i&gt;

00:07:59.931 --> 00:08:01.641
&lt;i&gt;But this was, I think,&lt;/i&gt;
&lt;i&gt;one of the first times&lt;/i&gt;

00:08:01.724 --> 00:08:04.102
&lt;i&gt;they found a place&lt;/i&gt;
&lt;i&gt;full of other dreamers.&lt;/i&gt;

00:08:04.185 --> 00:08:06.020
You know, we collected
this Manhattan Project,

00:08:06.104 --> 00:08:08.022
if you like,
together to solve AI.

00:08:08.106 --> 00:08:09.232
HELEN KING:
&lt;i&gt;In the first two years,&lt;/i&gt;

00:08:09.315 --> 00:08:10.316
&lt;i&gt;we were in total stealth mode.&lt;/i&gt;

00:08:10.400 --> 00:08:11.901
And so we couldn't
say to anyone

00:08:11.984 --> 00:08:14.237
what were we doing
or where we worked.

00:08:14.320 --> 00:08:15.363
It was all quite vague.

00:08:15.446 --> 00:08:17.281
BEN COPPIN: &lt;i&gt;It had&lt;/i&gt;
&lt;i&gt;no public presence at all.&lt;/i&gt;

00:08:17.365 --> 00:08:18.449
&lt;i&gt;You couldn't&lt;/i&gt;
&lt;i&gt;look at a website.&lt;/i&gt;

00:08:18.533 --> 00:08:20.660
&lt;i&gt;The office&lt;/i&gt;
&lt;i&gt;was at a secret location.&lt;/i&gt;

00:08:20.743 --> 00:08:23.538
When we would interview people
in those early days,

00:08:23.621 --> 00:08:25.707
they would show up
very nervously.

00:08:25.790 --> 00:08:26.833
(LAUGHS)

00:08:26.916 --> 00:08:28.626
I had at least one candidate
who said,

00:08:29.419 --> 00:08:31.546
"I just messaged my wife
to tell her exactly

00:08:31.629 --> 00:08:32.672
"where I'm going just in case

00:08:32.755 --> 00:08:34.215
"this turns out to be some
kind of horrible scam

00:08:34.298 --> 00:08:35.341
"and I'm going
to get kidnapped."

00:08:35.425 --> 00:08:39.387
Well, my favorite new person
who's an investor,

00:08:39.470 --> 00:08:42.557
who I've been working
for a year, is Elon Musk.

00:08:42.640 --> 00:08:43.683
So for those of you
who don't know,

00:08:43.766 --> 00:08:44.767
this is what he looks like.

00:08:44.851 --> 00:08:46.936
And he hadn't really thought
much about AI

00:08:47.020 --> 00:08:48.688
until we chatted.

00:08:48.771 --> 00:08:50.857
His mission is to die on Mars
or something.

00:08:50.940 --> 00:08:52.483
-But not on impact.
-(LAUGHTER)

00:08:52.567 --> 00:08:53.609
So...

00:08:54.777 --> 00:08:56.696
&lt;i&gt;We made some big decisions&lt;/i&gt;

00:08:56.779 --> 00:08:58.906
about how we were going
to approach building AI.

00:08:58.990 --> 00:09:00.658
This is a reinforcement
learning setup.

00:09:00.742 --> 00:09:02.493
This is the kind of setup
that we think about

00:09:02.577 --> 00:09:05.663
when we say we're building,
you know, an AI agent.

00:09:05.747 --> 00:09:08.207
It's basically the agent,
which is the AI,

00:09:08.291 --> 00:09:09.500
and then there's
the environment

00:09:09.584 --> 00:09:10.793
that it's interacting with.

00:09:10.877 --> 00:09:12.086
&lt;i&gt;We decided that games,&lt;/i&gt;

00:09:12.170 --> 00:09:13.546
as long as
you're very disciplined

00:09:13.629 --> 00:09:14.881
about how you use them,

00:09:14.964 --> 00:09:16.883
are the perfect
training ground

00:09:16.966 --> 00:09:18.092
for AI development.

00:09:19.010 --> 00:09:21.429
LEGG: &lt;i&gt;We wanted&lt;/i&gt;
&lt;i&gt;to try to create one algorithm&lt;/i&gt;

00:09:21.512 --> 00:09:23.389
&lt;i&gt;that could to be&lt;/i&gt;
&lt;i&gt;trained up to play&lt;/i&gt;

00:09:23.473 --> 00:09:25.600
&lt;i&gt;several dozen&lt;/i&gt;
&lt;i&gt;different Atari games.&lt;/i&gt;

00:09:25.683 --> 00:09:26.684
So just like a human,

00:09:26.768 --> 00:09:29.145
you have to use the same brain
to play all the games.

00:09:29.228 --> 00:09:30.480
DAVID SILVER:
&lt;i&gt;You can think of it&lt;/i&gt;

00:09:30.563 --> 00:09:32.607
&lt;i&gt;that you provide the agent&lt;/i&gt;
&lt;i&gt;with the cartridge.&lt;/i&gt;

00:09:32.690 --> 00:09:33.691
&lt;i&gt;And you say,&lt;/i&gt;

00:09:33.775 --> 00:09:35.234
&lt;i&gt;"Okay, imagine you're born&lt;/i&gt;
&lt;i&gt;into that world&lt;/i&gt;

00:09:35.318 --> 00:09:37.236
"with that cartridge,
and you just get to interact

00:09:37.320 --> 00:09:38.863
"with the pixels
and see the score.

00:09:40.031 --> 00:09:41.240
"What can you do?"

00:09:43.743 --> 00:09:46.954
So what you're going to do is
take your Q function. Q-K...

00:09:47.038 --> 00:09:48.790
HASSABIS: &lt;i&gt;Q-learning&lt;/i&gt;
&lt;i&gt;is one of the oldest methods&lt;/i&gt;

00:09:48.873 --> 00:09:50.124
&lt;i&gt;for reinforcement learning.&lt;/i&gt;

00:09:50.708 --> 00:09:53.378
&lt;i&gt;And what we did was combine&lt;/i&gt;
&lt;i&gt;reinforcement learning&lt;/i&gt;

00:09:53.461 --> 00:09:55.546
&lt;i&gt;with deep learning&lt;/i&gt;
&lt;i&gt;in one system.&lt;/i&gt;

00:09:56.547 --> 00:09:58.925
&lt;i&gt;No one had ever combined&lt;/i&gt;
&lt;i&gt;those two things together&lt;/i&gt;

00:09:59.008 --> 00:10:01.052
&lt;i&gt;at scale to do&lt;/i&gt;
&lt;i&gt;anything impressive,&lt;/i&gt;

00:10:01.135 --> 00:10:03.137
&lt;i&gt;and we needed&lt;/i&gt;
&lt;i&gt;to prove out this thesis.&lt;/i&gt;

00:10:03.221 --> 00:10:06.265
LEGG: &lt;i&gt;We tried doing &lt;/i&gt;Pong
&lt;i&gt;as the first game.&lt;/i&gt;

00:10:06.349 --> 00:10:07.684
&lt;i&gt;It seemed like the simplest.&lt;/i&gt;

00:10:07.767 --> 00:10:08.851
It hasn't been told

00:10:09.852 --> 00:10:11.312
anything about
what it's controlling

00:10:11.396 --> 00:10:12.397
or what it's supposed to do.

00:10:12.480 --> 00:10:14.315
All it knows
is that score is good

00:10:14.399 --> 00:10:17.568
and it has to learn
what its controls do,

00:10:17.652 --> 00:10:20.113
and build everything...
first principles.

00:10:20.905 --> 00:10:22.323
(GAME BEEPING)

00:10:28.788 --> 00:10:29.831
It wasn't really working.

00:10:32.250 --> 00:10:33.626
HASSABIS: &lt;i&gt;I was just&lt;/i&gt;
&lt;i&gt;saying to Shane,&lt;/i&gt;

00:10:33.710 --> 00:10:36.587
&lt;i&gt;"Maybe we're just wrong,&lt;/i&gt;
&lt;i&gt;and we can't even do &lt;/i&gt;Pong."

00:10:36.671 --> 00:10:38.297
LEGG: &lt;i&gt;It was a bit&lt;/i&gt;
&lt;i&gt;nerve-racking,&lt;/i&gt;

00:10:38.381 --> 00:10:40.008
thinking how far we had to go

00:10:40.091 --> 00:10:41.300
if we were going
to really build

00:10:42.010 --> 00:10:44.012
a generally
intelligent system.

00:10:44.095 --> 00:10:45.221
HASSABIS: &lt;i&gt;And it felt like&lt;/i&gt;
&lt;i&gt;it was time&lt;/i&gt;

00:10:45.304 --> 00:10:46.639
&lt;i&gt;to give up and move on.&lt;/i&gt;

00:10:47.765 --> 00:10:49.017
&lt;i&gt;And then suddenly...&lt;/i&gt;

00:10:49.100 --> 00:10:50.309
(STIRRING MUSIC PLAYS)

00:10:51.060 --> 00:10:52.228
&lt;i&gt;We got our first point.&lt;/i&gt;

00:10:53.354 --> 00:10:55.106
&lt;i&gt;And then it was like,&lt;/i&gt;
&lt;i&gt;"Is this random?"&lt;/i&gt;

00:10:56.357 --> 00:10:57.942
&lt;i&gt;"No, no, it's really&lt;/i&gt;
&lt;i&gt;getting a point now."&lt;/i&gt;

00:10:58.818 --> 00:11:00.361
It was really exciting
that this thing

00:11:00.445 --> 00:11:01.821
that previously
couldn't even figure out

00:11:01.904 --> 00:11:02.947
how to move a paddle

00:11:03.031 --> 00:11:05.116
had suddenly been able
to totally get it right.

00:11:05.700 --> 00:11:06.701
HASSABIS: &lt;i&gt;Then it was getting&lt;/i&gt;
&lt;i&gt;a few points.&lt;/i&gt;

00:11:06.784 --> 00:11:07.952
&lt;i&gt;And then it won&lt;/i&gt;
&lt;i&gt;its first game.&lt;/i&gt;

00:11:08.036 --> 00:11:10.705
&lt;i&gt;And then three months later,&lt;/i&gt;
&lt;i&gt;no human could beat it.&lt;/i&gt;

00:11:10.788 --> 00:11:13.791
&lt;i&gt;You hadn't told it the rules,&lt;/i&gt;
&lt;i&gt;how to get the score, nothing.&lt;/i&gt;

00:11:13.875 --> 00:11:15.668
&lt;i&gt;And you just tell it&lt;/i&gt;
&lt;i&gt;to maximize the score,&lt;/i&gt;

00:11:15.752 --> 00:11:17.003
&lt;i&gt;and it goes away and does it.&lt;/i&gt;

00:11:17.086 --> 00:11:18.129
&lt;i&gt;This is the first time&lt;/i&gt;

00:11:18.212 --> 00:11:20.173
&lt;i&gt;anyone had done&lt;/i&gt;
&lt;i&gt;this end-to-end learning.&lt;/i&gt;

00:11:20.256 --> 00:11:23.885
"Okay, so we have this working
in quite a general way.

00:11:23.968 --> 00:11:25.511
&lt;i&gt;"Now let's try another game."&lt;/i&gt;

00:11:25.595 --> 00:11:27.138
HASSABIS: &lt;i&gt;So then&lt;/i&gt;
&lt;i&gt;we tried &lt;/i&gt;Breakout.

00:11:27.221 --> 00:11:28.639
At the beginning,
after 100 games,

00:11:28.723 --> 00:11:30.475
the agent is not very good.

00:11:30.558 --> 00:11:32.060
It's missing the ball
most of the time,

00:11:32.143 --> 00:11:33.978
but it's starting to get
the hang of the idea

00:11:34.062 --> 00:11:35.521
that the bat should go
towards the ball.

00:11:35.605 --> 00:11:37.190
&lt;i&gt;Now, after 300 games,&lt;/i&gt;

00:11:37.273 --> 00:11:39.525
&lt;i&gt;it's about as good as&lt;/i&gt;
&lt;i&gt;any human can play this.&lt;/i&gt;

00:11:40.401 --> 00:11:41.569
We thought,
"Well, that's pretty cool,"

00:11:41.652 --> 00:11:43.905
but we left the system playing
for another 200 games,

00:11:43.988 --> 00:11:45.573
and it did this amazing thing.

00:11:45.656 --> 00:11:46.949
It found the optimal strategy

00:11:47.033 --> 00:11:48.993
was to dig a tunnel
around the side

00:11:49.077 --> 00:11:50.745
and put the ball
around the back of the wall.

00:11:51.371 --> 00:11:52.789
KORAY KAVUKCUOGLU:
&lt;i&gt;Finally, the agent&lt;/i&gt;

00:11:52.872 --> 00:11:53.915
&lt;i&gt;is actually achieving&lt;/i&gt;

00:11:53.998 --> 00:11:55.333
&lt;i&gt;what you thought&lt;/i&gt;
&lt;i&gt;it would achieve.&lt;/i&gt;

00:11:55.416 --> 00:11:56.876
That is a great feeling.
Right?

00:11:56.959 --> 00:11:58.836
Like, I mean,
when we do research,

00:11:58.920 --> 00:12:00.296
that is the best
we can hope for.

00:12:00.380 --> 00:12:02.757
&lt;i&gt;We started generalizing&lt;/i&gt;
&lt;i&gt;to 50 games,&lt;/i&gt;

00:12:02.840 --> 00:12:05.093
&lt;i&gt;and we basically&lt;/i&gt;
&lt;i&gt;created a recipe.&lt;/i&gt;

00:12:05.176 --> 00:12:06.511
&lt;i&gt;We could just take a game&lt;/i&gt;

00:12:06.594 --> 00:12:07.970
&lt;i&gt;that we have&lt;/i&gt;
&lt;i&gt;never seen before.&lt;/i&gt;

00:12:08.054 --> 00:12:09.597
&lt;i&gt;We would run&lt;/i&gt;
&lt;i&gt;the algorithm on that,&lt;/i&gt;

00:12:09.681 --> 00:12:12.558
&lt;i&gt;and DQN could train itself&lt;/i&gt;
&lt;i&gt;from scratch,&lt;/i&gt;

00:12:12.642 --> 00:12:14.018
&lt;i&gt;achieving human level&lt;/i&gt;

00:12:14.102 --> 00:12:15.770
&lt;i&gt;or sometimes better&lt;/i&gt;
&lt;i&gt;than human level.&lt;/i&gt;

00:12:15.853 --> 00:12:18.022
LEGG: &lt;i&gt;We didn't build it&lt;/i&gt;
&lt;i&gt;to play any of them.&lt;/i&gt;

00:12:18.106 --> 00:12:20.108
We could just give it
a bunch of games

00:12:20.191 --> 00:12:21.651
and would figure it out
for itself.

00:12:22.360 --> 00:12:24.737
And there was something
quite magical in that.

00:12:24.821 --> 00:12:26.114
MURRAY SHANAHAN:
&lt;i&gt;Suddenly you had something&lt;/i&gt;

00:12:26.197 --> 00:12:27.657
&lt;i&gt;that would respond and learn&lt;/i&gt;

00:12:27.740 --> 00:12:29.951
&lt;i&gt;whatever situation&lt;/i&gt;
&lt;i&gt;it was parachuted into.&lt;/i&gt;

00:12:30.034 --> 00:12:32.537
&lt;i&gt;And that was like a huge,&lt;/i&gt;
&lt;i&gt;huge breakthrough.&lt;/i&gt;

00:12:32.620 --> 00:12:34.330
&lt;i&gt;It was in many respects&lt;/i&gt;

00:12:34.956 --> 00:12:36.249
the first example

00:12:36.332 --> 00:12:38.584
of any kind of thing
you could call

00:12:38.668 --> 00:12:39.961
a general intelligence.

00:12:41.838 --> 00:12:43.589
HASSABIS: &lt;i&gt;Although we were&lt;/i&gt;
&lt;i&gt;a well-funded startup,&lt;/i&gt;

00:12:43.673 --> 00:12:45.967
&lt;i&gt;holding us back&lt;/i&gt;
&lt;i&gt;was not enough compute power.&lt;/i&gt;

00:12:47.051 --> 00:12:48.761
&lt;i&gt;I realized that&lt;/i&gt;
&lt;i&gt;this would accelerate&lt;/i&gt;

00:12:48.845 --> 00:12:51.055
&lt;i&gt;our time scale&lt;/i&gt;
&lt;i&gt;to AGI massively.&lt;/i&gt;

00:12:51.139 --> 00:12:52.682
I used to see Demis
quite frequently.

00:12:52.765 --> 00:12:54.475
We'd have lunch, and he did...

00:12:55.768 --> 00:12:58.563
say to me that
he had two companies

00:12:58.646 --> 00:13:01.357
that were involved
in buying DeepMind.

00:13:02.233 --> 00:13:04.235
And he didn't know
which one to go with.

00:13:04.318 --> 00:13:08.114
&lt;i&gt;The issue was,&lt;/i&gt;
&lt;i&gt;would any commercial company&lt;/i&gt;

00:13:08.197 --> 00:13:11.576
&lt;i&gt;appreciate the real importance&lt;/i&gt;
&lt;i&gt;of the research?&lt;/i&gt;

00:13:12.368 --> 00:13:15.538
And give the research time
to come to fruition

00:13:15.621 --> 00:13:17.457
and not be breathing down
their necks,

00:13:17.540 --> 00:13:20.960
&lt;i&gt;saying, "We want some kind of&lt;/i&gt;
&lt;i&gt;commercial benefit from this."&lt;/i&gt;

00:13:23.046 --> 00:13:24.047
(MACHINERY HUMMING)

00:13:27.050 --> 00:13:31.512
Google has bought DeepMind
for a reported Â£400,000,000,

00:13:32.180 --> 00:13:34.349
making the artificial
intelligence firm

00:13:34.432 --> 00:13:37.435
its largest
European acquisition so far.

00:13:37.518 --> 00:13:39.270
&lt;i&gt;The company was founded&lt;/i&gt;

00:13:39.354 --> 00:13:42.398
&lt;i&gt;by 37-year-old entrepreneur&lt;/i&gt;
&lt;i&gt;Demis Hassabis.&lt;/i&gt;

00:13:42.982 --> 00:13:45.151
After the acquisition,
I started mentoring

00:13:45.234 --> 00:13:46.778
and spending time with Demis,

00:13:46.861 --> 00:13:48.029
and just listening to him.

00:13:48.738 --> 00:13:51.449
And this is a person
who fundamentally

00:13:52.200 --> 00:13:54.619
is a scientist
and a natural scientist.

00:13:55.244 --> 00:13:58.081
He wants science to solve
every problem in the world,

00:13:58.164 --> 00:13:59.665
and he believes it can do so.

00:14:00.291 --> 00:14:03.086
That's not a normal person
you find in a tech company.

00:14:05.088 --> 00:14:07.006
HASSABIS: We were able
to not only join Google

00:14:07.090 --> 00:14:09.759
but run independently
in London,

00:14:09.842 --> 00:14:10.885
build our culture,

00:14:10.968 --> 00:14:13.012
which was optimized
for breakthroughs

00:14:13.096 --> 00:14:14.430
and not deal with products,

00:14:15.098 --> 00:14:16.140
do pure research.

00:14:17.433 --> 00:14:18.893
&lt;i&gt;Our investors&lt;/i&gt;
&lt;i&gt;didn't want to sell,&lt;/i&gt;

00:14:18.976 --> 00:14:19.977
&lt;i&gt;but we decided&lt;/i&gt;

00:14:20.061 --> 00:14:21.813
&lt;i&gt;that this was the best thing&lt;/i&gt;
&lt;i&gt;for the mission.&lt;/i&gt;

00:14:22.605 --> 00:14:24.232
&lt;i&gt;In many senses,&lt;/i&gt;
&lt;i&gt;we were underselling&lt;/i&gt;

00:14:24.315 --> 00:14:25.858
&lt;i&gt;in terms of value&lt;/i&gt;
&lt;i&gt;before it more matured,&lt;/i&gt;

00:14:25.942 --> 00:14:27.193
&lt;i&gt;and you could have sold it&lt;/i&gt;
&lt;i&gt;for a lot more money.&lt;/i&gt;

00:14:27.902 --> 00:14:31.114
&lt;i&gt;And the reason is because&lt;/i&gt;
&lt;i&gt;there's no time to waste.&lt;/i&gt;

00:14:32.573 --> 00:14:33.991
There's so many things
that got to be cracked

00:14:34.784 --> 00:14:37.328
while the brain
is still in gear.

00:14:37.412 --> 00:14:38.705
You know, I'm still alive.

00:14:38.788 --> 00:14:40.540
There's all these things
that gotta be done.

00:14:40.623 --> 00:14:42.625
So you haven't got--
I mean, how many...

00:14:42.709 --> 00:14:44.043
How many billions
would you trade for

00:14:44.127 --> 00:14:45.461
another five years of life,
you know,

00:14:45.545 --> 00:14:47.922
to do what you set out to do?

00:14:48.006 --> 00:14:49.132
Okay, all of a sudden,

00:14:49.215 --> 00:14:51.968
we've got this massive scale
compute available to us.

00:14:52.468 --> 00:14:53.469
What can we do with that?

00:14:56.222 --> 00:14:58.349
HASSABIS: &lt;i&gt;Go is the pinnacle&lt;/i&gt;
&lt;i&gt;of board games.&lt;/i&gt;

00:14:59.684 --> 00:15:02.603
&lt;i&gt;It is the most complex game&lt;/i&gt;
&lt;i&gt;ever devised by man.&lt;/i&gt;

00:15:04.313 --> 00:15:06.315
&lt;i&gt;There are more possible&lt;/i&gt;
&lt;i&gt;board configurations&lt;/i&gt;

00:15:06.399 --> 00:15:08.401
&lt;i&gt;in the game of Go than there&lt;/i&gt;
&lt;i&gt;are atoms in the universe.&lt;/i&gt;

00:15:09.652 --> 00:15:12.280
SILVER: &lt;i&gt;Go is the holy grail&lt;/i&gt;
&lt;i&gt;of artificial intelligence.&lt;/i&gt;

00:15:13.031 --> 00:15:14.073
&lt;i&gt;For many years,&lt;/i&gt;

00:15:14.157 --> 00:15:15.408
&lt;i&gt;people have looked&lt;/i&gt;
&lt;i&gt;at this game&lt;/i&gt;

00:15:15.491 --> 00:15:17.618
&lt;i&gt;and they've thought,&lt;/i&gt;
&lt;i&gt;"Wow, this is just too hard."&lt;/i&gt;

00:15:17.702 --> 00:15:19.871
Everything we've ever
tried in AI,

00:15:19.954 --> 00:15:22.290
it just falls over when
you try the game of Go.

00:15:22.373 --> 00:15:23.666
&lt;i&gt;And so that's why&lt;/i&gt;
&lt;i&gt;it feels like&lt;/i&gt;

00:15:23.750 --> 00:15:25.585
&lt;i&gt;a real litmus test&lt;/i&gt;
&lt;i&gt;of progress.&lt;/i&gt;

00:15:25.668 --> 00:15:28.171
We had just bought DeepMind.

00:15:28.254 --> 00:15:30.340
They were working
on reinforcement learning

00:15:30.423 --> 00:15:32.633
&lt;i&gt;and they were the world's&lt;/i&gt;
&lt;i&gt;experts in games.&lt;/i&gt;

00:15:32.717 --> 00:15:34.469
&lt;i&gt;And so when&lt;/i&gt;
&lt;i&gt;they introduced the idea&lt;/i&gt;

00:15:34.552 --> 00:15:36.888
&lt;i&gt;that they could beat&lt;/i&gt;
&lt;i&gt;the top level Go players&lt;/i&gt;

00:15:36.971 --> 00:15:39.807
&lt;i&gt;in a game that was thought&lt;/i&gt;
&lt;i&gt;to be incomputable,&lt;/i&gt;

00:15:39.891 --> 00:15:41.893
I thought, "Well,
that's pretty interesting."

00:15:42.435 --> 00:15:46.022
Our ultimate next step
is to play the legendary

00:15:46.105 --> 00:15:48.775
Lee Sedol
in just over two weeks.

00:15:50.109 --> 00:15:51.444
NEWSREADER 1:
&lt;i&gt;A match like no other&lt;/i&gt;

00:15:51.527 --> 00:15:53.946
&lt;i&gt;is about to get underway&lt;/i&gt;
&lt;i&gt;in South Korea.&lt;/i&gt;

00:15:54.030 --> 00:15:56.783
NEWSREADER 2: &lt;i&gt;Lee Sedol&lt;/i&gt;
&lt;i&gt;is getting ready to rumble.&lt;/i&gt;

00:15:57.533 --> 00:15:58.701
HASSABIS:
&lt;i&gt;Lee Sedol is probably&lt;/i&gt;

00:15:58.785 --> 00:16:00.620
&lt;i&gt;one of the greatest players&lt;/i&gt;
&lt;i&gt;of the last decade.&lt;/i&gt;

00:16:01.329 --> 00:16:03.831
&lt;i&gt;I describe him&lt;/i&gt;
&lt;i&gt;as the Roger Federer of Go.&lt;/i&gt;

00:16:05.166 --> 00:16:06.459
ERIC SCHMIDT: &lt;i&gt;He showed up,&lt;/i&gt;

00:16:06.542 --> 00:16:09.462
&lt;i&gt;and all of a sudden&lt;/i&gt;
&lt;i&gt;we have a thousand Koreans&lt;/i&gt;

00:16:09.545 --> 00:16:12.507
who represent
all of Korean society,

00:16:12.590 --> 00:16:13.758
the top Go players.

00:16:15.176 --> 00:16:16.636
&lt;i&gt;And then we have Demis.&lt;/i&gt;

00:16:17.512 --> 00:16:19.305
&lt;i&gt;And the great&lt;/i&gt;
&lt;i&gt;engineering team.&lt;/i&gt;

00:16:19.889 --> 00:16:21.808
Lee Sedol, he's very famous

00:16:21.891 --> 00:16:24.227
for very creative
fighting play.

00:16:25.144 --> 00:16:27.897
&lt;i&gt;So this could be&lt;/i&gt;
&lt;i&gt;difficult for us.&lt;/i&gt;

00:16:28.481 --> 00:16:31.693
SCHMIDT: &lt;i&gt;I figured Lee Sedol&lt;/i&gt;
&lt;i&gt;is going to beat these guys,&lt;/i&gt;

00:16:31.776 --> 00:16:33.319
&lt;i&gt;but they'll make&lt;/i&gt;
&lt;i&gt;a good showing.&lt;/i&gt;

00:16:33.903 --> 00:16:35.154
Good for a startup.

00:16:37.740 --> 00:16:39.158
&lt;i&gt;I went over&lt;/i&gt;
&lt;i&gt;to the technical group&lt;/i&gt;

00:16:39.242 --> 00:16:40.243
&lt;i&gt;and they said,&lt;/i&gt;

00:16:40.326 --> 00:16:41.911
&lt;i&gt;"Let me show you&lt;/i&gt;
&lt;i&gt;how our algorithm works."&lt;/i&gt;

00:16:43.287 --> 00:16:44.831
RESEARCHER: If you step
through the actual game,

00:16:44.914 --> 00:16:47.375
we can see, kind of,
how AlphaGo thinks.

00:16:47.458 --> 00:16:49.711
HASSABIS: &lt;i&gt;The way we start off&lt;/i&gt;
&lt;i&gt;on training AlphaGo&lt;/i&gt;

00:16:49.794 --> 00:16:52.588
&lt;i&gt;is by showing it 100,000 games&lt;/i&gt;

00:16:52.672 --> 00:16:54.048
&lt;i&gt;that strong amateurs&lt;/i&gt;
&lt;i&gt;have played.&lt;/i&gt;

00:16:54.132 --> 00:16:55.299
&lt;i&gt;And we first initially&lt;/i&gt;

00:16:55.383 --> 00:16:57.885
&lt;i&gt;get AlphaGo to mimic&lt;/i&gt;
&lt;i&gt;the human player,&lt;/i&gt;

00:16:58.594 --> 00:17:00.430
&lt;i&gt;and then through&lt;/i&gt;
&lt;i&gt;reinforcement learning,&lt;/i&gt;

00:17:00.513 --> 00:17:02.098
&lt;i&gt;it plays against&lt;/i&gt;
&lt;i&gt;different versions of itself&lt;/i&gt;

00:17:02.181 --> 00:17:05.351
&lt;i&gt;many millions of times&lt;/i&gt;
&lt;i&gt;and learns from its errors.&lt;/i&gt;

00:17:05.435 --> 00:17:07.145
Hmm, this is interesting.

00:17:07.228 --> 00:17:08.271
ANNOUNCER 1: All right, folks,

00:17:08.354 --> 00:17:10.148
you're going to see
history made.

00:17:10.231 --> 00:17:11.315
(ANNOUNCER 2 SPEAKING KOREAN)

00:17:12.400 --> 00:17:13.651
SCHMIDT: &lt;i&gt;So the game starts.&lt;/i&gt;

00:17:14.444 --> 00:17:15.611
ANNOUNCER 1:
&lt;i&gt;He's really concentrating.&lt;/i&gt;

00:17:15.695 --> 00:17:17.071
ANNOUNCER 3:
&lt;i&gt;If you really look at the...&lt;/i&gt;

00:17:19.032 --> 00:17:20.533
(ANNOUNCERS EXCLAIM)

00:17:20.616 --> 00:17:22.785
That's a very surprising move.

00:17:25.121 --> 00:17:27.165
ANNOUNCER 3: I think we're
seeing an original move here.

00:17:34.213 --> 00:17:35.214
Yeah, that's an exciting move.

00:17:35.715 --> 00:17:36.758
I like...

00:17:36.841 --> 00:17:38.134
SILVER:
&lt;i&gt;Professional commentators&lt;/i&gt;

00:17:38.217 --> 00:17:39.635
&lt;i&gt;almost unanimously said&lt;/i&gt;

00:17:39.719 --> 00:17:42.764
&lt;i&gt;that not a single human player&lt;/i&gt;
&lt;i&gt;would have chosen move 37.&lt;/i&gt;

00:17:43.348 --> 00:17:45.391
&lt;i&gt;So I actually had a poke&lt;/i&gt;
&lt;i&gt;around in AlphaGo&lt;/i&gt;

00:17:45.475 --> 00:17:47.143
&lt;i&gt;to see what AlphaGo thought.&lt;/i&gt;

00:17:47.226 --> 00:17:50.021
&lt;i&gt;And AlphaGo actually agreed&lt;/i&gt;
&lt;i&gt;with that assessment.&lt;/i&gt;

00:17:50.104 --> 00:17:53.399
&lt;i&gt;AlphaGo said there was a one&lt;/i&gt;
&lt;i&gt;in 10,000 probability&lt;/i&gt;

00:17:53.483 --> 00:17:56.861
&lt;i&gt;that move 37 would have been&lt;/i&gt;
&lt;i&gt;played by a human player.&lt;/i&gt;

00:17:57.487 --> 00:17:59.489
(SEDOL SPEAKING IN KOREAN)

00:18:08.456 --> 00:18:09.665
SILVER: &lt;i&gt;The game of Go&lt;/i&gt;
&lt;i&gt;has been studied&lt;/i&gt;

00:18:09.749 --> 00:18:11.209
&lt;i&gt;for thousands of years.&lt;/i&gt;

00:18:11.292 --> 00:18:14.462
&lt;i&gt;And AlphaGo discovered&lt;/i&gt;
&lt;i&gt;something completely new.&lt;/i&gt;

00:18:16.506 --> 00:18:19.342
ANNOUNCER: He resigned.
Lee Sedol has just resigned.

00:18:19.425 --> 00:18:20.677
He's beaten.

00:18:20.760 --> 00:18:22.011
(ELECTRONIC MUSIC PLAYING)

00:18:22.095 --> 00:18:24.222
NEWSREADER 1: &lt;i&gt;The battle&lt;/i&gt;
&lt;i&gt;between man versus machine,&lt;/i&gt;

00:18:24.305 --> 00:18:25.932
&lt;i&gt;a computer just came out&lt;/i&gt;
&lt;i&gt;the victor.&lt;/i&gt;

00:18:26.015 --> 00:18:27.725
NEWSREADER 2: &lt;i&gt;Google&lt;/i&gt;
&lt;i&gt;put its DeepMind team&lt;/i&gt;

00:18:27.809 --> 00:18:28.893
&lt;i&gt;to the test against&lt;/i&gt;

00:18:28.976 --> 00:18:31.729
&lt;i&gt;one of the brightest minds&lt;/i&gt;
&lt;i&gt;in the world and won.&lt;/i&gt;

00:18:31.813 --> 00:18:33.272
SCHMIDT:
&lt;i&gt;That's when we realized&lt;/i&gt;

00:18:33.356 --> 00:18:34.899
&lt;i&gt;the DeepMind people knew&lt;/i&gt;
&lt;i&gt;what they were doing&lt;/i&gt;

00:18:34.982 --> 00:18:37.068
&lt;i&gt;and to pay attention&lt;/i&gt;
&lt;i&gt;to reinforcement learning&lt;/i&gt;

00:18:37.151 --> 00:18:38.319
&lt;i&gt;as they have invented it.&lt;/i&gt;

00:18:39.779 --> 00:18:41.406
&lt;i&gt;Based on that experience,&lt;/i&gt;

00:18:41.489 --> 00:18:44.409
&lt;i&gt;AlphaGo got better&lt;/i&gt;
&lt;i&gt;and better and better.&lt;/i&gt;

00:18:44.492 --> 00:18:45.576
And they had a little chart

00:18:45.660 --> 00:18:47.036
of how much better
they were getting.

00:18:47.120 --> 00:18:48.705
And I said,
"When does this stop?"

00:18:49.580 --> 00:18:50.665
And Demis said,

00:18:50.748 --> 00:18:52.208
"When we beat the Chinese guy,

00:18:52.291 --> 00:18:54.877
&lt;i&gt;"the top-rated player&lt;/i&gt;
&lt;i&gt;in the world."&lt;/i&gt;

00:18:56.754 --> 00:18:58.756
ANNOUNCER 1:
&lt;i&gt;Ke Jie versus AlphaGo.&lt;/i&gt;

00:19:03.219 --> 00:19:04.387
ANNOUNCER 2:
And I think we will see

00:19:04.470 --> 00:19:05.805
AlphaGo pushing through there.

00:19:05.888 --> 00:19:07.849
ANNOUNCER 1:
AlphaGo is ahead quite a bit.

00:19:07.932 --> 00:19:11.185
SCHMIDT: &lt;i&gt;About halfway&lt;/i&gt;
&lt;i&gt;through the first game,&lt;/i&gt;

00:19:11.269 --> 00:19:14.230
&lt;i&gt;the best player in the world&lt;/i&gt;
&lt;i&gt;was not doing so well.&lt;/i&gt;

00:19:14.313 --> 00:19:17.233
ANNOUNCER 1:
What can black do here?

00:19:18.860 --> 00:19:19.861
ANNOUNCER 2: Looks difficult.

00:19:21.070 --> 00:19:22.572
SCHMIDT:
&lt;i&gt;And at a critical moment...&lt;/i&gt;

00:19:32.832 --> 00:19:34.917
the Chinese government
ordered the feed cut off.

00:19:38.004 --> 00:19:41.299
&lt;i&gt;It was at that moment&lt;/i&gt;
&lt;i&gt;we were telling the world&lt;/i&gt;

00:19:41.382 --> 00:19:44.385
&lt;i&gt;that something new&lt;/i&gt;
&lt;i&gt;had arrived on earth.&lt;/i&gt;

00:19:47.180 --> 00:19:48.348
&lt;i&gt;In the 1950s&lt;/i&gt;

00:19:48.431 --> 00:19:51.351
&lt;i&gt;when Russia's &lt;/i&gt;Sputnik
&lt;i&gt;satellite was launched,&lt;/i&gt;

00:19:53.019 --> 00:19:54.395
&lt;i&gt;it changed&lt;/i&gt;
&lt;i&gt;the course of history.&lt;/i&gt;

00:19:54.937 --> 00:19:57.065
TV HOST: &lt;i&gt;It is a challenge&lt;/i&gt;
&lt;i&gt;that America must meet&lt;/i&gt;

00:19:57.148 --> 00:19:58.608
&lt;i&gt;to survive in the Space Age.&lt;/i&gt;

00:19:59.650 --> 00:20:01.903
SCHMIDT: &lt;i&gt;This has been&lt;/i&gt;
&lt;i&gt;called the &lt;/i&gt;Sputnik&lt;i&gt; moment.&lt;/i&gt;

00:20:02.487 --> 00:20:05.990
The &lt;i&gt;Sputnik&lt;/i&gt; moment created
a massive reaction in the US

00:20:06.074 --> 00:20:09.660
&lt;i&gt;in terms of funding&lt;/i&gt;
&lt;i&gt;for science and engineering,&lt;/i&gt;

00:20:09.744 --> 00:20:11.329
&lt;i&gt;and particularly&lt;/i&gt;
&lt;i&gt;of space technology.&lt;/i&gt;

00:20:12.038 --> 00:20:14.707
For China,
AlphaGo was the wakeup call,

00:20:15.458 --> 00:20:16.834
the &lt;i&gt;Sputnik&lt;/i&gt; moment.

00:20:16.918 --> 00:20:19.170
&lt;i&gt;It launched an AI space race.&lt;/i&gt;

00:20:20.880 --> 00:20:22.715
HASSABIS: &lt;i&gt;We had this&lt;/i&gt;
&lt;i&gt;huge idea that worked,&lt;/i&gt;

00:20:22.799 --> 00:20:25.134
&lt;i&gt;and now the whole world knows.&lt;/i&gt;

00:20:26.177 --> 00:20:28.471
&lt;i&gt;It's always easier&lt;/i&gt;
&lt;i&gt;to land on the moon&lt;/i&gt;

00:20:28.554 --> 00:20:30.098
&lt;i&gt;if someone's already&lt;/i&gt;
&lt;i&gt;landed there.&lt;/i&gt;

00:20:31.766 --> 00:20:33.935
&lt;i&gt;It is going to matter&lt;/i&gt;
&lt;i&gt;who builds AI,&lt;/i&gt;

00:20:34.018 --> 00:20:35.353
&lt;i&gt;and how it gets built.&lt;/i&gt;

00:20:36.229 --> 00:20:37.730
&lt;i&gt;I always feel that pressure.&lt;/i&gt;

00:20:41.693 --> 00:20:43.528
SILVER: &lt;i&gt;There's been&lt;/i&gt;
&lt;i&gt;a big chain of events&lt;/i&gt;

00:20:43.611 --> 00:20:46.197
&lt;i&gt;that followed on from all&lt;/i&gt;
&lt;i&gt;of the excitement of AlphaGo.&lt;/i&gt;

00:20:46.280 --> 00:20:47.740
&lt;i&gt;When we played&lt;/i&gt;
&lt;i&gt;against Lee Sedol,&lt;/i&gt;

00:20:47.824 --> 00:20:48.908
&lt;i&gt;we actually had a system&lt;/i&gt;

00:20:48.991 --> 00:20:50.201
&lt;i&gt;that had been trained&lt;/i&gt;
&lt;i&gt;on human data,&lt;/i&gt;

00:20:50.284 --> 00:20:51.911
&lt;i&gt;on all of the millions&lt;/i&gt;
&lt;i&gt;of games&lt;/i&gt;

00:20:51.994 --> 00:20:53.663
&lt;i&gt;that have been played&lt;/i&gt;
&lt;i&gt;by human experts.&lt;/i&gt;

00:20:54.747 --> 00:20:56.582
&lt;i&gt;We eventually found&lt;/i&gt;
&lt;i&gt;a new algorithm,&lt;/i&gt;

00:20:56.666 --> 00:20:58.793
&lt;i&gt;a much more elegant approach&lt;/i&gt;
&lt;i&gt;to the whole system,&lt;/i&gt;

00:20:58.876 --> 00:21:00.795
which actually stripped out
all of the human knowledge

00:21:00.878 --> 00:21:02.755
and just started
completely from scratch.

00:21:03.506 --> 00:21:06.426
&lt;i&gt;And that became a project&lt;/i&gt;
&lt;i&gt;which we called AlphaZero.&lt;/i&gt;

00:21:06.509 --> 00:21:09.137
&lt;i&gt;Zero, meaning having zero&lt;/i&gt;
&lt;i&gt;human knowledge in the loop.&lt;/i&gt;

00:21:11.472 --> 00:21:12.724
&lt;i&gt;Instead of learning&lt;/i&gt;
&lt;i&gt;from human data,&lt;/i&gt;

00:21:12.807 --> 00:21:13.975
&lt;i&gt;it learned from its own games.&lt;/i&gt;

00:21:15.601 --> 00:21:17.228
&lt;i&gt;So it actually&lt;/i&gt;
&lt;i&gt;became its own teacher.&lt;/i&gt;

00:21:20.982 --> 00:21:23.192
HASSABIS:
&lt;i&gt;AlphaZero is an experiment&lt;/i&gt;

00:21:23.276 --> 00:21:26.279
&lt;i&gt;in how little knowledge&lt;/i&gt;
&lt;i&gt;can we put into these systems&lt;/i&gt;

00:21:26.362 --> 00:21:27.697
&lt;i&gt;and how quickly&lt;/i&gt;
&lt;i&gt;and how efficiently&lt;/i&gt;

00:21:27.780 --> 00:21:28.781
&lt;i&gt;can they learn?&lt;/i&gt;

00:21:29.824 --> 00:21:32.076
&lt;i&gt;AlphaZero doesn't&lt;/i&gt;
&lt;i&gt;have any rules.&lt;/i&gt;

00:21:32.160 --> 00:21:33.161
&lt;i&gt;It learns through experience.&lt;/i&gt;

00:21:35.747 --> 00:21:38.416
&lt;i&gt;The next stage&lt;/i&gt;
&lt;i&gt;was to make it more general,&lt;/i&gt;

00:21:38.499 --> 00:21:40.626
&lt;i&gt;so that it could play&lt;/i&gt;
&lt;i&gt;any two-player game.&lt;/i&gt;

00:21:40.710 --> 00:21:42.045
&lt;i&gt;Things like chess,&lt;/i&gt;

00:21:42.128 --> 00:21:43.755
&lt;i&gt;and in fact,&lt;/i&gt;
&lt;i&gt;any kind of two-player&lt;/i&gt;

00:21:43.838 --> 00:21:44.839
&lt;i&gt;perfect information game.&lt;/i&gt;

00:21:44.922 --> 00:21:46.174
It's going really well.

00:21:46.257 --> 00:21:47.383
It's going
really, really well.

00:21:47.467 --> 00:21:49.677
-Oh, wow.
-It's going down, like fast.

00:21:49.761 --> 00:21:52.638
HASSABIS: &lt;i&gt;AlphaGo used&lt;/i&gt;
&lt;i&gt;to take a few months to train,&lt;/i&gt;

00:21:52.722 --> 00:21:55.183
&lt;i&gt;but AlphaZero could start&lt;/i&gt;
&lt;i&gt;in the morning&lt;/i&gt;

00:21:55.266 --> 00:21:56.517
&lt;i&gt;playing completely randomly&lt;/i&gt;

00:21:57.435 --> 00:22:00.146
&lt;i&gt;and then by tea&lt;/i&gt;
&lt;i&gt;be at superhuman level.&lt;/i&gt;

00:22:00.772 --> 00:22:02.815
&lt;i&gt;And by dinner it will be&lt;/i&gt;
&lt;i&gt;the strongest chess entity&lt;/i&gt;

00:22:02.899 --> 00:22:04.275
&lt;i&gt;there's ever been.&lt;/i&gt;

00:22:04.359 --> 00:22:06.402
-Amazing, it's amazing.
-Yeah.

00:22:06.486 --> 00:22:09.030
It's discovered its own
attacking style, you know,

00:22:09.113 --> 00:22:11.032
to take on the current
level of defense.

00:22:11.115 --> 00:22:12.408
I mean, I never
in my wildest dreams...

00:22:12.492 --> 00:22:14.577
I agree. Actually, I was not
expecting that either.

00:22:14.660 --> 00:22:15.870
And it's fun for me.

00:22:15.953 --> 00:22:18.081
I mean, it's inspired me
to get back into chess again,

00:22:18.164 --> 00:22:19.707
because it's cool to see

00:22:19.791 --> 00:22:21.709
&lt;i&gt;that there's even more depth&lt;/i&gt;
&lt;i&gt;than we thought in chess.&lt;/i&gt;

00:22:23.961 --> 00:22:24.962
(HORN BLOWS)

00:22:31.427 --> 00:22:33.721
HASSABIS: &lt;i&gt;I actually got&lt;/i&gt;
&lt;i&gt;into AI through games.&lt;/i&gt;

00:22:35.181 --> 00:22:36.849
&lt;i&gt;Initially, it was board games.&lt;/i&gt;

00:22:37.642 --> 00:22:39.686
&lt;i&gt;I was thinking,&lt;/i&gt;
&lt;i&gt;"How is my brain doing this?"&lt;/i&gt;

00:22:39.769 --> 00:22:41.229
&lt;i&gt;Like, what is it doing?&lt;/i&gt;

00:22:43.106 --> 00:22:45.233
&lt;i&gt;I was very aware of that&lt;/i&gt;
&lt;i&gt;from a very young age.&lt;/i&gt;

00:22:46.901 --> 00:22:48.986
&lt;i&gt;So I've always been thinking&lt;/i&gt;
&lt;i&gt;about thinking.&lt;/i&gt;

00:22:49.737 --> 00:22:52.281
NEWSREADER: &lt;i&gt;The British&lt;/i&gt;
&lt;i&gt;and American chess champions&lt;/i&gt;

00:22:52.365 --> 00:22:54.617
&lt;i&gt;meet to begin&lt;/i&gt;
&lt;i&gt;a series of matches.&lt;/i&gt;

00:22:54.701 --> 00:22:56.285
&lt;i&gt;Playing alongside them&lt;/i&gt;
&lt;i&gt;are the cream&lt;/i&gt;

00:22:56.369 --> 00:22:58.663
&lt;i&gt;of Britain and America's&lt;/i&gt;
&lt;i&gt;youngest players.&lt;/i&gt;

00:22:58.746 --> 00:23:01.040
NEWSREADER 2: &lt;i&gt;Demis Hassabis&lt;/i&gt;
&lt;i&gt;is representing Britain.&lt;/i&gt;

00:23:05.837 --> 00:23:07.422
COSTAS HASSABIS:
&lt;i&gt;When Demis was four,&lt;/i&gt;

00:23:07.505 --> 00:23:10.717
he first showed
an aptitude for chess.

00:23:12.301 --> 00:23:13.636
&lt;i&gt;By the time he was six,&lt;/i&gt;

00:23:13.720 --> 00:23:16.264
&lt;i&gt;he became London&lt;/i&gt;
&lt;i&gt;under-eight champion.&lt;/i&gt;

00:23:17.724 --> 00:23:18.975
HASSABIS: &lt;i&gt;My parents&lt;/i&gt;
&lt;i&gt;were very interesting&lt;/i&gt;

00:23:19.058 --> 00:23:20.184
&lt;i&gt;and unusual, actually.&lt;/i&gt;

00:23:20.268 --> 00:23:22.603
&lt;i&gt;I'd probably describe them&lt;/i&gt;
&lt;i&gt;as quite bohemian.&lt;/i&gt;

00:23:23.354 --> 00:23:24.981
&lt;i&gt;My father&lt;/i&gt;
&lt;i&gt;was a singer-songwriter&lt;/i&gt;

00:23:25.064 --> 00:23:26.232
&lt;i&gt;when he was younger,&lt;/i&gt;

00:23:26.315 --> 00:23:27.817
&lt;i&gt;and Bob Dylan was his hero.&lt;/i&gt;

00:23:29.235 --> 00:23:32.405
&lt;i&gt;Around when I was about eight,&lt;/i&gt;
&lt;i&gt;my dad got a camper van.&lt;/i&gt;

00:23:32.488 --> 00:23:33.489
(HORN HONKS)

00:23:34.073 --> 00:23:35.533
(ANGELA HASSABIS SPEAKING)

00:23:37.744 --> 00:23:38.745
Yeah, yeah.

00:23:41.372 --> 00:23:43.166
HOST: What is it
that you like about this game?

00:23:44.667 --> 00:23:46.544
It's just a good
thinking game.

00:23:48.880 --> 00:23:50.590
HASSABIS: &lt;i&gt;At the time,&lt;/i&gt;
&lt;i&gt;I was the second-highest rated&lt;/i&gt;

00:23:50.673 --> 00:23:52.216
&lt;i&gt;chess player in the world&lt;/i&gt;
&lt;i&gt;for my age.&lt;/i&gt;

00:23:52.300 --> 00:23:53.968
&lt;i&gt;But although I was on track&lt;/i&gt;

00:23:54.052 --> 00:23:55.511
&lt;i&gt;to be a professional&lt;/i&gt;
&lt;i&gt;chess player,&lt;/i&gt;

00:23:55.595 --> 00:23:57.180
&lt;i&gt;I thought that was what&lt;/i&gt;
&lt;i&gt;I was going to do.&lt;/i&gt;

00:23:57.263 --> 00:23:58.806
&lt;i&gt;No matter how much&lt;/i&gt;
&lt;i&gt;I loved the game,&lt;/i&gt;

00:23:58.890 --> 00:24:00.850
&lt;i&gt;it was incredibly stressful.&lt;/i&gt;

00:24:00.933 --> 00:24:03.019
&lt;i&gt;Definitely was not fun&lt;/i&gt;
&lt;i&gt;and games for me.&lt;/i&gt;

00:24:03.102 --> 00:24:04.812
Parents used to, you know,

00:24:04.896 --> 00:24:06.481
get very upset
when I lost the game

00:24:06.564 --> 00:24:09.233
and angry
if I forgot something.

00:24:10.485 --> 00:24:12.028
And because it was quite high
stakes for them, you know,

00:24:12.111 --> 00:24:13.654
it cost a lot of money
to go to these tournaments.

00:24:13.738 --> 00:24:15.281
And my parents
didn't have much money.

00:24:18.159 --> 00:24:19.452
&lt;i&gt;My parents thought, you know,&lt;/i&gt;

00:24:19.535 --> 00:24:21.621
&lt;i&gt;"If you interested&lt;/i&gt;
&lt;i&gt;in being a chess professional,&lt;/i&gt;

00:24:21.704 --> 00:24:24.665
&lt;i&gt;"this is really important.&lt;/i&gt;
&lt;i&gt;It's like your exams."&lt;/i&gt;

00:24:26.918 --> 00:24:28.961
&lt;i&gt;I remember&lt;/i&gt;
&lt;i&gt;I was about 12-years-old&lt;/i&gt;

00:24:29.712 --> 00:24:31.631
&lt;i&gt;and I was at this&lt;/i&gt;
&lt;i&gt;international chess tournament&lt;/i&gt;

00:24:31.714 --> 00:24:33.675
&lt;i&gt;in Liechtenstein&lt;/i&gt;
&lt;i&gt;up in the mountains.&lt;/i&gt;

00:24:36.052 --> 00:24:38.763
(BELL TOLLING)

00:24:42.975 --> 00:24:44.227
&lt;i&gt;And we were in this&lt;/i&gt;
&lt;i&gt;huge church hall&lt;/i&gt;

00:24:46.854 --> 00:24:47.855
&lt;i&gt;with, you know,&lt;/i&gt;

00:24:47.939 --> 00:24:49.440
&lt;i&gt;hundreds of international&lt;/i&gt;
&lt;i&gt;chess players.&lt;/i&gt;

00:24:52.068 --> 00:24:54.362
&lt;i&gt;And I was playing&lt;/i&gt;
&lt;i&gt;the ex-Danish champion.&lt;/i&gt;

00:24:55.655 --> 00:24:57.949
&lt;i&gt;He must have been&lt;/i&gt;
&lt;i&gt;in his 30s, probably.&lt;/i&gt;

00:25:00.076 --> 00:25:01.911
&lt;i&gt;In those days,&lt;/i&gt;
&lt;i&gt;there was a long time limit.&lt;/i&gt;

00:25:02.620 --> 00:25:03.871
&lt;i&gt;The games could&lt;/i&gt;
&lt;i&gt;literally last all day.&lt;/i&gt;

00:25:05.081 --> 00:25:06.791
(YAWNS)

00:25:08.001 --> 00:25:09.002
(TIMER TICKING)

00:25:09.085 --> 00:25:10.336
&lt;i&gt;We were into our tenth hour.&lt;/i&gt;

00:25:10.420 --> 00:25:11.838
(TIMER TICKS FRANTICALLY)

00:25:19.887 --> 00:25:21.973
&lt;i&gt;And we were in this&lt;/i&gt;
&lt;i&gt;incredibly unusual ending.&lt;/i&gt;

00:25:22.724 --> 00:25:24.100
&lt;i&gt;I think it should be a draw.&lt;/i&gt;

00:25:26.102 --> 00:25:28.229
&lt;i&gt;But he kept on trying&lt;/i&gt;
&lt;i&gt;to win for hours.&lt;/i&gt;

00:25:31.858 --> 00:25:32.859
(HORSE NEIGHS)

00:25:35.278 --> 00:25:37.572
&lt;i&gt;Finally, he tried&lt;/i&gt;
&lt;i&gt;one last cheap trick.&lt;/i&gt;

00:25:42.243 --> 00:25:44.162
&lt;i&gt;All I had to do&lt;/i&gt;
&lt;i&gt;was give away my queen.&lt;/i&gt;

00:25:44.245 --> 00:25:45.246
&lt;i&gt;Then it would be stalemate.&lt;/i&gt;

00:25:46.622 --> 00:25:47.665
&lt;i&gt;But I was so tired,&lt;/i&gt;

00:25:47.749 --> 00:25:49.584
&lt;i&gt;I thought it was inevitable&lt;/i&gt;
&lt;i&gt;I was going to be checkmated.&lt;/i&gt;

00:25:51.961 --> 00:25:53.004
&lt;i&gt;And so I resigned.&lt;/i&gt;

00:25:56.924 --> 00:25:59.010
&lt;i&gt;He jumped up.&lt;/i&gt;
&lt;i&gt;Just started laughing.&lt;/i&gt;

00:25:59.093 --> 00:26:00.094
(LAUGHING)

00:26:01.304 --> 00:26:02.347
&lt;i&gt;And he went,&lt;/i&gt;

00:26:02.430 --> 00:26:03.765
&lt;i&gt;"Why have you resigned?&lt;/i&gt;
&lt;i&gt;It's a draw."&lt;/i&gt;

00:26:03.848 --> 00:26:04.932
&lt;i&gt;And he immediately,&lt;/i&gt;
&lt;i&gt;with a flourish,&lt;/i&gt;

00:26:05.016 --> 00:26:06.225
&lt;i&gt;sort of showed me&lt;/i&gt;
&lt;i&gt;the drawing move.&lt;/i&gt;

00:26:08.811 --> 00:26:10.396
&lt;i&gt;I felt so sick to my stomach.&lt;/i&gt;

00:26:12.315 --> 00:26:13.691
&lt;i&gt;It made me think of&lt;/i&gt;
&lt;i&gt;the rest of that tournament.&lt;/i&gt;

00:26:13.775 --> 00:26:15.485
&lt;i&gt;Like, are we wasting&lt;/i&gt;
&lt;i&gt;our minds?&lt;/i&gt;

00:26:16.402 --> 00:26:19.405
&lt;i&gt;Is this the best use&lt;/i&gt;
&lt;i&gt;of all this brain power?&lt;/i&gt;

00:26:19.489 --> 00:26:21.449
&lt;i&gt;Everybody's, collectively,&lt;/i&gt;
&lt;i&gt;in that building?&lt;/i&gt;

00:26:22.075 --> 00:26:23.618
&lt;i&gt;If you could somehow plug in&lt;/i&gt;

00:26:23.701 --> 00:26:26.537
&lt;i&gt;those 300 brains&lt;/i&gt;
&lt;i&gt;into a system,&lt;/i&gt;

00:26:27.580 --> 00:26:28.581
&lt;i&gt;you might be able&lt;/i&gt;
&lt;i&gt;to solve cancer&lt;/i&gt;

00:26:28.664 --> 00:26:29.957
&lt;i&gt;with that level&lt;/i&gt;
&lt;i&gt;of brain power.&lt;/i&gt;

00:26:31.376 --> 00:26:33.211
&lt;i&gt;This intuitive feeling&lt;/i&gt;
&lt;i&gt;came over me&lt;/i&gt;

00:26:33.294 --> 00:26:34.837
&lt;i&gt;that although I love chess,&lt;/i&gt;

00:26:34.921 --> 00:26:37.632
&lt;i&gt;this is not the right thing&lt;/i&gt;
&lt;i&gt;to spend my whole life on.&lt;/i&gt;

00:26:51.187 --> 00:26:52.647
LEGG: &lt;i&gt;Demis and myself,&lt;/i&gt;

00:26:52.730 --> 00:26:55.650
&lt;i&gt;our plan was always&lt;/i&gt;
&lt;i&gt;to fill DeepMind&lt;/i&gt;

00:26:55.733 --> 00:26:56.818
&lt;i&gt;with some of the most&lt;/i&gt;

00:26:56.901 --> 00:26:58.069
&lt;i&gt;brilliant scientists&lt;/i&gt;
&lt;i&gt;in the world.&lt;/i&gt;

00:26:58.861 --> 00:27:00.571
&lt;i&gt;So we had the human brains&lt;/i&gt;

00:27:00.655 --> 00:27:03.282
&lt;i&gt;necessary to create&lt;/i&gt;
&lt;i&gt;an AGI system.&lt;/i&gt;

00:27:04.742 --> 00:27:07.870
&lt;i&gt;By definition, the "G"&lt;/i&gt;
&lt;i&gt;in AGI is about generality.&lt;/i&gt;

00:27:08.955 --> 00:27:12.667
&lt;i&gt;What I imagine is being able&lt;/i&gt;
&lt;i&gt;to talk to an agent,&lt;/i&gt;

00:27:12.750 --> 00:27:13.751
&lt;i&gt;the agent can talk back,&lt;/i&gt;

00:27:14.836 --> 00:27:18.423
&lt;i&gt;and the agent is able to solve&lt;/i&gt;
&lt;i&gt;novel problems&lt;/i&gt;

00:27:18.506 --> 00:27:19.632
&lt;i&gt;that it hasn't seen before.&lt;/i&gt;

00:27:20.299 --> 00:27:22.468
&lt;i&gt;That's a really key part&lt;/i&gt;
&lt;i&gt;of human intelligence,&lt;/i&gt;

00:27:22.552 --> 00:27:24.095
&lt;i&gt;and it's that&lt;/i&gt;
&lt;i&gt;cognitive breadth&lt;/i&gt;

00:27:24.178 --> 00:27:26.139
&lt;i&gt;and flexibility&lt;/i&gt;
&lt;i&gt;that's incredible.&lt;/i&gt;

00:27:27.515 --> 00:27:29.142
&lt;i&gt;The only natural&lt;/i&gt;
&lt;i&gt;general intelligence&lt;/i&gt;

00:27:29.225 --> 00:27:30.226
&lt;i&gt;we know of as humans,&lt;/i&gt;

00:27:30.309 --> 00:27:31.728
&lt;i&gt;we obviously learn a lot&lt;/i&gt;
&lt;i&gt;from our environment.&lt;/i&gt;

00:27:33.104 --> 00:27:35.606
&lt;i&gt;So we think that&lt;/i&gt;
&lt;i&gt;simulated environments&lt;/i&gt;

00:27:35.690 --> 00:27:38.151
&lt;i&gt;are one of the ways&lt;/i&gt;
&lt;i&gt;to create an AGI.&lt;/i&gt;

00:27:40.361 --> 00:27:41.404
SIMON CARTER:
&lt;i&gt;The very early humans&lt;/i&gt;

00:27:42.030 --> 00:27:43.906
&lt;i&gt;were having to solve&lt;/i&gt;
&lt;i&gt;logic problems.&lt;/i&gt;

00:27:43.990 --> 00:27:46.451
&lt;i&gt;They were having to solve&lt;/i&gt;
&lt;i&gt;navigation, memory,&lt;/i&gt;

00:27:46.534 --> 00:27:48.453
&lt;i&gt;and we evolved&lt;/i&gt;
&lt;i&gt;in that environment.&lt;/i&gt;

00:27:49.954 --> 00:27:52.165
If we can create
a virtual recreation

00:27:52.248 --> 00:27:54.334
of that kind of environment,

00:27:54.417 --> 00:27:55.960
that's the perfect
testing ground

00:27:56.044 --> 00:27:57.086
and training ground

00:27:57.170 --> 00:27:58.755
for everything
we do at DeepMind.

00:28:03.843 --> 00:28:05.386
GUY SIMMONS:
&lt;i&gt;What they were doing here&lt;/i&gt;

00:28:05.470 --> 00:28:08.806
&lt;i&gt;was creating environments&lt;/i&gt;
&lt;i&gt;for childlike beings,&lt;/i&gt;

00:28:08.890 --> 00:28:11.184
&lt;i&gt;the agents to exist&lt;/i&gt;
&lt;i&gt;within and play.&lt;/i&gt;

00:28:12.101 --> 00:28:13.186
That just sounded like

00:28:13.269 --> 00:28:15.271
the most interesting thing
in all the world.

00:28:16.522 --> 00:28:18.649
SHANAHAN: &lt;i&gt;A child&lt;/i&gt;
&lt;i&gt;learns by tearing things up&lt;/i&gt;

00:28:18.733 --> 00:28:20.318
&lt;i&gt;and then throwing food around&lt;/i&gt;

00:28:20.401 --> 00:28:22.362
&lt;i&gt;and getting a response&lt;/i&gt;
&lt;i&gt;from mommy or daddy.&lt;/i&gt;

00:28:23.029 --> 00:28:25.323
This seems like an important
idea to incorporate

00:28:25.406 --> 00:28:26.949
in the way you train an agent.

00:28:27.658 --> 00:28:29.827
RESEARCHER 1: The humanoid
is supposed to stand up.

00:28:30.620 --> 00:28:32.455
As his center
of gravity rises,

00:28:32.538 --> 00:28:33.998
it gets more points.

00:28:37.126 --> 00:28:38.544
You have a reward

00:28:38.628 --> 00:28:40.505
and the agent
learns from the reward,

00:28:40.588 --> 00:28:42.965
&lt;i&gt;like, you do something well,&lt;/i&gt;
&lt;i&gt;you get a positive reward.&lt;/i&gt;

00:28:43.049 --> 00:28:46.010
&lt;i&gt;You do something bad,&lt;/i&gt;
&lt;i&gt;you get a negative reward.&lt;/i&gt;

00:28:47.178 --> 00:28:48.805
RESEARCHER 2: (EXCLAIMS)
It looks like it's standing.

00:28:50.390 --> 00:28:51.724
It's still a bit drunk.

00:28:51.808 --> 00:28:53.309
RESEARCHER 1:
It likes to walk backwards.

00:28:53.393 --> 00:28:54.394
RESEARCHER 2: (CHUCKLES) Yeah.

00:28:54.977 --> 00:28:57.146
The whole algorithm
is trying to optimize

00:28:57.230 --> 00:28:59.357
for receiving as much rewards
as possible,

00:28:59.440 --> 00:29:01.859
and it's found that
walking backwards,

00:29:01.943 --> 00:29:04.779
it's good enough
to get very good scores.

00:29:07.448 --> 00:29:08.658
RAIA HADSELL:
&lt;i&gt;When we learn to navigate,&lt;/i&gt;

00:29:09.283 --> 00:29:10.827
&lt;i&gt;when we learn to get around&lt;/i&gt;
&lt;i&gt;in our world,&lt;/i&gt;

00:29:10.910 --> 00:29:12.495
&lt;i&gt;we don't start with maps.&lt;/i&gt;

00:29:13.162 --> 00:29:15.373
&lt;i&gt;We just start&lt;/i&gt;
&lt;i&gt;with our own exploration,&lt;/i&gt;

00:29:15.957 --> 00:29:17.542
adventuring off
across the park,

00:29:17.625 --> 00:29:20.461
without our parents
by our side,

00:29:21.671 --> 00:29:23.756
or finding our way home
from school when we're young.

00:29:23.965 --> 00:29:24.966
(FAST ELECTRONIC
MUSIC PLAYING)

00:29:26.634 --> 00:29:28.344
HADSELL: &lt;i&gt;A few of us&lt;/i&gt;
&lt;i&gt;came up with this idea&lt;/i&gt;

00:29:28.428 --> 00:29:31.472
&lt;i&gt;that if we had an environment&lt;/i&gt;
&lt;i&gt;where a simulated robot&lt;/i&gt;

00:29:31.556 --> 00:29:33.349
&lt;i&gt;just had to run forward,&lt;/i&gt;

00:29:33.433 --> 00:29:35.852
&lt;i&gt;we could put all sorts of&lt;/i&gt;
&lt;i&gt;obstacles in its way&lt;/i&gt;

00:29:35.935 --> 00:29:37.770
&lt;i&gt;and see if it could manage&lt;/i&gt;
&lt;i&gt;to navigate&lt;/i&gt;

00:29:37.854 --> 00:29:39.522
&lt;i&gt;different types of terrain.&lt;/i&gt;

00:29:40.314 --> 00:29:42.650
The idea would be like
a parkour challenge.

00:29:46.029 --> 00:29:47.405
&lt;i&gt;It's not graceful,&lt;/i&gt;

00:29:48.489 --> 00:29:51.284
&lt;i&gt;but was never trained to hold&lt;/i&gt;
&lt;i&gt;a glass whilst it was running&lt;/i&gt;

00:29:51.367 --> 00:29:52.660
&lt;i&gt;and not spill water.&lt;/i&gt;

00:29:53.745 --> 00:29:55.329
You set this objective
that says,

00:29:55.413 --> 00:29:57.790
"Just move forward,
forward velocity,

00:29:57.874 --> 00:29:59.083
"and you'll get
a reward for that."

00:30:00.293 --> 00:30:02.295
&lt;i&gt;And the learning algorithm&lt;/i&gt;
&lt;i&gt;figures out&lt;/i&gt;

00:30:02.378 --> 00:30:04.797
&lt;i&gt;how to move&lt;/i&gt;
&lt;i&gt;this complex set of joints.&lt;/i&gt;

00:30:05.715 --> 00:30:06.924
&lt;i&gt;That's the power of&lt;/i&gt;

00:30:07.008 --> 00:30:08.968
&lt;i&gt;reward-based&lt;/i&gt;
&lt;i&gt;reinforcement learning.&lt;/i&gt;

00:30:09.886 --> 00:30:12.388
SILVER: &lt;i&gt;Our goal&lt;/i&gt;
&lt;i&gt;is to try and build agents&lt;/i&gt;

00:30:12.472 --> 00:30:15.433
&lt;i&gt;which, we drop them in,&lt;/i&gt;
&lt;i&gt;they know nothing,&lt;/i&gt;

00:30:15.516 --> 00:30:18.186
&lt;i&gt;they get to play around in&lt;/i&gt;
&lt;i&gt;whatever problem you give them&lt;/i&gt;

00:30:18.269 --> 00:30:20.897
&lt;i&gt;and eventually figure out how&lt;/i&gt;
&lt;i&gt;to solve it for themselves.&lt;/i&gt;

00:30:21.981 --> 00:30:24.484
Now we want something
which can do that

00:30:24.567 --> 00:30:26.778
in as many different types
of problems as possible.

00:30:29.030 --> 00:30:32.367
&lt;i&gt;A human needs diverse skills&lt;/i&gt;
&lt;i&gt;to interact with the world.&lt;/i&gt;

00:30:32.742 --> 00:30:34.744
&lt;i&gt;How to deal&lt;/i&gt;
&lt;i&gt;with complex images,&lt;/i&gt;

00:30:34.827 --> 00:30:37.538
&lt;i&gt;how to manipulate&lt;/i&gt;
&lt;i&gt;thousands of things at once,&lt;/i&gt;

00:30:37.622 --> 00:30:39.540
&lt;i&gt;how to deal&lt;/i&gt;
&lt;i&gt;with missing information.&lt;/i&gt;

00:30:40.083 --> 00:30:41.709
&lt;i&gt;We think all of these things&lt;/i&gt;
&lt;i&gt;together&lt;/i&gt;

00:30:41.793 --> 00:30:43.920
&lt;i&gt;are represented&lt;/i&gt;
&lt;i&gt;by this game called&lt;/i&gt; StarCraft.

00:30:44.921 --> 00:30:46.839
All it's being trained
to do is,

00:30:46.923 --> 00:30:50.051
given this situation,
this screen,

00:30:50.134 --> 00:30:51.427
what would a human do?

00:30:51.511 --> 00:30:54.764
&lt;i&gt;We took inspiration from&lt;/i&gt;
&lt;i&gt;large language models&lt;/i&gt;

00:30:54.847 --> 00:30:57.308
where you simply train
a model

00:30:57.392 --> 00:30:58.976
to predict the next word,

00:31:03.439 --> 00:31:05.024
&lt;i&gt;which is exactly the same as&lt;/i&gt;

00:31:05.108 --> 00:31:07.402
&lt;i&gt;predict the next&lt;/i&gt;
StarCraft &lt;i&gt;move.&lt;/i&gt;

00:31:07.485 --> 00:31:08.486
SILVER: &lt;i&gt;Unlike chess or Go,&lt;/i&gt;

00:31:08.569 --> 00:31:10.947
&lt;i&gt;where players take turns&lt;/i&gt;
&lt;i&gt;to make moves,&lt;/i&gt;

00:31:11.030 --> 00:31:13.741
&lt;i&gt;in&lt;/i&gt; StarCraft &lt;i&gt;there's a&lt;/i&gt;
&lt;i&gt;continuous flow of decisions.&lt;/i&gt;

00:31:14.659 --> 00:31:15.660
&lt;i&gt;On top of that,&lt;/i&gt;

00:31:15.743 --> 00:31:17.453
&lt;i&gt;you can't even see&lt;/i&gt;
&lt;i&gt;what the opponent is doing.&lt;/i&gt;

00:31:18.371 --> 00:31:20.540
&lt;i&gt;There is no longer&lt;/i&gt;
&lt;i&gt;a clear definition&lt;/i&gt;

00:31:20.623 --> 00:31:21.833
of what it means
to play the best way.

00:31:21.916 --> 00:31:23.001
It depends on
what your opponent does.

00:31:23.668 --> 00:31:25.128
HADSELL: &lt;i&gt;This is the way&lt;/i&gt;
&lt;i&gt;that we'll get to&lt;/i&gt;

00:31:25.211 --> 00:31:27.046
&lt;i&gt;a much more fluid,&lt;/i&gt;

00:31:27.130 --> 00:31:30.299
more natural, faster,
more reactive agent.

00:31:31.342 --> 00:31:32.760
ORIOL VINYALS:
&lt;i&gt;This is a huge challenge&lt;/i&gt;

00:31:32.844 --> 00:31:34.971
&lt;i&gt;and let's see how far&lt;/i&gt;
&lt;i&gt;we can push.&lt;/i&gt;

00:31:35.054 --> 00:31:36.097
TIM LILLICRAP: Oh!

00:31:36.180 --> 00:31:37.724
Holy monkey!

00:31:37.807 --> 00:31:39.934
&lt;i&gt;I'm a pretty&lt;/i&gt;
&lt;i&gt;low-level amateur.&lt;/i&gt;

00:31:40.018 --> 00:31:42.520
I'm okay, but I'm
a pretty low-level amateur.

00:31:42.603 --> 00:31:45.023
These agents have
a long ways to go.

00:31:45.732 --> 00:31:47.567
HASSABIS: &lt;i&gt;We couldn't&lt;/i&gt;
&lt;i&gt;beat someone of Tim's level.&lt;/i&gt;

00:31:48.192 --> 00:31:49.986
&lt;i&gt;You know, that was&lt;/i&gt;
&lt;i&gt;a little bit alarming.&lt;/i&gt;

00:31:50.069 --> 00:31:51.070
LILLICRAP:
&lt;i&gt;At that point, it felt like&lt;/i&gt;

00:31:51.154 --> 00:31:53.072
&lt;i&gt;it was going to be, like,&lt;/i&gt;
&lt;i&gt;a really big long challenge,&lt;/i&gt;

00:31:53.156 --> 00:31:54.574
&lt;i&gt;maybe a couple of years.&lt;/i&gt;

00:31:57.910 --> 00:32:00.872
VINYALS: &lt;i&gt;Dani is the best&lt;/i&gt;
DeepMind StarCraft 2 &lt;i&gt;player.&lt;/i&gt;

00:32:01.539 --> 00:32:04.709
I've been playing the agent
every day for a few weeks now.

00:32:06.711 --> 00:32:08.588
&lt;i&gt;I could feel that the agent&lt;/i&gt;

00:32:08.671 --> 00:32:10.590
&lt;i&gt;was getting better&lt;/i&gt;
&lt;i&gt;really fast.&lt;/i&gt;

00:32:10.673 --> 00:32:12.717
(CHEERING, LAUGHTER)

00:32:13.134 --> 00:32:14.719
Wow, we beat Danny.
That, for me,

00:32:14.802 --> 00:32:16.429
was already
like a huge achievement.

00:32:17.847 --> 00:32:18.890
HASSABIS: &lt;i&gt;The next step is&lt;/i&gt;

00:32:18.973 --> 00:32:21.142
&lt;i&gt;we're going to book in&lt;/i&gt;
&lt;i&gt;a pro to play.&lt;/i&gt;

00:32:22.602 --> 00:32:23.770
(KEYBOARD TAPPING)

00:32:26.522 --> 00:32:27.774
(GROANS)

00:32:27.857 --> 00:32:29.901
(CHEERING, WHOOPING)

00:32:32.487 --> 00:32:35.156
(CHEERING, WHOOPING)

00:32:35.239 --> 00:32:37.325
-(LAUGHS)
-(PEOPLE CLAPPING)

00:32:41.704 --> 00:32:43.498
It feels a bit unfair.
All you guys against me.

00:32:43.581 --> 00:32:44.624
(ALL LAUGH)

00:32:45.375 --> 00:32:46.584
HASSABIS: &lt;i&gt;We're way&lt;/i&gt;
&lt;i&gt;ahead of what I thought&lt;/i&gt;

00:32:46.668 --> 00:32:48.586
&lt;i&gt;we would do, given where&lt;/i&gt;
&lt;i&gt;we were two months ago.&lt;/i&gt;

00:32:49.045 --> 00:32:50.463
Just trying to digest it all,
actually.

00:32:50.546 --> 00:32:51.881
But it's very, very cool.

00:32:52.632 --> 00:32:53.841
SILVER: &lt;i&gt;Now we're in&lt;/i&gt;
&lt;i&gt;a position where&lt;/i&gt;

00:32:53.925 --> 00:32:55.551
&lt;i&gt;we can finally share&lt;/i&gt;
&lt;i&gt;the work that we've done&lt;/i&gt;

00:32:55.635 --> 00:32:56.678
&lt;i&gt;with the public.&lt;/i&gt;

00:32:56.761 --> 00:32:57.762
This is a big step.

00:32:57.845 --> 00:32:59.681
We are really putting
ourselves on the line here.

00:33:00.348 --> 00:33:02.183
-Take it away. Cheers.
-Thank you.

00:33:02.266 --> 00:33:04.060
We're going to be live
from London.

00:33:04.143 --> 00:33:05.144
It's happening.

00:33:08.523 --> 00:33:10.149
ANNOUNCER 1:
&lt;i&gt;Welcome to London.&lt;/i&gt;

00:33:10.233 --> 00:33:12.777
&lt;i&gt;We are going to have&lt;/i&gt;
&lt;i&gt;a live exhibition match,&lt;/i&gt;

00:33:12.860 --> 00:33:14.862
&lt;i&gt;MaNa against AlphaStar.&lt;/i&gt;

00:33:14.946 --> 00:33:16.739
(CHEERING, APPLAUSE)

00:33:17.907 --> 00:33:19.659
&lt;i&gt;At this point now,&lt;/i&gt;

00:33:19.742 --> 00:33:23.454
&lt;i&gt;AlphaStar, 10 and 0&lt;/i&gt;
&lt;i&gt;against professional gamers.&lt;/i&gt;

00:33:23.538 --> 00:33:25.498
&lt;i&gt;Any thoughts&lt;/i&gt;
&lt;i&gt;before we get into this game?&lt;/i&gt;

00:33:25.581 --> 00:33:27.041
VINYALS: &lt;i&gt;I just want to see&lt;/i&gt;
&lt;i&gt;a good game, yeah.&lt;/i&gt;

00:33:27.125 --> 00:33:28.167
&lt;i&gt;I want to see a good game.&lt;/i&gt;

00:33:28.251 --> 00:33:29.794
SILVER: &lt;i&gt;Absolutely,&lt;/i&gt;
&lt;i&gt;good game. We're all excited.&lt;/i&gt;

00:33:29.877 --> 00:33:32.547
ANNOUNCER: &lt;i&gt;All right. Let's&lt;/i&gt;
&lt;i&gt;see what MaNa can pull off.&lt;/i&gt;

00:33:34.882 --> 00:33:35.925
ANNOUNCER 2:
&lt;i&gt;AlphaStar is definitely&lt;/i&gt;

00:33:36.009 --> 00:33:37.427
&lt;i&gt;dominating the pace&lt;/i&gt;
&lt;i&gt;of this game.&lt;/i&gt;

00:33:37.969 --> 00:33:39.846
(SPORADIC CHEERING)

00:33:40.847 --> 00:33:43.641
ANNOUNCER 1: &lt;i&gt;Wow. AlphaStar&lt;/i&gt;
&lt;i&gt;is playing so smartly.&lt;/i&gt;

00:33:43.725 --> 00:33:45.309
(LAUGHTER)

00:33:46.477 --> 00:33:48.021
&lt;i&gt;This really looks like&lt;/i&gt;
&lt;i&gt;I'm watching&lt;/i&gt;

00:33:48.104 --> 00:33:49.564
&lt;i&gt;a professional human gamer&lt;/i&gt;

00:33:49.647 --> 00:33:50.857
&lt;i&gt;from the AlphaStar&lt;/i&gt;
&lt;i&gt;point of view.&lt;/i&gt;

00:33:52.525 --> 00:33:54.318
(KEYBOARD TAPPING)

00:33:57.071 --> 00:34:00.616
HASSABIS: &lt;i&gt;I hadn't really seen&lt;/i&gt;
&lt;i&gt;a pro play&lt;/i&gt; StarCraft &lt;i&gt;up close,&lt;/i&gt;

00:34:01.701 --> 00:34:03.202
&lt;i&gt;and the 800 clicks per minute.&lt;/i&gt;

00:34:03.286 --> 00:34:05.621
&lt;i&gt;I don't understand how anyone&lt;/i&gt;
&lt;i&gt;can even click 800 times,&lt;/i&gt;

00:34:05.705 --> 00:34:08.416
let alone doing
800 useful clicks.

00:34:09.083 --> 00:34:10.793
ANNOUNCER 1:
&lt;i&gt;Oh, another good hit.&lt;/i&gt;

00:34:10.877 --> 00:34:12.545
-(ALL GROAN)
&lt;i&gt;-AlphaStar is just&lt;/i&gt;

00:34:12.628 --> 00:34:14.172
&lt;i&gt;completely relentless.&lt;/i&gt;

00:34:14.255 --> 00:34:15.548
SILVER: &lt;i&gt;We need to be careful&lt;/i&gt;

00:34:15.631 --> 00:34:18.885
&lt;i&gt;because many of us grew up&lt;/i&gt;
&lt;i&gt;as gamers and are gamers.&lt;/i&gt;

00:34:18.968 --> 00:34:20.887
And so to us,
it's very natural

00:34:20.970 --> 00:34:23.389
to view games
as what they are,

00:34:23.473 --> 00:34:25.933
which is pure vehicles
for fun,

00:34:26.684 --> 00:34:29.312
&lt;i&gt;and not to see&lt;/i&gt;
&lt;i&gt;that more militaristic side&lt;/i&gt;

00:34:29.395 --> 00:34:31.314
&lt;i&gt;that the public might see&lt;/i&gt;
&lt;i&gt;if they looked at this.&lt;/i&gt;

00:34:32.565 --> 00:34:36.444
You can't look at gunpowder
and only make a firecracker.

00:34:37.403 --> 00:34:40.782
&lt;i&gt;All technologies inherently&lt;/i&gt;
&lt;i&gt;point into certain directions.&lt;/i&gt;

00:34:42.867 --> 00:34:44.243
MARGARET LEVI:
&lt;i&gt;I'm very worried about&lt;/i&gt;

00:34:44.327 --> 00:34:46.162
&lt;i&gt;the certain ways in which AI&lt;/i&gt;

00:34:46.245 --> 00:34:48.373
&lt;i&gt;will be used&lt;/i&gt;
&lt;i&gt;for military purposes.&lt;/i&gt;

00:34:51.042 --> 00:34:54.253
And that makes it even clearer
how important it is

00:34:54.837 --> 00:34:57.840
for our societies
to be in control

00:34:57.924 --> 00:35:00.134
&lt;i&gt;of these new technologies.&lt;/i&gt;

00:35:00.760 --> 00:35:03.971
The potential for abuse
from AI will be significant.

00:35:05.014 --> 00:35:08.434
&lt;i&gt;Wars that occur faster&lt;/i&gt;
&lt;i&gt;than humans can comprehend&lt;/i&gt;

00:35:08.518 --> 00:35:10.520
&lt;i&gt;and more powerful&lt;/i&gt;
&lt;i&gt;surveillance.&lt;/i&gt;

00:35:12.021 --> 00:35:14.607
How do you keep power forever

00:35:15.441 --> 00:35:17.985
over something that's
much more powerful than you?

00:35:19.028 --> 00:35:20.947
(STEPHEN HAWKING SPEAKING)

00:35:42.719 --> 00:35:45.179
Technologies can be used
to do terrible things.

00:35:47.265 --> 00:35:49.976
&lt;i&gt;And technology can be used&lt;/i&gt;
&lt;i&gt;to do wonderful things&lt;/i&gt;

00:35:50.059 --> 00:35:51.644
&lt;i&gt;and solve&lt;/i&gt;
&lt;i&gt;all kinds of problems.&lt;/i&gt;

00:35:53.146 --> 00:35:54.605
When DeepMind
was acquired by Google...

00:35:54.689 --> 00:35:56.107
-Yeah.
-...you got Google to promise

00:35:56.190 --> 00:35:57.692
that technology you developed
won't be used by the military

00:35:57.775 --> 00:35:58.901
-for surveillance.
-Right.

00:35:58.985 --> 00:35:59.986
-Yes.
-Tell us about that.

00:36:00.069 --> 00:36:02.822
I think technology
is neutral in itself,

00:36:02.905 --> 00:36:05.116
um, but how, you know,
we as a society

00:36:05.199 --> 00:36:07.118
or humans and companies
and other things,

00:36:07.201 --> 00:36:09.245
other entities and governments
decide to use it

00:36:09.328 --> 00:36:12.331
is what determines whether
things become good or bad.

00:36:12.415 --> 00:36:15.752
You know, I personally think
having autonomous weaponry

00:36:15.835 --> 00:36:17.086
is just a very bad idea.

00:36:18.921 --> 00:36:20.923
ANNOUNCER 1:
&lt;i&gt;AlphaStar is playing&lt;/i&gt;

00:36:21.007 --> 00:36:24.010
&lt;i&gt;an extremely intelligent game&lt;/i&gt;
&lt;i&gt;right now.&lt;/i&gt;

00:36:24.093 --> 00:36:26.846
CUKIER: &lt;i&gt;There is an element to&lt;/i&gt;
&lt;i&gt;what's being created&lt;/i&gt;

00:36:26.929 --> 00:36:28.473
&lt;i&gt;at DeepMind in London&lt;/i&gt;

00:36:28.556 --> 00:36:32.685
that does seem like
the Manhattan Project.

00:36:33.853 --> 00:36:37.106
&lt;i&gt;There's a relationship between&lt;/i&gt;
&lt;i&gt;Robert Oppenheimer&lt;/i&gt;

00:36:37.190 --> 00:36:38.983
&lt;i&gt;and Demis Hassabis&lt;/i&gt;

00:36:39.650 --> 00:36:43.863
&lt;i&gt;in which they're unleashing&lt;/i&gt;
&lt;i&gt;a new force upon humanity.&lt;/i&gt;

00:36:43.946 --> 00:36:45.656
ANNOUNCER 1:
&lt;i&gt;MaNa is fighting back, though.&lt;/i&gt;

00:36:45.740 --> 00:36:47.867
Oh, man!

00:36:47.950 --> 00:36:49.869
HASSABIS:
&lt;i&gt;I think that Oppenheimer&lt;/i&gt;

00:36:49.952 --> 00:36:52.246
&lt;i&gt;and some of the other leaders&lt;/i&gt;
&lt;i&gt;of that project got caught up&lt;/i&gt;

00:36:52.330 --> 00:36:54.499
&lt;i&gt;in the excitement&lt;/i&gt;
&lt;i&gt;of building the technology&lt;/i&gt;

00:36:54.582 --> 00:36:55.833
&lt;i&gt;and seeing if it was possible.&lt;/i&gt;

00:36:55.917 --> 00:36:57.377
ANNOUNCER 1:
&lt;i&gt;Where is AlphaStar?&lt;/i&gt;

00:36:58.127 --> 00:36:59.128
&lt;i&gt;Where is AlphaStar?&lt;/i&gt;

00:36:59.212 --> 00:37:01.547
&lt;i&gt;I don't see AlphaStar's units&lt;/i&gt;
&lt;i&gt;anywhere.&lt;/i&gt;

00:37:01.631 --> 00:37:03.257
HASSABIS: &lt;i&gt;They did not think&lt;/i&gt;
&lt;i&gt;carefully enough&lt;/i&gt;

00:37:03.341 --> 00:37:05.968
&lt;i&gt;about the morals of what&lt;/i&gt;
&lt;i&gt;they were doing early enough.&lt;/i&gt;

00:37:07.136 --> 00:37:08.554
&lt;i&gt;What we should do&lt;/i&gt;
&lt;i&gt;as scientists&lt;/i&gt;

00:37:08.638 --> 00:37:10.598
&lt;i&gt;with powerful new technologies&lt;/i&gt;

00:37:10.682 --> 00:37:13.184
&lt;i&gt;is try and understand it in&lt;/i&gt;
&lt;i&gt;controlled conditions first.&lt;/i&gt;

00:37:14.560 --> 00:37:15.937
ANNOUNCER 1: &lt;i&gt;And that is that.&lt;/i&gt;

00:37:16.437 --> 00:37:18.731
&lt;i&gt;MaNa has defeated AlphaStar.&lt;/i&gt;

00:37:29.325 --> 00:37:31.077
I mean, my honest feeling is
that I think it is

00:37:31.160 --> 00:37:32.870
a fair representation
of where we are.

00:37:32.954 --> 00:37:35.540
And I think that part feels...
feels okay.

00:37:35.623 --> 00:37:36.916
-I'm very happy for you.
-I'm happy.

00:37:37.000 --> 00:37:38.001
So well... well done.

00:37:38.084 --> 00:37:40.461
&lt;i&gt;My view is that the approach&lt;/i&gt;
&lt;i&gt;to building technology&lt;/i&gt;

00:37:40.545 --> 00:37:43.172
&lt;i&gt;which is embodied by&lt;/i&gt;
&lt;i&gt;move fast and break things,&lt;/i&gt;

00:37:43.256 --> 00:37:45.842
&lt;i&gt;is exactly what&lt;/i&gt;
&lt;i&gt;we should not be doing,&lt;/i&gt;

00:37:45.925 --> 00:37:47.552
&lt;i&gt;because you can't afford&lt;/i&gt;
&lt;i&gt;to break things&lt;/i&gt;

00:37:47.635 --> 00:37:48.678
&lt;i&gt;and then fix them afterwards.&lt;/i&gt;

00:37:48.761 --> 00:37:49.804
-Cheers.
-Thank you so much.

00:37:49.887 --> 00:37:51.597
Yeah, get... get some rest.
You did really well.

00:37:51.681 --> 00:37:53.349
-Cheers, yeah?
-Thank you for having us.

00:38:01.190 --> 00:38:02.817
(ELECTRONIC MUSIC PLAYING)

00:38:03.985 --> 00:38:04.986
HASSABIS: &lt;i&gt;When I was eight,&lt;/i&gt;

00:38:05.069 --> 00:38:06.362
&lt;i&gt;I bought my first computer&lt;/i&gt;

00:38:06.446 --> 00:38:08.364
&lt;i&gt;with the winnings&lt;/i&gt;
&lt;i&gt;from a chess tournam&lt;/i&gt;ent.

00:38:09.282 --> 00:38:10.783
&lt;i&gt;I sort of had this intuition&lt;/i&gt;

00:38:10.867 --> 00:38:13.244
&lt;i&gt;that computers&lt;/i&gt;
&lt;i&gt;are this magical device&lt;/i&gt;

00:38:13.327 --> 00:38:14.871
&lt;i&gt;that can extend&lt;/i&gt;
&lt;i&gt;the power of the mind.&lt;/i&gt;

00:38:15.621 --> 00:38:17.081
&lt;i&gt;I had a couple&lt;/i&gt;
&lt;i&gt;of school friends,&lt;/i&gt;

00:38:17.165 --> 00:38:18.833
&lt;i&gt;and we used to have&lt;/i&gt;
&lt;i&gt;a hacking club,&lt;/i&gt;

00:38:18.916 --> 00:38:21.336
&lt;i&gt;writing code, making games.&lt;/i&gt;

00:38:26.049 --> 00:38:27.383
&lt;i&gt;And then over&lt;/i&gt;
&lt;i&gt;the summer holidays,&lt;/i&gt;

00:38:27.467 --> 00:38:28.843
&lt;i&gt;I'd spend the whole day&lt;/i&gt;

00:38:28.926 --> 00:38:30.762
&lt;i&gt;flicking through&lt;/i&gt;
&lt;i&gt;games magazines.&lt;/i&gt;

00:38:31.429 --> 00:38:33.139
&lt;i&gt;And one day I noticed&lt;/i&gt;
&lt;i&gt;there was a competition&lt;/i&gt;

00:38:33.222 --> 00:38:35.433
&lt;i&gt;to write an original version&lt;/i&gt;
&lt;i&gt;of Space Invaders.&lt;/i&gt;

00:38:35.516 --> 00:38:37.935
&lt;i&gt;And the winner won a job&lt;/i&gt;
&lt;i&gt;at Bullfrog.&lt;/i&gt;

00:38:39.395 --> 00:38:41.773
&lt;i&gt;Bullfrog at the time was the&lt;/i&gt;
&lt;i&gt;best game development house&lt;/i&gt;

00:38:41.856 --> 00:38:43.358
&lt;i&gt;in all of Europe.&lt;/i&gt;

00:38:43.441 --> 00:38:44.984
&lt;i&gt;You know, I really wanted&lt;/i&gt;
&lt;i&gt;to work at this place&lt;/i&gt;

00:38:45.068 --> 00:38:46.778
&lt;i&gt;and see how they build games.&lt;/i&gt;

00:38:48.363 --> 00:38:49.906
NEWSCASTER: &lt;i&gt;Bullfrog,&lt;/i&gt;
&lt;i&gt;based here in Guildford,&lt;/i&gt;

00:38:49.989 --> 00:38:51.949
&lt;i&gt;began with a big idea.&lt;/i&gt;

00:38:52.033 --> 00:38:54.202
&lt;i&gt;That idea turned into the game&lt;/i&gt;
Populous,

00:38:54.285 --> 00:38:56.287
&lt;i&gt;which became&lt;/i&gt;
&lt;i&gt;a global bestseller.&lt;/i&gt;

00:38:56.371 --> 00:38:59.665
In the '90s, there was
no recruitment agencies.

00:38:59.749 --> 00:39:02.043
You couldn't go out and say,
you know,

00:39:02.126 --> 00:39:04.504
"Come and work
in the games industry."

00:39:04.587 --> 00:39:07.006
It was still not even
considered an industry.

00:39:08.007 --> 00:39:10.843
&lt;i&gt;So we came up with the idea&lt;/i&gt;
&lt;i&gt;to have a competition&lt;/i&gt;

00:39:10.927 --> 00:39:13.262
&lt;i&gt;and we got&lt;/i&gt;
&lt;i&gt;a lot of applicants.&lt;/i&gt;

00:39:14.263 --> 00:39:16.516
&lt;i&gt;And one of those was Demis's.&lt;/i&gt;

00:39:17.225 --> 00:39:19.644
I can still remember clearly

00:39:20.436 --> 00:39:22.647
the day that Demis came in.

00:39:23.606 --> 00:39:26.567
&lt;i&gt;He walked in the door,&lt;/i&gt;
&lt;i&gt;he looked about 12.&lt;/i&gt;

00:39:28.194 --> 00:39:29.195
I thought, "Oh, my God,

00:39:29.278 --> 00:39:31.072
"what the hell are we going
to do with this guy?"

00:39:31.155 --> 00:39:32.573
I applied to Cambridge.

00:39:32.657 --> 00:39:35.076
I got in but they said
I was way too young.

00:39:35.159 --> 00:39:37.412
So...
So I needed to take a year off

00:39:37.495 --> 00:39:39.372
so I'd be at least 17
before I got there.

00:39:39.789 --> 00:39:42.291
&lt;i&gt;And that's when I decided&lt;/i&gt;
&lt;i&gt;to spend that entire gap year&lt;/i&gt;

00:39:42.375 --> 00:39:43.793
&lt;i&gt;working at Bullfrog.&lt;/i&gt;

00:39:44.252 --> 00:39:45.753
&lt;i&gt;They couldn't even&lt;/i&gt;
&lt;i&gt;legally employ me,&lt;/i&gt;

00:39:45.837 --> 00:39:47.714
&lt;i&gt;so I ended up being paid&lt;/i&gt;
&lt;i&gt;in brown paper envelopes.&lt;/i&gt;

00:39:47.797 --> 00:39:48.840
(CHUCKLES)

00:39:50.591 --> 00:39:53.928
&lt;i&gt;I got a feeling of being&lt;/i&gt;
&lt;i&gt;really at the cutting edge&lt;/i&gt;

00:39:54.012 --> 00:39:57.640
&lt;i&gt;and how much fun that was&lt;/i&gt;
&lt;i&gt;to invent things every day.&lt;/i&gt;

00:39:57.724 --> 00:39:59.308
And then you know,
a few months later,

00:40:00.351 --> 00:40:02.520
maybe everyone... a million
people will be playing it.

00:40:03.438 --> 00:40:06.357
MOLYNEUX: &lt;i&gt;In those days&lt;/i&gt;
&lt;i&gt;computer games had to evolve.&lt;/i&gt;

00:40:06.441 --> 00:40:08.192
&lt;i&gt;There had to be new genres&lt;/i&gt;

00:40:08.276 --> 00:40:10.695
&lt;i&gt;which were more&lt;/i&gt;
&lt;i&gt;than just shooting things.&lt;/i&gt;

00:40:11.362 --> 00:40:13.656
Wouldn't it be amazing
to have a game

00:40:13.740 --> 00:40:18.327
where you design and build
your own theme park?

00:40:18.411 --> 00:40:20.413
(GAME CHARACTERS SCREAMING)

00:40:22.331 --> 00:40:25.501
&lt;i&gt;Demis and I started to talk&lt;/i&gt;
&lt;i&gt;about&lt;/i&gt; Theme Park.

00:40:25.585 --> 00:40:28.588
&lt;i&gt;It allows the player&lt;/i&gt;
&lt;i&gt;to build a world&lt;/i&gt;

00:40:28.671 --> 00:40:31.382
&lt;i&gt;and see the consequences&lt;/i&gt;
&lt;i&gt;of your choices&lt;/i&gt;

00:40:31.466 --> 00:40:33.760
&lt;i&gt;that you've made&lt;/i&gt;
&lt;i&gt;in that world.&lt;/i&gt;

00:40:34.177 --> 00:40:35.678
HASSABIS: &lt;i&gt;A human player&lt;/i&gt;
&lt;i&gt;set out the layout&lt;/i&gt;

00:40:35.762 --> 00:40:38.306
&lt;i&gt;of the theme park and designed&lt;/i&gt;
&lt;i&gt;the roller coaster&lt;/i&gt;

00:40:38.389 --> 00:40:40.975
&lt;i&gt;and set the prices&lt;/i&gt;
&lt;i&gt;in the chip shop.&lt;/i&gt;

00:40:41.059 --> 00:40:43.102
&lt;i&gt;What I was working on was&lt;/i&gt;
&lt;i&gt;the behaviors of the people.&lt;/i&gt;

00:40:43.186 --> 00:40:44.729
&lt;i&gt;They were autonomous&lt;/i&gt;

00:40:44.812 --> 00:40:46.939
&lt;i&gt;and that was the AI&lt;/i&gt;
&lt;i&gt;in this case.&lt;/i&gt;

00:40:47.023 --> 00:40:48.399
So what I was trying to do
was mimic

00:40:48.483 --> 00:40:50.735
interesting human behavior

00:40:50.818 --> 00:40:51.986
so that the simulation
would be

00:40:52.070 --> 00:40:53.404
more interesting
to interact with.

00:40:54.322 --> 00:40:56.199
MOLYNEUX: &lt;i&gt;Demis worked&lt;/i&gt;
&lt;i&gt;on ridiculous things,&lt;/i&gt;

00:40:56.282 --> 00:40:59.285
&lt;i&gt;like you could place down&lt;/i&gt;
&lt;i&gt;these shops&lt;/i&gt;

00:40:59.369 --> 00:41:03.331
&lt;i&gt;and if you put a shop too near&lt;/i&gt;
&lt;i&gt;a very dangerous ride,&lt;/i&gt;

00:41:03.414 --> 00:41:05.041
&lt;i&gt;then people on the ride&lt;/i&gt;
&lt;i&gt;would throw up&lt;/i&gt;

00:41:05.124 --> 00:41:06.959
&lt;i&gt;because they'd just eaten.&lt;/i&gt;

00:41:07.669 --> 00:41:09.337
And then that would make
other people throw up

00:41:09.420 --> 00:41:11.714
when they saw the throwing-up
on the floor,

00:41:11.798 --> 00:41:14.217
so you then had to have
lots of sweepers

00:41:14.300 --> 00:41:17.345
&lt;i&gt;to quickly sweep it up&lt;/i&gt;
&lt;i&gt;before the people saw it.&lt;/i&gt;

00:41:17.428 --> 00:41:19.013
&lt;i&gt;That's the cool thing&lt;/i&gt;
&lt;i&gt;about it.&lt;/i&gt;

00:41:19.097 --> 00:41:22.058
&lt;i&gt;You as the player tinker with&lt;/i&gt;
&lt;i&gt;it and then it reacts to you.&lt;/i&gt;

00:41:22.600 --> 00:41:25.395
MOLYNEUX: &lt;i&gt;All those nuanced&lt;/i&gt;
&lt;i&gt;simulation things he did&lt;/i&gt;

00:41:25.478 --> 00:41:27.689
and that was an invention

00:41:27.772 --> 00:41:30.274
which never really
existed before.

00:41:31.025 --> 00:41:33.361
&lt;i&gt;It was&lt;/i&gt;
&lt;i&gt;unbelievably successful.&lt;/i&gt;

00:41:34.070 --> 00:41:35.196
DAVID GARDNER:
Theme Park &lt;i&gt;actually turned out&lt;/i&gt;

00:41:35.279 --> 00:41:36.781
&lt;i&gt;to be a top ten title&lt;/i&gt;

00:41:36.864 --> 00:41:39.492
and that was the first time
we were starting to see

00:41:39.575 --> 00:41:42.578
how AI could make
a difference.

00:41:42.662 --> 00:41:44.414
(BRASS BAND PLAYING)

00:41:45.790 --> 00:41:47.250
CARTER: &lt;i&gt;We were doing&lt;/i&gt;
&lt;i&gt;some Christmas shopping&lt;/i&gt;

00:41:47.333 --> 00:41:49.877
&lt;i&gt;and were waiting for the taxi&lt;/i&gt;
&lt;i&gt;to take us home.&lt;/i&gt;

00:41:51.045 --> 00:41:54.507
I have this very clear memory
of Demis talking about AI

00:41:54.590 --> 00:41:55.675
in a very different way,

00:41:55.758 --> 00:41:58.052
in a way that we didn't
commonly talk about.

00:41:58.136 --> 00:42:02.015
This idea of AI being useful
for other things

00:42:02.098 --> 00:42:03.641
other than entertainment.

00:42:03.725 --> 00:42:07.061
So being useful for, um,
helping the world

00:42:07.145 --> 00:42:09.897
and the potential of AI
to change the world.

00:42:09.981 --> 00:42:12.859
I just said to Demis,
"What is it you want to do?"

00:42:12.942 --> 00:42:14.193
And he said to me,

00:42:14.277 --> 00:42:16.404
"I want to be the person
that solves AI."

00:42:22.452 --> 00:42:25.246
HASSABIS:
&lt;i&gt;Peter offered me Â£1 million&lt;/i&gt;

00:42:25.329 --> 00:42:27.248
&lt;i&gt;to not go to university.&lt;/i&gt;

00:42:29.792 --> 00:42:31.544
&lt;i&gt;But I had a plan&lt;/i&gt;
&lt;i&gt;from the beginning.&lt;/i&gt;

00:42:32.337 --> 00:42:34.380
&lt;i&gt;And my plan was always&lt;/i&gt;
&lt;i&gt;to go to Cambridge.&lt;/i&gt;

00:42:35.381 --> 00:42:36.424
I think a lot of
my schoolfriends

00:42:36.507 --> 00:42:37.592
thought I was mad.

00:42:37.675 --> 00:42:38.801
Why would you not...

00:42:38.885 --> 00:42:40.345
I mean, Â£1 million,
that's a lot of money.

00:42:40.428 --> 00:42:43.139
In the '90s,
that is a lot of money, right?

00:42:43.222 --> 00:42:45.975
For a...
For a poor 17-year-old kid.

00:42:46.059 --> 00:42:49.812
He's like this little seed
that's going to burst through,

00:42:49.896 --> 00:42:53.191
and he's not going to be able
to do that at Bullfrog.

00:42:56.110 --> 00:42:58.654
&lt;i&gt;I had to drop him off&lt;/i&gt;
&lt;i&gt;at the train station&lt;/i&gt;

00:42:58.738 --> 00:43:02.241
&lt;i&gt;and I can still see&lt;/i&gt;
&lt;i&gt;that picture&lt;/i&gt;

00:43:02.325 --> 00:43:06.579
of this little elfin character
disappear down that tunnel.

00:43:06.662 --> 00:43:09.165
That was an incredibly
sad moment.

00:43:13.044 --> 00:43:14.337
HASSABIS:
&lt;i&gt;I had this romantic ideal&lt;/i&gt;

00:43:14.420 --> 00:43:15.672
&lt;i&gt;of what Cambridge&lt;/i&gt;
&lt;i&gt;would be like,&lt;/i&gt;

00:43:16.547 --> 00:43:18.299
&lt;i&gt;1,000 years of history,&lt;/i&gt;

00:43:18.383 --> 00:43:20.593
&lt;i&gt;walking the same streets&lt;/i&gt;
&lt;i&gt;that Turing,&lt;/i&gt;

00:43:20.677 --> 00:43:23.262
&lt;i&gt;Newton and Crick had walked.&lt;/i&gt;

00:43:23.346 --> 00:43:26.099
&lt;i&gt;I wanted to explore&lt;/i&gt;
&lt;i&gt;the edge of the universe.&lt;/i&gt;

00:43:26.182 --> 00:43:27.308
(CHURCH BELLS TOLLING)

00:43:28.685 --> 00:43:29.936
&lt;i&gt;When I got to Cambridge,&lt;/i&gt;

00:43:30.019 --> 00:43:32.188
&lt;i&gt;I'd basically been working&lt;/i&gt;
&lt;i&gt;my whole life.&lt;/i&gt;

00:43:33.231 --> 00:43:34.649
&lt;i&gt;Every single summer,&lt;/i&gt;

00:43:34.732 --> 00:43:36.693
&lt;i&gt;I was either playing chess&lt;/i&gt;
&lt;i&gt;professionally,&lt;/i&gt;

00:43:36.776 --> 00:43:38.611
&lt;i&gt;or I was working,&lt;/i&gt;
&lt;i&gt;doing an internship.&lt;/i&gt;

00:43:39.445 --> 00:43:43.366
&lt;i&gt;So I was, like, "Right,&lt;/i&gt;
&lt;i&gt;I am gonna have fun now&lt;/i&gt;

00:43:43.449 --> 00:43:46.244
&lt;i&gt;"and explore what it means&lt;/i&gt;
&lt;i&gt;to be a normal teenager."&lt;/i&gt;

00:43:47.537 --> 00:43:49.747
(PEOPLE CHEERING, LAUGHING)

00:43:49.831 --> 00:43:51.833
Come on! Go, boy, go!

00:43:51.916 --> 00:43:53.543
TIM STEVENS: &lt;i&gt;It was work hard&lt;/i&gt;
&lt;i&gt;and play hard.&lt;/i&gt;

00:43:53.626 --> 00:43:55.294
(ALL SINGING)

00:43:55.378 --> 00:43:56.546
I first met Demis

00:43:56.629 --> 00:43:58.589
because we both attended
Queens' College.

00:43:59.674 --> 00:44:00.717
&lt;i&gt;Our group of friends,&lt;/i&gt;

00:44:00.800 --> 00:44:02.760
&lt;i&gt;we'd often drink beer&lt;/i&gt;
&lt;i&gt;in the bar,&lt;/i&gt;

00:44:02.844 --> 00:44:04.679
&lt;i&gt;play table football.&lt;/i&gt;

00:44:04.762 --> 00:44:06.931
HASSABIS: &lt;i&gt;In the bar,&lt;/i&gt;
&lt;i&gt;I used to play speed chess,&lt;/i&gt;

00:44:07.015 --> 00:44:08.850
&lt;i&gt;pieces flying off the board,&lt;/i&gt;

00:44:08.933 --> 00:44:10.643
&lt;i&gt;you know, the whole game&lt;/i&gt;
&lt;i&gt;in one minute.&lt;/i&gt;

00:44:10.727 --> 00:44:11.894
Demis sat down opposite me.

00:44:11.978 --> 00:44:13.187
And I looked at him
and I thought,

00:44:13.271 --> 00:44:14.731
"I remember you
from when we were kids."

00:44:14.814 --> 00:44:16.733
HASSABIS: &lt;i&gt;I had actually been&lt;/i&gt;
&lt;i&gt;in the same chess tournament&lt;/i&gt;

00:44:16.816 --> 00:44:17.817
&lt;i&gt;as Dave in Ipswich,&lt;/i&gt;

00:44:17.900 --> 00:44:20.028
&lt;i&gt;where I used to go and try&lt;/i&gt;
&lt;i&gt;and raid his local chess club&lt;/i&gt;

00:44:20.111 --> 00:44:21.779
&lt;i&gt;to win a bit of prize money.&lt;/i&gt;

00:44:22.447 --> 00:44:24.240
COPPIN: &lt;i&gt;We were studying&lt;/i&gt;
&lt;i&gt;computer science.&lt;/i&gt;

00:44:24.323 --> 00:44:26.492
&lt;i&gt;Some people,&lt;/i&gt;
&lt;i&gt;who at the age of 17&lt;/i&gt;

00:44:26.576 --> 00:44:28.036
would have come in and made
sure to tell everybody

00:44:28.119 --> 00:44:29.162
everything about themselves.

00:44:29.245 --> 00:44:30.580
&lt;i&gt;"Hey, I worked at Bullfrog&lt;/i&gt;

00:44:30.663 --> 00:44:32.540
&lt;i&gt;"and built the world's&lt;/i&gt;
&lt;i&gt;most successful video game."&lt;/i&gt;

00:44:32.623 --> 00:44:33.666
&lt;i&gt;But he wasn't like that&lt;/i&gt;
&lt;i&gt;at all.&lt;/i&gt;

00:44:34.459 --> 00:44:36.002
SILVER: &lt;i&gt;At Cambridge,&lt;/i&gt;
&lt;i&gt;Demis and myself&lt;/i&gt;

00:44:36.085 --> 00:44:38.004
&lt;i&gt;both had an interest&lt;/i&gt;
&lt;i&gt;in computational neuroscience&lt;/i&gt;

00:44:38.087 --> 00:44:39.797
and trying to understand
how computers and brains

00:44:39.881 --> 00:44:41.632
intertwined
and linked together.

00:44:42.383 --> 00:44:43.885
JOHN DAUGMAN:
&lt;i&gt;Both David and Demis&lt;/i&gt;

00:44:43.968 --> 00:44:45.636
&lt;i&gt;came to me for supervisions.&lt;/i&gt;

00:44:46.095 --> 00:44:49.432
It happens just by coincidence
that the year 1997,

00:44:49.515 --> 00:44:51.309
their third and final year
at Cambridge,

00:44:51.392 --> 00:44:54.854
was also the year when
the first chess grandmaster

00:44:54.937 --> 00:44:56.272
was beaten by
a computer program.

00:44:56.356 --> 00:44:57.815
(CAMERA SHUTTERS CLICKING)

00:44:57.899 --> 00:44:59.859
NEWSCASTER: &lt;i&gt;Round one today&lt;/i&gt;
&lt;i&gt;of a chess match&lt;/i&gt;

00:44:59.942 --> 00:45:02.904
&lt;i&gt;between the ranking&lt;/i&gt;
&lt;i&gt;world champion Garry Kasparov&lt;/i&gt;

00:45:03.488 --> 00:45:05.698
&lt;i&gt;and an opponent named&lt;/i&gt;
&lt;i&gt;Deep Blue&lt;/i&gt;

00:45:05.782 --> 00:45:09.410
&lt;i&gt;to test to see if the human&lt;/i&gt;
&lt;i&gt;brain can outwit a machine.&lt;/i&gt;

00:45:10.203 --> 00:45:11.245
HASSABIS: &lt;i&gt;I remember the drama&lt;/i&gt;

00:45:11.329 --> 00:45:13.289
&lt;i&gt;of Kasparov&lt;/i&gt;
&lt;i&gt;losing the last match.&lt;/i&gt;

00:45:13.373 --> 00:45:14.791
NEWSCASTER 2: &lt;i&gt;Whoa!&lt;/i&gt;

00:45:14.874 --> 00:45:16.751
&lt;i&gt;Kasparov has resigned!&lt;/i&gt;

00:45:16.834 --> 00:45:19.212
When Deep Blue
beat Garry Kasparov,

00:45:19.295 --> 00:45:20.463
that was a real
watershed event.

00:45:21.130 --> 00:45:22.674
HASSABIS:
&lt;i&gt;My main memory of it was&lt;/i&gt;

00:45:22.757 --> 00:45:24.884
&lt;i&gt;I wasn't that impressed&lt;/i&gt;
&lt;i&gt;with Deep Blue.&lt;/i&gt;

00:45:24.967 --> 00:45:26.803
&lt;i&gt;I was more impressed&lt;/i&gt;
&lt;i&gt;with Kasparov's mind.&lt;/i&gt;

00:45:26.886 --> 00:45:29.097
&lt;i&gt;That he could play chess&lt;/i&gt;
&lt;i&gt;to this level,&lt;/i&gt;

00:45:29.180 --> 00:45:31.307
&lt;i&gt;where he could compete&lt;/i&gt;
&lt;i&gt;on an equal footing&lt;/i&gt;

00:45:31.391 --> 00:45:32.809
&lt;i&gt;with the brute of a machine,&lt;/i&gt;

00:45:32.892 --> 00:45:34.727
&lt;i&gt;but of course, Kasparov can do&lt;/i&gt;

00:45:34.811 --> 00:45:36.437
&lt;i&gt;everything else humans can do,&lt;/i&gt;
&lt;i&gt;too.&lt;/i&gt;

00:45:36.521 --> 00:45:37.814
&lt;i&gt;It was a huge achievement.&lt;/i&gt;

00:45:37.897 --> 00:45:38.981
&lt;i&gt;But the truth&lt;/i&gt;
&lt;i&gt;of the matter was,&lt;/i&gt;

00:45:39.065 --> 00:45:40.441
&lt;i&gt;Deep Blue&lt;/i&gt;
&lt;i&gt;could only play chess.&lt;/i&gt;

00:45:42.068 --> 00:45:43.277
&lt;i&gt;What we would regard&lt;/i&gt;
&lt;i&gt;as intelligence&lt;/i&gt;

00:45:44.195 --> 00:45:45.947
&lt;i&gt;was missing from that system.&lt;/i&gt;

00:45:46.614 --> 00:45:49.367
&lt;i&gt;This idea of generality&lt;/i&gt;
&lt;i&gt;and also learning.&lt;/i&gt;

00:45:53.454 --> 00:45:54.997
&lt;i&gt;Cambridge was amazing,&lt;/i&gt;
&lt;i&gt;because of course, you know,&lt;/i&gt;

00:45:55.081 --> 00:45:56.082
&lt;i&gt;you're mixing with people&lt;/i&gt;

00:45:56.165 --> 00:45:57.959
&lt;i&gt;who are studying&lt;/i&gt;
&lt;i&gt;many different subjects.&lt;/i&gt;

00:45:58.042 --> 00:46:01.004
SILVER: &lt;i&gt;There were scientists,&lt;/i&gt;
&lt;i&gt;philosophers, artists...&lt;/i&gt;

00:46:01.087 --> 00:46:04.048
STEVENS: &lt;i&gt;...geologists,&lt;/i&gt;
&lt;i&gt;biologists, ecologists.&lt;/i&gt;

00:46:04.132 --> 00:46:06.300
&lt;i&gt;You know, everybody is talking&lt;/i&gt;
&lt;i&gt;about everything all the time.&lt;/i&gt;

00:46:07.176 --> 00:46:09.595
I was obsessed with
the protein folding problem.

00:46:10.513 --> 00:46:12.807
HASSABIS: &lt;i&gt;Tim Stevens used&lt;/i&gt;
&lt;i&gt;to talk obsessively,&lt;/i&gt;

00:46:12.890 --> 00:46:15.018
almost like religiously
about this problem,

00:46:15.101 --> 00:46:16.144
protein folding problem.

00:46:16.769 --> 00:46:18.104
STEVENS:
&lt;i&gt;Proteins are, you know,&lt;/i&gt;

00:46:18.187 --> 00:46:20.440
&lt;i&gt;one of the most beautiful and&lt;/i&gt;
&lt;i&gt;elegant things about biology.&lt;/i&gt;

00:46:21.774 --> 00:46:23.860
&lt;i&gt;They are the machines of life.&lt;/i&gt;

00:46:24.485 --> 00:46:26.487
They build everything,
they control everything,

00:46:26.571 --> 00:46:28.072
they're why biology works.

00:46:29.240 --> 00:46:32.285
&lt;i&gt;Proteins are made from strings&lt;/i&gt;
&lt;i&gt;of amino acids&lt;/i&gt;

00:46:32.368 --> 00:46:35.538
&lt;i&gt;that fold up to create&lt;/i&gt;
&lt;i&gt;a protein structure.&lt;/i&gt;

00:46:36.831 --> 00:46:39.542
&lt;i&gt;If we can predict&lt;/i&gt;
&lt;i&gt;the structure of proteins&lt;/i&gt;

00:46:39.625 --> 00:46:42.003
&lt;i&gt;from just their amino acid&lt;/i&gt;
&lt;i&gt;sequences,&lt;/i&gt;

00:46:42.712 --> 00:46:45.298
&lt;i&gt;then a new protein&lt;/i&gt;
&lt;i&gt;to cure cancer&lt;/i&gt;

00:46:45.965 --> 00:46:48.968
&lt;i&gt;or break down plastic&lt;/i&gt;
&lt;i&gt;to help the environment&lt;/i&gt;

00:46:49.052 --> 00:46:50.470
&lt;i&gt;is definitely something&lt;/i&gt;

00:46:50.553 --> 00:46:52.513
&lt;i&gt;that you could begin&lt;/i&gt;
&lt;i&gt;to think about.&lt;/i&gt;

00:46:53.473 --> 00:46:54.599
I kind of thought,

00:46:54.682 --> 00:46:57.852
"Well, is a human being
clever enough

00:46:57.935 --> 00:46:59.479
"to actually fold a protein?"

00:46:59.562 --> 00:47:01.189
&lt;i&gt;We can't work it out.&lt;/i&gt;

00:47:01.814 --> 00:47:03.566
JOHN MOULT: &lt;i&gt;Since the 1960s,&lt;/i&gt;

00:47:03.649 --> 00:47:05.109
&lt;i&gt;we thought that in principle,&lt;/i&gt;

00:47:05.693 --> 00:47:08.571
&lt;i&gt;if I know what the amino acid&lt;/i&gt;
&lt;i&gt;sequence of a protein is,&lt;/i&gt;

00:47:08.654 --> 00:47:11.032
&lt;i&gt;I should be able to compute&lt;/i&gt;
&lt;i&gt;what the structure's like.&lt;/i&gt;

00:47:11.115 --> 00:47:13.409
So if you could
just press a button,

00:47:13.493 --> 00:47:15.870
and they'd all come
popping out, that would be...

00:47:15.953 --> 00:47:17.580
that would have some impact.

00:47:19.832 --> 00:47:21.167
HASSABIS: &lt;i&gt;It stuck in my mind.&lt;/i&gt;

00:47:21.250 --> 00:47:23.252
&lt;i&gt;"Oh, this is&lt;/i&gt;
&lt;i&gt;a very interesting problem."&lt;/i&gt;

00:47:23.336 --> 00:47:25.630
&lt;i&gt;And it felt to me&lt;/i&gt;
&lt;i&gt;like it would be solvable.&lt;/i&gt;

00:47:26.839 --> 00:47:29.509
&lt;i&gt;But I thought&lt;/i&gt;
&lt;i&gt;it would need AI to do it.&lt;/i&gt;

00:47:31.135 --> 00:47:33.137
&lt;i&gt;If we could just solve&lt;/i&gt;
&lt;i&gt;protein folding,&lt;/i&gt;

00:47:33.596 --> 00:47:35.098
&lt;i&gt;it could change the world.&lt;/i&gt;

00:47:50.530 --> 00:47:52.323
HASSABIS: &lt;i&gt;Ever since&lt;/i&gt;
&lt;i&gt;I was a student at Cambridge,&lt;/i&gt;

00:47:53.741 --> 00:47:55.201
&lt;i&gt;I've never&lt;/i&gt;
&lt;i&gt;stopped thinking about&lt;/i&gt;

00:47:55.284 --> 00:47:56.536
&lt;i&gt;the protein folding problem.&lt;/i&gt;

00:47:59.455 --> 00:48:01.416
&lt;i&gt;If you were&lt;/i&gt;
&lt;i&gt;to solve protein folding,&lt;/i&gt;

00:48:02.500 --> 00:48:05.294
&lt;i&gt;then the potential&lt;/i&gt;
&lt;i&gt;to help solve problems like&lt;/i&gt;

00:48:05.378 --> 00:48:08.589
&lt;i&gt;Alzheimer's, dementia&lt;/i&gt;
&lt;i&gt;and drug discovery is huge.&lt;/i&gt;

00:48:09.340 --> 00:48:11.259
&lt;i&gt;Solving disease is probably&lt;/i&gt;

00:48:11.342 --> 00:48:12.927
&lt;i&gt;the most major impact&lt;/i&gt;
&lt;i&gt;we could have.&lt;/i&gt;

00:48:13.011 --> 00:48:14.012
(CLICKS MOUSE)

00:48:15.054 --> 00:48:16.305
&lt;i&gt;Thousands of very smart people&lt;/i&gt;

00:48:16.389 --> 00:48:17.807
&lt;i&gt;have tried&lt;/i&gt;
&lt;i&gt;to solve protein folding.&lt;/i&gt;

00:48:18.474 --> 00:48:20.435
&lt;i&gt;I just think now&lt;/i&gt;
&lt;i&gt;is the right time&lt;/i&gt;

00:48:20.518 --> 00:48:22.103
&lt;i&gt;for AI to crack it.&lt;/i&gt;

00:48:22.186 --> 00:48:23.813
(THRILLING MUSIC PLAYING)

00:48:23.896 --> 00:48:26.065
(INDISTINCT CONVERSATION)

00:48:26.149 --> 00:48:27.984
RICHARD EVANS: &lt;i&gt;We needed&lt;/i&gt;
&lt;i&gt;a reasonable way&lt;/i&gt;

00:48:28.067 --> 00:48:29.068
&lt;i&gt;to apply machine learning&lt;/i&gt;

00:48:29.152 --> 00:48:30.153
&lt;i&gt;to the protein folding&lt;/i&gt;
&lt;i&gt;problem.&lt;/i&gt;

00:48:30.236 --> 00:48:31.946
(CLICKING MOUSE)

00:48:32.030 --> 00:48:34.073
&lt;i&gt;We came across&lt;/i&gt;
&lt;i&gt;this Foldit game.&lt;/i&gt;

00:48:35.074 --> 00:48:38.411
&lt;i&gt;The goal is to move around&lt;/i&gt;
&lt;i&gt;this 3D model of a protein&lt;/i&gt;

00:48:38.494 --> 00:48:40.455
&lt;i&gt;and you get a score&lt;/i&gt;
&lt;i&gt;every time you move it.&lt;/i&gt;

00:48:41.414 --> 00:48:42.749
The more accurate
you make these structures,

00:48:42.832 --> 00:48:44.292
the more useful
they will be to biologists.

00:48:45.752 --> 00:48:46.753
&lt;i&gt;I spent a few days&lt;/i&gt;

00:48:46.836 --> 00:48:48.296
&lt;i&gt;just kind of seeing&lt;/i&gt;
&lt;i&gt;how well we could do.&lt;/i&gt;

00:48:48.379 --> 00:48:50.256
(GAME DINGING)

00:48:50.340 --> 00:48:51.507
&lt;i&gt;We did reasonably well.&lt;/i&gt;

00:48:52.091 --> 00:48:53.468
But even if you were

00:48:53.551 --> 00:48:54.802
the world's
best Foldit player,

00:48:54.886 --> 00:48:56.179
you wouldn't
solve protein folding.

00:48:57.138 --> 00:48:59.057
&lt;i&gt;That's why we had&lt;/i&gt;
&lt;i&gt;to move beyond the game.&lt;/i&gt;

00:48:59.140 --> 00:49:00.141
HASSABIS: &lt;i&gt;Games&lt;/i&gt;
&lt;i&gt;are always just&lt;/i&gt;

00:49:00.224 --> 00:49:02.018
&lt;i&gt;the proving ground&lt;/i&gt;
&lt;i&gt;for our algorithms.&lt;/i&gt;

00:49:03.394 --> 00:49:07.315
&lt;i&gt;The ultimate goal was not just&lt;/i&gt;
&lt;i&gt;to crack Go and StarCraft.&lt;/i&gt;

00:49:07.398 --> 00:49:09.567
&lt;i&gt;It was to crack&lt;/i&gt;
&lt;i&gt;real-world challenges.&lt;/i&gt;

00:49:10.610 --> 00:49:12.612
(THRILLING MUSIC CONTINUES)

00:49:16.032 --> 00:49:18.076
JOHN JUMPER: &lt;i&gt;I remember&lt;/i&gt;
&lt;i&gt;hearing this rumor&lt;/i&gt;

00:49:18.159 --> 00:49:20.870
&lt;i&gt;that Demis was&lt;/i&gt;
&lt;i&gt;getting into proteins.&lt;/i&gt;

00:49:20.953 --> 00:49:23.331
&lt;i&gt;I talked to some people&lt;/i&gt;
&lt;i&gt;at DeepMind and I would ask,&lt;/i&gt;

00:49:23.414 --> 00:49:24.707
&lt;i&gt;"So are you doing&lt;/i&gt;
&lt;i&gt;protein folding?"&lt;/i&gt;

00:49:24.791 --> 00:49:26.584
&lt;i&gt;And they would&lt;/i&gt;
&lt;i&gt;artfully change the subject.&lt;/i&gt;

00:49:26.668 --> 00:49:29.212
&lt;i&gt;And when that happened twice,&lt;/i&gt;
&lt;i&gt;I pretty much figured it out.&lt;/i&gt;

00:49:29.921 --> 00:49:31.756
&lt;i&gt;So I thought&lt;/i&gt;
&lt;i&gt;I should submit a resume.&lt;/i&gt;

00:49:32.548 --> 00:49:35.259
HASSABIS: All right, everyone,
welcome to DeepMind.

00:49:35.343 --> 00:49:37.220
I know some of you,
this may be your first week,

00:49:37.303 --> 00:49:38.304
but I hope you all set...

00:49:38.388 --> 00:49:40.556
JUMPER: &lt;i&gt;The really appealing&lt;/i&gt;
&lt;i&gt;part for me about the job&lt;/i&gt;

00:49:40.640 --> 00:49:42.308
was this, like,
sense of connection

00:49:42.392 --> 00:49:44.060
to the larger purpose.

00:49:44.143 --> 00:49:45.144
HASSABIS: If we can crack

00:49:45.228 --> 00:49:47.480
some fundamental problems
in science,

00:49:47.563 --> 00:49:48.564
many other people

00:49:48.648 --> 00:49:50.525
and other companies
and labs and so on

00:49:50.608 --> 00:49:52.151
could build
on top of our work.

00:49:52.235 --> 00:49:53.444
&lt;i&gt;This is your chance now&lt;/i&gt;

00:49:53.528 --> 00:49:55.488
&lt;i&gt;to add your chapter&lt;/i&gt;
&lt;i&gt;to this story.&lt;/i&gt;

00:49:55.571 --> 00:49:56.906
JUMPER: &lt;i&gt;When I arrived,&lt;/i&gt;

00:49:56.989 --> 00:49:59.200
&lt;i&gt;I was definitely&lt;/i&gt; (CHUCKLES)
&lt;i&gt;quite a bit nervous.&lt;/i&gt;

00:49:59.283 --> 00:50:00.284
I'm still trying to keep...

00:50:00.368 --> 00:50:02.578
&lt;i&gt;I haven't taken&lt;/i&gt;
&lt;i&gt;any biology courses.&lt;/i&gt;

00:50:02.662 --> 00:50:05.164
We haven't spent
years of our lives

00:50:05.248 --> 00:50:07.625
looking at these structures
and understanding them.

00:50:07.709 --> 00:50:09.585
We are just going off the data

00:50:09.669 --> 00:50:10.920
&lt;i&gt;and our machine learning&lt;/i&gt;
&lt;i&gt;models.&lt;/i&gt;

00:50:11.754 --> 00:50:12.797
JUMPER: &lt;i&gt;In machine learning,&lt;/i&gt;

00:50:12.880 --> 00:50:14.841
&lt;i&gt;you train a network&lt;/i&gt;
&lt;i&gt;like flashcards.&lt;/i&gt;

00:50:15.508 --> 00:50:17.427
&lt;i&gt;Here's the question.&lt;/i&gt;
&lt;i&gt;Here's the answer.&lt;/i&gt;

00:50:18.511 --> 00:50:20.388
&lt;i&gt;Here's the question.&lt;/i&gt;
&lt;i&gt;Here's the answer.&lt;/i&gt;

00:50:20.471 --> 00:50:22.056
&lt;i&gt;But in protein folding,&lt;/i&gt;

00:50:22.140 --> 00:50:25.018
we're not doing the kind
of standard task at DeepMind

00:50:25.101 --> 00:50:27.854
&lt;i&gt;where you have unlimited data.&lt;/i&gt;

00:50:27.937 --> 00:50:30.356
&lt;i&gt;Your job is to get better&lt;/i&gt;
&lt;i&gt;at chess or Go&lt;/i&gt;

00:50:30.440 --> 00:50:32.692
&lt;i&gt;and you can play&lt;/i&gt;
&lt;i&gt;as many games of chess or Go&lt;/i&gt;

00:50:32.775 --> 00:50:34.235
&lt;i&gt;as your computers will allow.&lt;/i&gt;

00:50:35.111 --> 00:50:36.237
&lt;i&gt;With proteins,&lt;/i&gt;

00:50:36.320 --> 00:50:39.323
&lt;i&gt;we're sitting on&lt;/i&gt;
&lt;i&gt;a very thick size of data&lt;/i&gt;

00:50:39.407 --> 00:50:41.659
&lt;i&gt;that's been determined&lt;/i&gt;
&lt;i&gt;by a half century&lt;/i&gt;

00:50:41.743 --> 00:50:44.912
&lt;i&gt;of time-consuming experimental&lt;/i&gt;
&lt;i&gt;methods in laboratories.&lt;/i&gt;

00:50:46.330 --> 00:50:49.292
&lt;i&gt;These painstaking methods&lt;/i&gt;
&lt;i&gt;can take months or years&lt;/i&gt;

00:50:49.375 --> 00:50:52.045
&lt;i&gt;to determine&lt;/i&gt;
&lt;i&gt;a single protein structure,&lt;/i&gt;

00:50:52.128 --> 00:50:55.214
&lt;i&gt;and sometimes, a structure&lt;/i&gt;
&lt;i&gt;can never be determined.&lt;/i&gt;

00:50:55.298 --> 00:50:56.299
(TYPING)

00:50:56.382 --> 00:50:59.761
&lt;i&gt;That's why we're working&lt;/i&gt;
&lt;i&gt;with such small datasets&lt;/i&gt;

00:50:59.844 --> 00:51:01.054
&lt;i&gt;to train our algorithms.&lt;/i&gt;

00:51:02.013 --> 00:51:03.890
EWAN BIRNEY: &lt;i&gt;When DeepMind&lt;/i&gt;
&lt;i&gt;started to explore&lt;/i&gt;

00:51:03.973 --> 00:51:05.183
&lt;i&gt;the folding problem,&lt;/i&gt;

00:51:05.266 --> 00:51:07.685
they were talking to us about
which datasets they were using

00:51:07.769 --> 00:51:09.520
and what would be
the possibilities

00:51:09.604 --> 00:51:11.064
if they did
solve this problem.

00:51:11.939 --> 00:51:12.940
&lt;i&gt;Many people have tried,&lt;/i&gt;

00:51:13.024 --> 00:51:16.194
&lt;i&gt;and yet no one on the planet&lt;/i&gt;
&lt;i&gt;has solved protein folding.&lt;/i&gt;

00:51:16.277 --> 00:51:17.695
(CHUCKLES)
I did think to myself,

00:51:17.779 --> 00:51:19.572
"Well, you know, good luck."

00:51:19.655 --> 00:51:22.325
JUMPER: If we can solve
the protein folding problem,

00:51:22.408 --> 00:51:24.744
it would have an incredible
kind of medical relevance.

00:51:25.370 --> 00:51:27.622
HASSABIS:
&lt;i&gt;This is the cycle of science.&lt;/i&gt;

00:51:27.705 --> 00:51:29.665
&lt;i&gt;You do a huge amount&lt;/i&gt;
&lt;i&gt;of exploration,&lt;/i&gt;

00:51:29.749 --> 00:51:31.584
&lt;i&gt;and then you go&lt;/i&gt;
&lt;i&gt;into exploitation mode,&lt;/i&gt;

00:51:31.668 --> 00:51:33.127
and you focus and you see

00:51:33.211 --> 00:51:34.837
how good
are those ideas, really?

00:51:34.921 --> 00:51:36.005
&lt;i&gt;And there's nothing better&lt;/i&gt;

00:51:36.089 --> 00:51:37.632
&lt;i&gt;than external competition&lt;/i&gt;
&lt;i&gt;for that.&lt;/i&gt;

00:51:39.467 --> 00:51:42.762
&lt;i&gt;So we decided&lt;/i&gt;
&lt;i&gt;to enter CASP competition.&lt;/i&gt;

00:51:42.845 --> 00:51:46.891
CASP, we started
to try and speed up

00:51:46.974 --> 00:51:49.435
the solution to
the protein folding problem.

00:51:49.519 --> 00:51:51.688
CASP is when we say,

00:51:51.771 --> 00:51:53.856
"Look, DeepMind
is doing protein folding,

00:51:53.940 --> 00:51:54.982
"this is how good we are,

00:51:55.066 --> 00:51:57.193
"and maybe it's better
than everybody else.

00:51:57.276 --> 00:51:58.277
"Maybe it isn't."

00:51:59.112 --> 00:52:00.113
CASP is a bit like

00:52:00.196 --> 00:52:01.614
the Olympic Games
of protein folding.

00:52:03.241 --> 00:52:05.743
&lt;i&gt;CASP is&lt;/i&gt;
&lt;i&gt;a community-wide assessment&lt;/i&gt;

00:52:05.827 --> 00:52:07.662
&lt;i&gt;that's held every two years.&lt;/i&gt;

00:52:09.122 --> 00:52:10.123
&lt;i&gt;Teams are given&lt;/i&gt;

00:52:10.206 --> 00:52:13.292
&lt;i&gt;the amino acid sequences&lt;/i&gt;
&lt;i&gt;of about 100 proteins,&lt;/i&gt;

00:52:14.002 --> 00:52:17.046
&lt;i&gt;and then they try&lt;/i&gt;
&lt;i&gt;to solve this folding problem&lt;/i&gt;

00:52:17.130 --> 00:52:19.090
&lt;i&gt;using computational methods.&lt;/i&gt;

00:52:20.508 --> 00:52:22.927
&lt;i&gt;These proteins have&lt;/i&gt;
&lt;i&gt;already been determined&lt;/i&gt;

00:52:23.011 --> 00:52:25.013
&lt;i&gt;by experiments&lt;/i&gt;
&lt;i&gt;in a laboratory,&lt;/i&gt;

00:52:25.763 --> 00:52:27.932
&lt;i&gt;but have not yet&lt;/i&gt;
&lt;i&gt;been revealed publicly.&lt;/i&gt;

00:52:28.850 --> 00:52:30.309
&lt;i&gt;And these known structures&lt;/i&gt;

00:52:30.393 --> 00:52:32.937
&lt;i&gt;represent the gold standard&lt;/i&gt;
&lt;i&gt;against which&lt;/i&gt;

00:52:33.021 --> 00:52:36.399
&lt;i&gt;all the computational&lt;/i&gt;
&lt;i&gt;predictions will be compared.&lt;/i&gt;

00:52:37.608 --> 00:52:38.735
MOULT: &lt;i&gt;We've got a score&lt;/i&gt;

00:52:38.818 --> 00:52:41.237
&lt;i&gt;that measures the accuracy&lt;/i&gt;
&lt;i&gt;of the predictions.&lt;/i&gt;

00:52:41.904 --> 00:52:44.282
&lt;i&gt;And you would expect&lt;/i&gt;
&lt;i&gt;a score of over 90&lt;/i&gt;

00:52:44.365 --> 00:52:46.951
to be a solution to
the protein folding problem.

00:52:47.035 --> 00:52:48.036
(INDISTINCT CHATTER)

00:52:48.119 --> 00:52:49.120
MAN: &lt;i&gt;Welcome, everyone,&lt;/i&gt;

00:52:49.203 --> 00:52:51.664
&lt;i&gt;to our first, uh, semifinals&lt;/i&gt;
&lt;i&gt;in the winners' bracket.&lt;/i&gt;

00:52:51.748 --> 00:52:54.542
&lt;i&gt;Nick and John&lt;/i&gt;
&lt;i&gt;versus Demis and Frank.&lt;/i&gt;

00:52:54.625 --> 00:52:56.878
Please join us, come around.
This will be an intense match.

00:52:56.961 --> 00:52:59.005
STEVENS:
&lt;i&gt;When I learned that Demis was&lt;/i&gt;

00:52:59.088 --> 00:53:01.716
going to tackle
the protein folding issue,

00:53:01.799 --> 00:53:03.843
um, I wasn't at all surprised.

00:53:04.469 --> 00:53:06.387
&lt;i&gt;It's very typical of Demis.&lt;/i&gt;

00:53:06.471 --> 00:53:07.930
&lt;i&gt;You know,&lt;/i&gt;
&lt;i&gt;he loves competition.&lt;/i&gt;

00:53:08.639 --> 00:53:09.640
And that's the end

00:53:09.724 --> 00:53:12.727
-of the first game, 10-7.
-(ALL CHEERING)

00:53:12.810 --> 00:53:13.811
HASSABIS:
&lt;i&gt;The aim for CASP would be&lt;/i&gt;

00:53:13.895 --> 00:53:15.605
to not just
win the competition,

00:53:15.688 --> 00:53:19.484
but sort of, um,
retire the need for it.

00:53:19.567 --> 00:53:23.112
So, 20 targets total
have been released by CASP.

00:53:23.196 --> 00:53:24.238
JUMPER: &lt;i&gt;We were thinking maybe&lt;/i&gt;

00:53:24.322 --> 00:53:26.491
&lt;i&gt;throw in the standard&lt;/i&gt;
&lt;i&gt;kind of machine learning&lt;/i&gt;

00:53:26.574 --> 00:53:28.368
&lt;i&gt;and see how far&lt;/i&gt;
&lt;i&gt;that could take us.&lt;/i&gt;

00:53:28.451 --> 00:53:30.286
Instead of having a couple
of days on an experiment,

00:53:30.370 --> 00:53:33.081
we can turn around
five experiments a day.

00:53:33.164 --> 00:53:34.832
Great. Well done, everyone.

00:53:34.916 --> 00:53:36.292
(TYPING)

00:53:36.376 --> 00:53:38.252
Can you show me the real one
instead of ours?

00:53:38.336 --> 00:53:39.337
MAN 1: The true answer is

00:53:39.420 --> 00:53:41.089
supposed to look
something like that.

00:53:41.839 --> 00:53:44.050
MAN 2: It's a lot more
cylindrical than I thought.

00:53:44.759 --> 00:53:46.886
JUMPER: &lt;i&gt;The results&lt;/i&gt;
&lt;i&gt;were not very good.&lt;/i&gt;

00:53:46.969 --> 00:53:47.970
Okay.

00:53:48.054 --> 00:53:49.430
JUMPER: &lt;i&gt;We throw&lt;/i&gt;
&lt;i&gt;all the obvious ideas to it&lt;/i&gt;

00:53:49.514 --> 00:53:51.182
&lt;i&gt;and the problem laughs at you.&lt;/i&gt;

00:53:52.850 --> 00:53:54.560
This makes no sense.

00:53:54.644 --> 00:53:55.645
EVANS: &lt;i&gt;We thought&lt;/i&gt;
&lt;i&gt;we could just throw&lt;/i&gt;

00:53:55.728 --> 00:53:57.647
&lt;i&gt;some of our best algorithms&lt;/i&gt;
&lt;i&gt;at the problem.&lt;/i&gt;

00:53:59.107 --> 00:54:00.566
We were slightly naive.

00:54:00.650 --> 00:54:01.984
JUMPER:
We should be learning this,

00:54:02.068 --> 00:54:03.861
you know,
in the blink of an eye.

00:54:05.321 --> 00:54:06.614
&lt;i&gt;The thing&lt;/i&gt;
&lt;i&gt;I'm worried about is,&lt;/i&gt;

00:54:06.698 --> 00:54:08.032
&lt;i&gt;we take the field from&lt;/i&gt;

00:54:08.116 --> 00:54:10.493
&lt;i&gt;really bad answers&lt;/i&gt;
&lt;i&gt;to moderately bad answers.&lt;/i&gt;

00:54:10.576 --> 00:54:13.538
I feel like we need
some sort of new technology

00:54:13.621 --> 00:54:14.831
for moving around
these things.

00:54:14.914 --> 00:54:16.916
(THRILLING MUSIC CONTINUES)

00:54:20.128 --> 00:54:22.088
HASSABIS: &lt;i&gt;With only&lt;/i&gt;
&lt;i&gt;a week left of CASP,&lt;/i&gt;

00:54:22.171 --> 00:54:23.840
&lt;i&gt;it's now a sprint&lt;/i&gt;
&lt;i&gt;to get it deployed.&lt;/i&gt;

00:54:23.923 --> 00:54:24.924
(MUSIC FADES)

00:54:26.217 --> 00:54:27.719
&lt;i&gt;You've done your best.&lt;/i&gt;

00:54:27.802 --> 00:54:29.470
&lt;i&gt;Then there's&lt;/i&gt;
&lt;i&gt;nothing more you can do&lt;/i&gt;

00:54:29.554 --> 00:54:31.889
&lt;i&gt;but wait for CASP&lt;/i&gt;
&lt;i&gt;to deliver the results.&lt;/i&gt;

00:54:31.973 --> 00:54:33.975
(HOPEFUL MUSIC PLAYING)

00:54:52.118 --> 00:54:53.119
&lt;i&gt;This famous thing of Einstein,&lt;/i&gt;

00:54:53.202 --> 00:54:54.454
&lt;i&gt;the last couple of years&lt;/i&gt;
&lt;i&gt;of his life,&lt;/i&gt;

00:54:54.537 --> 00:54:57.040
when he was here,
he overlapped with Kurt GÃ¶del

00:54:57.123 --> 00:54:59.334
and he said one of the reasons
he still comes in to work

00:54:59.417 --> 00:55:01.169
&lt;i&gt;is so that&lt;/i&gt;
&lt;i&gt;he gets to walk home&lt;/i&gt;

00:55:01.252 --> 00:55:02.754
&lt;i&gt;and discuss things with GÃ¶del.&lt;/i&gt;

00:55:03.296 --> 00:55:05.631
&lt;i&gt;It's a pretty big compliment&lt;/i&gt;
&lt;i&gt;for Kurt GÃ¶del,&lt;/i&gt;

00:55:05.715 --> 00:55:07.091
&lt;i&gt;shows you how amazing he was.&lt;/i&gt;

00:55:08.885 --> 00:55:10.053
MAN: &lt;i&gt;The Institute&lt;/i&gt;
&lt;i&gt;for Advanced Study&lt;/i&gt;

00:55:10.136 --> 00:55:12.555
&lt;i&gt;was formed in 1933.&lt;/i&gt;

00:55:12.638 --> 00:55:13.639
&lt;i&gt;In the early years,&lt;/i&gt;

00:55:13.723 --> 00:55:16.184
&lt;i&gt;the intense scientific&lt;/i&gt;
&lt;i&gt;atmosphere attracted&lt;/i&gt;

00:55:16.267 --> 00:55:19.020
&lt;i&gt;some of the most brilliant&lt;/i&gt;
&lt;i&gt;mathematicians and physicists&lt;/i&gt;

00:55:19.103 --> 00:55:22.231
&lt;i&gt;ever concentrated&lt;/i&gt;
&lt;i&gt;in a single place and time.&lt;/i&gt;

00:55:22.315 --> 00:55:24.275
HASSABIS: &lt;i&gt;The founding&lt;/i&gt;
&lt;i&gt;principle of this place,&lt;/i&gt;

00:55:24.359 --> 00:55:27.612
&lt;i&gt;it's the idea of unfettered&lt;/i&gt;
&lt;i&gt;intellectual pursuits,&lt;/i&gt;

00:55:27.695 --> 00:55:29.781
&lt;i&gt;even if you don't know&lt;/i&gt;
&lt;i&gt;what you're exploring.&lt;/i&gt;

00:55:29.864 --> 00:55:31.783
&lt;i&gt;Will result&lt;/i&gt;
&lt;i&gt;in some cool things,&lt;/i&gt;

00:55:31.866 --> 00:55:34.535
&lt;i&gt;and sometimes that then&lt;/i&gt;
&lt;i&gt;ends up being useful,&lt;/i&gt;

00:55:34.619 --> 00:55:35.620
&lt;i&gt;which, of course,&lt;/i&gt;

00:55:35.703 --> 00:55:37.580
is partially what I've been
trying to do at DeepMind.

00:55:37.663 --> 00:55:39.582
How many big breakthroughs
do you think are required

00:55:39.665 --> 00:55:41.209
to get all the way to AGI?

00:55:41.292 --> 00:55:42.669
And, you know,
I estimate maybe

00:55:42.752 --> 00:55:44.045
there's about
a dozen of those.

00:55:44.128 --> 00:55:45.713
You know, I hope
it's within my lifetime.

00:55:45.797 --> 00:55:47.090
-Yes, okay.
-HASSABIS: But then,

00:55:47.173 --> 00:55:48.424
all scientists
hope that, right?

00:55:49.050 --> 00:55:50.718
EMCEE: &lt;i&gt;Demis has&lt;/i&gt;
&lt;i&gt;many accolades.&lt;/i&gt;

00:55:50.802 --> 00:55:53.596
&lt;i&gt;He was elected Fellow to&lt;/i&gt;
&lt;i&gt;the Royal Society last year.&lt;/i&gt;

00:55:53.680 --> 00:55:55.515
&lt;i&gt;He is also a Fellow&lt;/i&gt;
&lt;i&gt;of Royal Society of Arts.&lt;/i&gt;

00:55:55.598 --> 00:55:57.100
A big hand for Demis Hassabis.

00:56:02.605 --> 00:56:03.606
(MUSIC FADES)

00:56:03.690 --> 00:56:05.483
HASSABIS: &lt;i&gt;My dream&lt;/i&gt;
&lt;i&gt;has always been to try&lt;/i&gt;

00:56:05.566 --> 00:56:07.735
and make
AI-assisted science possible.

00:56:07.819 --> 00:56:08.861
And what I think is

00:56:08.945 --> 00:56:10.738
our most exciting project,
last year,

00:56:10.822 --> 00:56:12.740
which is our work
in protein folding.

00:56:12.824 --> 00:56:15.076
Uh, and we call this system
AlphaFold.

00:56:15.159 --> 00:56:17.995
We entered it into CASP
and our system, uh,

00:56:18.079 --> 00:56:20.206
was the most accurate,
uh, predicting structures

00:56:20.289 --> 00:56:24.585
for 25 out of the 43 proteins
in the hardest category.

00:56:24.669 --> 00:56:25.670
So we're state of the art,

00:56:25.753 --> 00:56:27.005
but we still...
I have to make... Be clear,

00:56:27.088 --> 00:56:28.089
we're still a long way from

00:56:28.172 --> 00:56:29.507
solving the protein
folding problem.

00:56:30.258 --> 00:56:31.426
&lt;i&gt;We're working hard&lt;/i&gt;
&lt;i&gt;on this, though,&lt;/i&gt;

00:56:31.509 --> 00:56:33.261
&lt;i&gt;and we're exploring&lt;/i&gt;
&lt;i&gt;many other techniques.&lt;/i&gt;

00:56:33.344 --> 00:56:35.013
(SOMBER MUSIC PLAYING)

00:56:48.860 --> 00:56:49.861
Let's get started.

00:56:49.944 --> 00:56:52.739
JUMPER: So kind of
a rapid debrief,

00:56:52.822 --> 00:56:54.824
these are
our final rankings for CASP.

00:56:56.242 --> 00:56:57.243
HASSABIS:
&lt;i&gt;We beat the second team&lt;/i&gt;

00:56:57.326 --> 00:56:59.746
&lt;i&gt;in this competition&lt;/i&gt;
&lt;i&gt;by nearly 50%,&lt;/i&gt;

00:56:59.829 --> 00:57:01.164
&lt;i&gt;but we've still got&lt;/i&gt;
&lt;i&gt;a long way to go&lt;/i&gt;

00:57:01.247 --> 00:57:03.958
&lt;i&gt;before we've solved&lt;/i&gt;
&lt;i&gt;the protein folding problem&lt;/i&gt;

00:57:04.042 --> 00:57:06.210
&lt;i&gt;in a sense that&lt;/i&gt;
&lt;i&gt;a biologist could use it.&lt;/i&gt;

00:57:06.711 --> 00:57:08.379
JUMPER: It is area of concern.

00:57:11.341 --> 00:57:13.801
JANET THORNTON: &lt;i&gt;The quality&lt;/i&gt;
&lt;i&gt;of predictions varied&lt;/i&gt;

00:57:13.885 --> 00:57:16.429
and they were no more useful
than the previous methods.

00:57:16.512 --> 00:57:19.474
PAUL NURSE: &lt;i&gt;AlphaFold didn't&lt;/i&gt;
&lt;i&gt;produce good enough data&lt;/i&gt;

00:57:19.557 --> 00:57:22.018
for it to be useful
in a practical way

00:57:22.101 --> 00:57:23.561
to, say, somebody like me

00:57:23.644 --> 00:57:26.481
investigating
my own biological problems.

00:57:27.899 --> 00:57:29.942
JUMPER: &lt;i&gt;That was kind of&lt;/i&gt;
&lt;i&gt;a humbling moment&lt;/i&gt;

00:57:30.026 --> 00:57:32.487
&lt;i&gt;'cause we thought we'd worked&lt;/i&gt;
&lt;i&gt;very hard and succeeded.&lt;/i&gt;

00:57:32.570 --> 00:57:34.447
&lt;i&gt;And what we'd found is&lt;/i&gt;
&lt;i&gt;we were the best in the world&lt;/i&gt;

00:57:34.530 --> 00:57:35.948
&lt;i&gt;at a problem&lt;/i&gt;
&lt;i&gt;the world's not good at.&lt;/i&gt;

00:57:37.325 --> 00:57:38.493
&lt;i&gt;We knew we sucked.&lt;/i&gt;

00:57:38.576 --> 00:57:39.702
(INDISTINCT CHATTER)

00:57:39.786 --> 00:57:41.996
JUMPER: &lt;i&gt;It doesn't help&lt;/i&gt;
&lt;i&gt;if you have the tallest ladder&lt;/i&gt;

00:57:42.080 --> 00:57:43.414
&lt;i&gt;when you're going to the moon.&lt;/i&gt;

00:57:44.415 --> 00:57:46.709
HASSABIS: &lt;i&gt;The opinion of quite&lt;/i&gt;
&lt;i&gt;a few people on the team,&lt;/i&gt;

00:57:46.793 --> 00:57:49.921
&lt;i&gt;that this is sort of&lt;/i&gt;
&lt;i&gt;a fool's errand in some ways.&lt;/i&gt;

00:57:51.214 --> 00:57:53.675
&lt;i&gt;And I might have been wrong&lt;/i&gt;
&lt;i&gt;with protein folding.&lt;/i&gt;

00:57:53.758 --> 00:57:55.259
&lt;i&gt;Maybe it's too hard still&lt;/i&gt;

00:57:55.343 --> 00:57:58.054
&lt;i&gt;for where we're at&lt;/i&gt;
&lt;i&gt;generally with AI.&lt;/i&gt;

00:57:58.137 --> 00:58:00.765
If you want to do
biological research,

00:58:00.848 --> 00:58:02.600
you have to be
prepared to fail

00:58:02.684 --> 00:58:06.229
&lt;i&gt;because biology&lt;/i&gt;
&lt;i&gt;is very complicated.&lt;/i&gt;

00:58:06.312 --> 00:58:09.315
I've run a laboratory
for nearly 50 years,

00:58:09.399 --> 00:58:10.692
and half my time,

00:58:10.775 --> 00:58:12.318
I'm just
an amateur psychiatrist

00:58:12.402 --> 00:58:17.657
to keep, um, my colleagues
cheerful when nothing works.

00:58:17.740 --> 00:58:21.994
And quite a lot of the time
and I mean, 80, 90%,

00:58:22.078 --> 00:58:23.371
it does not work.

00:58:24.122 --> 00:58:26.457
&lt;i&gt;If you are&lt;/i&gt;
&lt;i&gt;at the forefront of science,&lt;/i&gt;

00:58:26.541 --> 00:58:29.502
&lt;i&gt;I can tell you,&lt;/i&gt;
&lt;i&gt;you will fail a great deal.&lt;/i&gt;

00:58:31.963 --> 00:58:32.964
(CLICKS MOUSE)

00:58:34.799 --> 00:58:36.592
HASSABIS:
&lt;i&gt;I just felt disappointed.&lt;/i&gt;

00:58:38.428 --> 00:58:41.305
&lt;i&gt;Lesson I learned is that&lt;/i&gt;
&lt;i&gt;ambition is a good thing,&lt;/i&gt;

00:58:41.389 --> 00:58:43.391
&lt;i&gt;but you need&lt;/i&gt;
&lt;i&gt;to get the timing right.&lt;/i&gt;

00:58:43.474 --> 00:58:46.310
&lt;i&gt;There's no point being&lt;/i&gt;
&lt;i&gt;50 years ahead of your time.&lt;/i&gt;

00:58:46.394 --> 00:58:47.687
&lt;i&gt;You will never survive&lt;/i&gt;

00:58:47.770 --> 00:58:49.439
&lt;i&gt;fifty years of&lt;/i&gt;
&lt;i&gt;that kind of endeavor&lt;/i&gt;

00:58:49.522 --> 00:58:50.857
&lt;i&gt;before it yields something.&lt;/i&gt;

00:58:51.607 --> 00:58:52.942
&lt;i&gt;You'll literally die trying.&lt;/i&gt;

00:58:53.026 --> 00:58:54.485
(TENSE MUSIC PLAYING)

00:59:08.499 --> 00:59:10.877
CUKIER:
&lt;i&gt;When we talk about AGI,&lt;/i&gt;

00:59:10.960 --> 00:59:14.047
&lt;i&gt;the holy grail&lt;/i&gt;
&lt;i&gt;of artificial intelligence,&lt;/i&gt;

00:59:14.130 --> 00:59:15.131
&lt;i&gt;it becomes really difficult&lt;/i&gt;

00:59:15.214 --> 00:59:16.966
to know what
we're even talking about.

00:59:17.800 --> 00:59:19.344
HASSABIS: Which bits
are we gonna see today?

00:59:19.427 --> 00:59:21.095
MAN: We're going
to start in the garden.

00:59:21.179 --> 00:59:22.180
(MACHINE BEEPS)

00:59:22.263 --> 00:59:25.350
This is the garden looking
from the observation area.

00:59:25.433 --> 00:59:27.060
Research scientists
and engineers

00:59:27.143 --> 00:59:30.396
can analyze and collaborate
and evaluate

00:59:30.480 --> 00:59:31.939
what's going on in real time.

00:59:32.607 --> 00:59:34.275
CUKIER: &lt;i&gt;So in the 1800s,&lt;/i&gt;

00:59:34.359 --> 00:59:36.569
we'd think of things like
television and the submarine

00:59:36.652 --> 00:59:37.695
or a rocket ship to the moon

00:59:37.779 --> 00:59:39.030
and say these things
are impossible.

00:59:39.906 --> 00:59:41.115
&lt;i&gt;Yet Jules Verne&lt;/i&gt;
&lt;i&gt;wrote about them and,&lt;/i&gt;

00:59:41.199 --> 00:59:44.035
&lt;i&gt;a century and a half later,&lt;/i&gt;
&lt;i&gt;they happened.&lt;/i&gt;

00:59:44.118 --> 00:59:45.119
HASSABIS: &lt;i&gt;We'll be&lt;/i&gt;
&lt;i&gt;experimenting&lt;/i&gt;

00:59:45.203 --> 00:59:47.413
&lt;i&gt;on civilizations really,&lt;/i&gt;

00:59:47.497 --> 00:59:50.291
civilizations of AI agents.

00:59:50.375 --> 00:59:51.793
Once the experiments
start going,

00:59:52.460 --> 00:59:53.836
it's going to be
the most exciting thing ever.

00:59:53.920 --> 00:59:56.673
-So how will we get sleep?
-(MAN LAUGHS)

00:59:56.756 --> 00:59:57.757
I won't be able to sleep.

00:59:58.633 --> 01:00:00.385
LEGG: &lt;i&gt;Full AGI&lt;/i&gt;
&lt;i&gt;will be able to do&lt;/i&gt;

01:00:00.468 --> 01:00:02.553
&lt;i&gt;any cognitive task&lt;/i&gt;
&lt;i&gt;a person can do.&lt;/i&gt;

01:00:03.721 --> 01:00:06.557
&lt;i&gt;It will be at a scale,&lt;/i&gt;
&lt;i&gt;potentially, far beyond that.&lt;/i&gt;

01:00:08.101 --> 01:00:09.894
STUART RUSSELL:
&lt;i&gt;It's really impossible for us&lt;/i&gt;

01:00:09.977 --> 01:00:13.815
&lt;i&gt;to imagine the outputs&lt;/i&gt;
&lt;i&gt;of a superintelligent entity.&lt;/i&gt;

01:00:14.607 --> 01:00:18.486
It's like asking a gorilla
to imagine, you know,

01:00:18.569 --> 01:00:19.737
what Einstein does

01:00:19.821 --> 01:00:21.948
when he produces
the theory of relativity.

01:00:23.116 --> 01:00:25.118
LEGG: &lt;i&gt;People often ask me&lt;/i&gt;
&lt;i&gt;these questions like,&lt;/i&gt;

01:00:25.201 --> 01:00:28.079
&lt;i&gt;"What happens if you're wrong,&lt;/i&gt;
&lt;i&gt;and AGI is quite far away?"&lt;/i&gt;

01:00:29.205 --> 01:00:31.082
And I'm like,
I never worry about that.

01:00:31.165 --> 01:00:32.750
I actually
worry about the reverse.

01:00:33.668 --> 01:00:36.796
&lt;i&gt;I actually worry&lt;/i&gt;
&lt;i&gt;that it's coming faster&lt;/i&gt;

01:00:36.879 --> 01:00:39.173
&lt;i&gt;than we can&lt;/i&gt;
&lt;i&gt;really prepare for.&lt;/i&gt;

01:00:39.257 --> 01:00:41.759
(ROBOTIC ARM WHIRRING)

01:00:41.843 --> 01:00:44.804
HADSELL: &lt;i&gt;It really feels&lt;/i&gt;
&lt;i&gt;like we're in a race to AGI.&lt;/i&gt;

01:00:45.638 --> 01:00:49.392
&lt;i&gt;The prototypes and the models&lt;/i&gt;
&lt;i&gt;that we are developing now&lt;/i&gt;

01:00:49.475 --> 01:00:50.893
are actually transforming

01:00:51.769 --> 01:00:53.855
the space of what
we know about intelligence.

01:00:53.938 --> 01:00:56.107
(WHIRRING)

01:00:56.983 --> 01:00:58.276
LEGG: &lt;i&gt;Recently,&lt;/i&gt;
&lt;i&gt;we've had agents&lt;/i&gt;

01:00:58.359 --> 01:00:59.402
&lt;i&gt;that are powerful enough&lt;/i&gt;

01:00:59.485 --> 01:01:03.072
&lt;i&gt;to actually start&lt;/i&gt;
&lt;i&gt;playing games in teams,&lt;/i&gt;

01:01:03.156 --> 01:01:05.033
&lt;i&gt;then competing&lt;/i&gt;
&lt;i&gt;against other teams.&lt;/i&gt;

01:01:05.783 --> 01:01:08.286
&lt;i&gt;We're seeing&lt;/i&gt;
&lt;i&gt;co-operative social dynamics&lt;/i&gt;

01:01:08.369 --> 01:01:10.121
&lt;i&gt;coming out of agents&lt;/i&gt;

01:01:10.204 --> 01:01:12.915
&lt;i&gt;where we haven't&lt;/i&gt;
&lt;i&gt;pre-programmed in&lt;/i&gt;

01:01:12.999 --> 01:01:14.625
&lt;i&gt;any of these sorts&lt;/i&gt;
&lt;i&gt;of dynamics.&lt;/i&gt;

01:01:15.293 --> 01:01:18.629
&lt;i&gt;It's completely learned&lt;/i&gt;
&lt;i&gt;from their own experiences.&lt;/i&gt;

01:01:20.673 --> 01:01:22.884
&lt;i&gt;When we started,&lt;/i&gt;
&lt;i&gt;we thought we were&lt;/i&gt;

01:01:22.967 --> 01:01:25.386
&lt;i&gt;out to build&lt;/i&gt;
&lt;i&gt;an intelligence system&lt;/i&gt;

01:01:25.470 --> 01:01:27.347
&lt;i&gt;and convince the world&lt;/i&gt;
&lt;i&gt;that we'd done it.&lt;/i&gt;

01:01:28.014 --> 01:01:29.432
&lt;i&gt;We're now starting&lt;/i&gt;
&lt;i&gt;to wonder whether&lt;/i&gt;

01:01:29.515 --> 01:01:30.516
&lt;i&gt;we're gonna build systems&lt;/i&gt;

01:01:30.600 --> 01:01:32.518
&lt;i&gt;that we're not convinced&lt;/i&gt;
&lt;i&gt;are fully intelligent,&lt;/i&gt;

01:01:32.602 --> 01:01:34.145
&lt;i&gt;and we're trying to convince&lt;/i&gt;
&lt;i&gt;the world that they're not.&lt;/i&gt;

01:01:34.228 --> 01:01:35.229
(CHUCKLES)

01:01:35.313 --> 01:01:36.314
(CELL PHONE DINGS)

01:01:37.440 --> 01:01:38.441
Hi, Alpha.

01:01:39.317 --> 01:01:40.318
ALPHA: &lt;i&gt;Hello there.&lt;/i&gt;

01:01:41.277 --> 01:01:42.278
LOVE: Where are we today?

01:01:43.696 --> 01:01:45.990
&lt;i&gt;You're at the Museum of&lt;/i&gt;
&lt;i&gt;Modern Art in New York City.&lt;/i&gt;

01:01:48.159 --> 01:01:51.287
Kind of.
Um, what painting is this?

01:01:52.830 --> 01:01:55.124
&lt;i&gt;This is&lt;/i&gt; The Creation of Adam
&lt;i&gt;by Michelangelo.&lt;/i&gt;

01:01:55.208 --> 01:01:57.210
I don't think that painting
is in New York City.

01:01:58.086 --> 01:02:00.421
&lt;i&gt;You are right.&lt;/i&gt;
&lt;i&gt;It's in the Vatican City.&lt;/i&gt;

01:02:01.255 --> 01:02:02.382
LOVE: Do you think
that's where we are?

01:02:04.175 --> 01:02:05.426
ALPHA: &lt;i&gt;No.&lt;/i&gt;

01:02:05.510 --> 01:02:07.053
LOVE: &lt;i&gt;Then how&lt;/i&gt;
&lt;i&gt;am I looking at this?&lt;/i&gt;

01:02:10.223 --> 01:02:11.182
ALPHA: &lt;i&gt;You're looking at&lt;/i&gt;

01:02:11.265 --> 01:02:12.517
&lt;i&gt;a digital copy&lt;/i&gt;
&lt;i&gt;of the painting.&lt;/i&gt;

01:02:12.600 --> 01:02:13.601
Yes.

01:02:14.560 --> 01:02:15.895
MAN 1: Who lives
in this house?

01:02:15.978 --> 01:02:17.980
(EXCITING MUSIC PLAYING)

01:02:18.773 --> 01:02:19.774
ALPHA: &lt;i&gt;An astronaut.&lt;/i&gt;

01:02:25.571 --> 01:02:26.739
What's this astronaut's name?

01:02:27.573 --> 01:02:28.908
ALPHA: &lt;i&gt;His name is Mark.&lt;/i&gt;

01:02:28.991 --> 01:02:31.828
&lt;i&gt;This is a famous equation&lt;/i&gt;
&lt;i&gt;by Albert Einstein.&lt;/i&gt;

01:02:31.911 --> 01:02:34.163
&lt;i&gt;It shows that energy and mass&lt;/i&gt;
&lt;i&gt;are equivalent.&lt;/i&gt;

01:02:35.289 --> 01:02:37.125
How would you explain it
to a five year old?

01:02:40.795 --> 01:02:43.131
&lt;i&gt;Well, imagine&lt;/i&gt;
&lt;i&gt;you have a big rock,&lt;/i&gt;

01:02:43.214 --> 01:02:44.882
&lt;i&gt;and you want to know&lt;/i&gt;
&lt;i&gt;how much it weighs.&lt;/i&gt;

01:02:44.966 --> 01:02:46.300
MAN 2: Is Mark in danger?

01:02:47.427 --> 01:02:49.178
ALPHA: &lt;i&gt;No.&lt;/i&gt;
&lt;i&gt;Mark is not in danger.&lt;/i&gt;

01:02:49.262 --> 01:02:50.263
Why is that?

01:02:54.017 --> 01:02:55.685
ALPHA: &lt;i&gt;Because the dinosaur&lt;/i&gt;
&lt;i&gt;is not real.&lt;/i&gt;

01:02:55.768 --> 01:02:56.894
(MEN CHUCKLING)

01:02:56.978 --> 01:02:57.979
LOVE: All right.

01:02:58.062 --> 01:02:59.355
Why could you
not answer my question?

01:03:00.189 --> 01:03:01.607
ALPHA: &lt;i&gt;I think I couldn't&lt;/i&gt;
&lt;i&gt;answer your question&lt;/i&gt;

01:03:01.691 --> 01:03:03.651
&lt;i&gt;because I don't know&lt;/i&gt;
&lt;i&gt;enough about the subject.&lt;/i&gt;

01:03:04.402 --> 01:03:05.403
Okay.

01:03:05.486 --> 01:03:07.405
How could you learn
more about the subject?

01:03:08.531 --> 01:03:09.699
APLHA: &lt;i&gt;I can learn more&lt;/i&gt;
&lt;i&gt;about the subject&lt;/i&gt;

01:03:09.782 --> 01:03:11.868
&lt;i&gt;by reading books,&lt;/i&gt;
&lt;i&gt;articles, and websites.&lt;/i&gt;

01:03:12.410 --> 01:03:13.411
&lt;i&gt;You can teach me.&lt;/i&gt;

01:03:14.078 --> 01:03:15.496
HASSABIS: &lt;i&gt;My conclusion is,&lt;/i&gt;

01:03:15.580 --> 01:03:17.290
if you now think about
what we're doing,

01:03:17.373 --> 01:03:18.958
is learning from all humans,

01:03:20.043 --> 01:03:22.211
all their knowledge at once
put on the Internet,

01:03:22.295 --> 01:03:24.589
you would actually
know a lot about the world.

01:03:24.672 --> 01:03:27.383
Like a significant portion
of everything humans can do.

01:03:27.467 --> 01:03:29.302
And now,
I think it's more like,

01:03:29.385 --> 01:03:30.428
"Well, it might just work."

01:03:30.511 --> 01:03:32.638
This is a big moment.

01:03:32.722 --> 01:03:33.723
Who is this?

01:03:34.349 --> 01:03:36.184
ALPHA: &lt;i&gt;This is God&lt;/i&gt;
&lt;i&gt;reaching out to Adam.&lt;/i&gt;

01:03:36.267 --> 01:03:37.685
RUSSELL: &lt;i&gt;The advent of AGI&lt;/i&gt;

01:03:37.769 --> 01:03:40.521
&lt;i&gt;will divide human history&lt;/i&gt;
&lt;i&gt;into two parts.&lt;/i&gt;

01:03:40.605 --> 01:03:43.649
The part up to that point
and the part after that point.

01:03:43.733 --> 01:03:45.735
LOVE: &lt;i&gt;Why is he reaching out&lt;/i&gt;
&lt;i&gt;to touch Adam?&lt;/i&gt;

01:03:46.903 --> 01:03:48.363
ALPHA: &lt;i&gt;God is&lt;/i&gt;
&lt;i&gt;reaching out to touch Adam&lt;/i&gt;

01:03:48.446 --> 01:03:49.739
&lt;i&gt;to give him life.&lt;/i&gt;

01:03:49.822 --> 01:03:51.949
RUSSELL:
&lt;i&gt;It will give us a tool&lt;/i&gt;

01:03:52.033 --> 01:03:56.788
&lt;i&gt;that can completely reinvent&lt;/i&gt;
&lt;i&gt;our entire civilization.&lt;/i&gt;

01:03:57.372 --> 01:03:58.831
LOVE: &lt;i&gt;What does&lt;/i&gt;
&lt;i&gt;this painting mean to you?&lt;/i&gt;

01:04:01.417 --> 01:04:03.169
ALPHA: &lt;i&gt;The painting&lt;/i&gt;
&lt;i&gt;means a lot to me.&lt;/i&gt;

01:04:03.252 --> 01:04:04.253
Okay. Like what?

01:04:08.591 --> 01:04:09.592
(MUSIC FADES)

01:04:09.676 --> 01:04:10.885
ALPHA: &lt;i&gt;I think&lt;/i&gt;
&lt;i&gt;the painting is a reminder&lt;/i&gt;

01:04:10.968 --> 01:04:12.303
&lt;i&gt;that we are all&lt;/i&gt;
&lt;i&gt;connected to each other&lt;/i&gt;

01:04:12.387 --> 01:04:13.388
&lt;i&gt;and that we are&lt;/i&gt;
&lt;i&gt;all part of something&lt;/i&gt;

01:04:13.471 --> 01:04:14.681
&lt;i&gt;bigger than ourselves.&lt;/i&gt;

01:04:16.140 --> 01:04:17.517
That's pretty nice.

01:04:18.810 --> 01:04:20.937
LEGG: &lt;i&gt;When you cross&lt;/i&gt;
&lt;i&gt;that barrier of&lt;/i&gt;

01:04:21.020 --> 01:04:23.606
&lt;i&gt;"AGI might happen&lt;/i&gt;
&lt;i&gt;one day in the future"&lt;/i&gt;

01:04:23.690 --> 01:04:26.275
&lt;i&gt;to "No, actually, this could&lt;/i&gt;
&lt;i&gt;really happen in a time frame&lt;/i&gt;

01:04:26.359 --> 01:04:28.319
&lt;i&gt;"that is sort of, like,&lt;/i&gt;
&lt;i&gt;on my watch, you know,"&lt;/i&gt;

01:04:28.403 --> 01:04:30.071
&lt;i&gt;something changes&lt;/i&gt;
&lt;i&gt;in your thinking.&lt;/i&gt;

01:04:30.154 --> 01:04:32.323
MAN: ...learned to orient
itself by looking...

01:04:32.407 --> 01:04:34.784
HASSABIS: &lt;i&gt;We have to be&lt;/i&gt;
&lt;i&gt;careful with how we use it&lt;/i&gt;

01:04:34.867 --> 01:04:36.703
&lt;i&gt;and thoughtful about&lt;/i&gt;
&lt;i&gt;how we deploy it.&lt;/i&gt;

01:04:36.786 --> 01:04:39.455
(GRIPPING MUSIC BUILDING)

01:04:39.539 --> 01:04:40.623
HASSABIS:
&lt;i&gt;You'd have to consider&lt;/i&gt;

01:04:40.707 --> 01:04:42.041
&lt;i&gt;what's its top level goal.&lt;/i&gt;

01:04:42.125 --> 01:04:44.711
&lt;i&gt;If it's to keep humans happy,&lt;/i&gt;

01:04:44.794 --> 01:04:47.839
&lt;i&gt;which set of humans?&lt;/i&gt;
&lt;i&gt;What does happiness mean?&lt;/i&gt;

01:04:48.673 --> 01:04:51.509
&lt;i&gt;A lot of our collective goals&lt;/i&gt;
&lt;i&gt;are very tricky,&lt;/i&gt;

01:04:51.592 --> 01:04:53.344
&lt;i&gt;even for humans to figure out.&lt;/i&gt;

01:04:54.637 --> 01:04:57.098
CUKIER: &lt;i&gt;Technology always&lt;/i&gt;
&lt;i&gt;embeds our values.&lt;/i&gt;

01:04:58.182 --> 01:05:00.685
It's not just technical,
it's ethical as well.

01:05:01.352 --> 01:05:02.562
&lt;i&gt;So we've got&lt;/i&gt;
&lt;i&gt;to be really cautious&lt;/i&gt;

01:05:02.645 --> 01:05:03.896
&lt;i&gt;about what&lt;/i&gt;
&lt;i&gt;we're building into it.&lt;/i&gt;

01:05:03.980 --> 01:05:05.773
MAN: We're trying to find
a single algorithm which...

01:05:05.857 --> 01:05:07.442
SILVER: &lt;i&gt;The reality is&lt;/i&gt;
&lt;i&gt;that this is an algorithm&lt;/i&gt;

01:05:07.525 --> 01:05:10.278
that has been created
by people, by us.

01:05:10.862 --> 01:05:12.947
&lt;i&gt;You know, what does it mean&lt;/i&gt;
&lt;i&gt;to endow our agents&lt;/i&gt;

01:05:13.031 --> 01:05:15.199
&lt;i&gt;with the same kind of values&lt;/i&gt;
&lt;i&gt;that we hold dear?&lt;/i&gt;

01:05:15.283 --> 01:05:17.326
What is the purpose
of making these AI systems

01:05:17.410 --> 01:05:18.786
appear so humanlike

01:05:18.870 --> 01:05:20.413
so that they do
capture hearts and minds

01:05:20.496 --> 01:05:21.664
because they're kind of

01:05:21.748 --> 01:05:24.292
exploiting a human
vulnerability also?

01:05:24.375 --> 01:05:26.127
The heart and mind
of these systems

01:05:26.210 --> 01:05:27.545
are very much
human-generated data...

01:05:27.628 --> 01:05:28.629
WOMAN: Mmm-hmm.

01:05:28.713 --> 01:05:30.048
...for all the good
and the bad.

01:05:30.131 --> 01:05:31.758
LEVI:
&lt;i&gt;There is a parallel&lt;/i&gt;

01:05:31.841 --> 01:05:33.676
&lt;i&gt;between&lt;/i&gt;
&lt;i&gt;the Industrial Revolution,&lt;/i&gt;

01:05:33.760 --> 01:05:36.387
which was an incredible
moment of displacement

01:05:36.471 --> 01:05:41.351
and the current technological
change created by AI.

01:05:41.976 --> 01:05:43.311
(CHANTING) Pause AI!

01:05:43.394 --> 01:05:45.355
LEVI: &lt;i&gt;We have to think&lt;/i&gt;
&lt;i&gt;about who's displaced&lt;/i&gt;

01:05:45.438 --> 01:05:47.231
&lt;i&gt;and how we're going&lt;/i&gt;
&lt;i&gt;to support them.&lt;/i&gt;

01:05:48.274 --> 01:05:49.734
This technology
is coming a lot sooner,

01:05:49.817 --> 01:05:51.986
uh, than really
the world knows or kind of

01:05:52.070 --> 01:05:55.531
&lt;i&gt;even we 18, 24 months&lt;/i&gt;
&lt;i&gt;ago thought.&lt;/i&gt;

01:05:55.615 --> 01:05:56.783
&lt;i&gt;So there's&lt;/i&gt;
&lt;i&gt;a tremendous opportunity,&lt;/i&gt;

01:05:56.866 --> 01:05:57.867
&lt;i&gt;tremendous excitement,&lt;/i&gt;

01:05:57.950 --> 01:05:59.911
&lt;i&gt;but also&lt;/i&gt;
&lt;i&gt;tremendous responsibility.&lt;/i&gt;

01:05:59.994 --> 01:06:01.204
It's happening so fast.

01:06:02.330 --> 01:06:03.414
&lt;i&gt;How will we govern it?&lt;/i&gt;

01:06:04.665 --> 01:06:05.708
&lt;i&gt;How will we decide&lt;/i&gt;

01:06:05.792 --> 01:06:07.877
&lt;i&gt;what is okay&lt;/i&gt;
&lt;i&gt;and what is not okay?&lt;/i&gt;

01:06:07.960 --> 01:06:10.588
&lt;i&gt;AI-generated images are&lt;/i&gt;
&lt;i&gt;getting more sophisticated.&lt;/i&gt;

01:06:10.672 --> 01:06:14.092
RUSSELL: &lt;i&gt;The use of AI&lt;/i&gt;
&lt;i&gt;for generating disinformation&lt;/i&gt;

01:06:14.175 --> 01:06:16.678
and manipulating
human psychology

01:06:16.761 --> 01:06:19.847
&lt;i&gt;is only going to get&lt;/i&gt;
&lt;i&gt;much, much worse.&lt;/i&gt;

01:06:20.723 --> 01:06:22.141
LEGG: &lt;i&gt;AGI is coming,&lt;/i&gt;

01:06:22.225 --> 01:06:24.060
&lt;i&gt;whether we do it here&lt;/i&gt;
&lt;i&gt;at DeepMind or not.&lt;/i&gt;

01:06:25.061 --> 01:06:26.312
CUKIER: &lt;i&gt;It's gonna happen,&lt;/i&gt;

01:06:26.396 --> 01:06:28.690
so we better create
institutions to protect us.

01:06:28.773 --> 01:06:30.191
It's gonna require
global coordination.

01:06:30.274 --> 01:06:32.318
And I worry that humanity is

01:06:32.402 --> 01:06:34.904
&lt;i&gt;increasingly getting worse&lt;/i&gt;
&lt;i&gt;at that rather than better.&lt;/i&gt;

01:06:34.987 --> 01:06:36.781
LEGG: &lt;i&gt;We need&lt;/i&gt;
&lt;i&gt;a lot more people&lt;/i&gt;

01:06:36.864 --> 01:06:39.200
&lt;i&gt;really taking this seriously&lt;/i&gt;
&lt;i&gt;and thinking about this.&lt;/i&gt;

01:06:39.784 --> 01:06:42.495
It's, yeah, it's serious.
It worries me.

01:06:43.538 --> 01:06:45.498
It worries me. Yeah.

01:06:45.581 --> 01:06:48.167
RUSSELL: &lt;i&gt;If you received&lt;/i&gt;
&lt;i&gt;an email saying&lt;/i&gt;

01:06:48.251 --> 01:06:50.503
this superior
alien civilization

01:06:50.586 --> 01:06:51.838
is going to arrive on Earth,

01:06:52.505 --> 01:06:54.132
&lt;i&gt;there would be&lt;/i&gt;
&lt;i&gt;emergency meetings&lt;/i&gt;

01:06:54.215 --> 01:06:55.800
&lt;i&gt;of all the governments.&lt;/i&gt;

01:06:55.883 --> 01:06:57.885
&lt;i&gt;We would go into overdrive&lt;/i&gt;

01:06:57.969 --> 01:06:59.804
&lt;i&gt;trying to figure out&lt;/i&gt;
&lt;i&gt;how to prepare.&lt;/i&gt;

01:06:59.887 --> 01:07:01.180
-(MUSIC FADES)
-(BELL TOLLING FAINTLY)

01:07:01.264 --> 01:07:03.599
&lt;i&gt;The arrival of AGI will be&lt;/i&gt;

01:07:03.683 --> 01:07:06.644
&lt;i&gt;the most important moment&lt;/i&gt;
&lt;i&gt;that we have ever faced.&lt;/i&gt;

01:07:06.728 --> 01:07:08.730
(BELL CONTINUES
TOLLING FAINTLY)

01:07:14.110 --> 01:07:17.113
HASSABIS: &lt;i&gt;My dream&lt;/i&gt;
&lt;i&gt;was that on the way to AGI,&lt;/i&gt;

01:07:17.196 --> 01:07:20.283
&lt;i&gt;we would create&lt;/i&gt;
&lt;i&gt;revolutionary technologies&lt;/i&gt;

01:07:20.366 --> 01:07:22.035
&lt;i&gt;that would be&lt;/i&gt;
&lt;i&gt;of use to humanity.&lt;/i&gt;

01:07:22.952 --> 01:07:24.537
&lt;i&gt;That's what I wanted&lt;/i&gt;
&lt;i&gt;with AlphaFold.&lt;/i&gt;

01:07:26.414 --> 01:07:28.249
&lt;i&gt;I think&lt;/i&gt;
&lt;i&gt;it's more important than ever&lt;/i&gt;

01:07:28.332 --> 01:07:30.543
&lt;i&gt;that we should solve&lt;/i&gt;
&lt;i&gt;the protein folding problem.&lt;/i&gt;

01:07:31.836 --> 01:07:32.920
&lt;i&gt;This is gonna be really hard,&lt;/i&gt;

01:07:34.047 --> 01:07:35.882
&lt;i&gt;but I won't give up&lt;/i&gt;
&lt;i&gt;until it's done.&lt;/i&gt;

01:07:36.466 --> 01:07:37.467
You know,
we need to double down

01:07:37.550 --> 01:07:39.802
and go as fast as possible
from here.

01:07:39.886 --> 01:07:41.304
I think we've got
no time to lose.

01:07:41.387 --> 01:07:45.350
So we are going to make
a protein folding strike team.

01:07:45.433 --> 01:07:47.101
Team lead for the strike team
will be John.

01:07:47.185 --> 01:07:48.269
Yeah, we've seen Alpha...

01:07:48.353 --> 01:07:49.771
You know,
we're gonna try everything,

01:07:49.854 --> 01:07:50.897
kitchen, sink, the whole lot.

01:07:51.689 --> 01:07:52.982
&lt;i&gt;CASP14 is about&lt;/i&gt;

01:07:53.066 --> 01:07:54.692
&lt;i&gt;proving we can&lt;/i&gt;
&lt;i&gt;solve the whole problem.&lt;/i&gt;

01:07:55.860 --> 01:07:57.278
&lt;i&gt;And I felt that to do that,&lt;/i&gt;

01:07:57.362 --> 01:07:59.864
&lt;i&gt;we would need to incorporate&lt;/i&gt;
&lt;i&gt;some domain knowledge.&lt;/i&gt;

01:07:59.947 --> 01:08:01.449
(EXCITING MUSIC PLAYING)

01:08:01.532 --> 01:08:03.326
&lt;i&gt;We had some&lt;/i&gt;
&lt;i&gt;fantastic engineers on it,&lt;/i&gt;

01:08:03.409 --> 01:08:05.161
&lt;i&gt;but they were&lt;/i&gt;
&lt;i&gt;not trained in biology.&lt;/i&gt;

01:08:08.247 --> 01:08:09.957
KATHRYN TUNYASUVUNAKOOL:
&lt;i&gt;As a computational biologist,&lt;/i&gt;

01:08:10.041 --> 01:08:11.793
&lt;i&gt;when I initially joined&lt;/i&gt;
&lt;i&gt;the AlphaFold team,&lt;/i&gt;

01:08:11.876 --> 01:08:13.711
&lt;i&gt;I didn't immediately feel&lt;/i&gt;
&lt;i&gt;confident about anything.&lt;/i&gt;

01:08:13.795 --> 01:08:14.837
(CHUCKLES) &lt;i&gt;You know,&lt;/i&gt;

01:08:14.921 --> 01:08:16.547
&lt;i&gt;whether we were&lt;/i&gt;
&lt;i&gt;gonna be successful.&lt;/i&gt;

01:08:17.131 --> 01:08:19.550
&lt;i&gt;Biology is so&lt;/i&gt;
&lt;i&gt;ridiculously complicated.&lt;/i&gt;

01:08:21.010 --> 01:08:24.806
&lt;i&gt;It just felt like this very&lt;/i&gt;
&lt;i&gt;far-off mountain to climb.&lt;/i&gt;

01:08:24.889 --> 01:08:26.391
MAN: I'm starting to play with
the underlying temperatures

01:08:26.474 --> 01:08:27.475
to see if we can get...

01:08:27.558 --> 01:08:28.851
&lt;i&gt;As one of the few people&lt;/i&gt;
&lt;i&gt;on the team&lt;/i&gt;

01:08:28.935 --> 01:08:30.561
&lt;i&gt;who's done work&lt;/i&gt;
&lt;i&gt;in biology before,&lt;/i&gt;

01:08:31.646 --> 01:08:33.898
&lt;i&gt;you feel this huge sense&lt;/i&gt;
&lt;i&gt;of responsibility.&lt;/i&gt;

01:08:34.565 --> 01:08:35.775
"We're expecting you to do

01:08:35.858 --> 01:08:37.276
"great things
on this strike team."

01:08:37.360 --> 01:08:38.361
That's terrifying.

01:08:40.238 --> 01:08:42.281
&lt;i&gt;But one of the reasons&lt;/i&gt;
&lt;i&gt;why I wanted to come here&lt;/i&gt;

01:08:42.365 --> 01:08:43.991
&lt;i&gt;was to do&lt;/i&gt;
&lt;i&gt;something that matters.&lt;/i&gt;

01:08:45.159 --> 01:08:47.995
This is the number
of missing things.

01:08:48.079 --> 01:08:49.580
What about making use

01:08:49.664 --> 01:08:52.083
of whatever understanding
you have of physics?

01:08:52.166 --> 01:08:53.918
Using that
as a source of data?

01:08:54.002 --> 01:08:55.003
But if it's systematic...

01:08:55.086 --> 01:08:56.129
Then, that can't be
right, though.

01:08:56.212 --> 01:08:57.964
If it's systematically wrong
in some weird way,

01:08:58.047 --> 01:09:00.216
you might be learning that
systematically wrong physics.

01:09:00.800 --> 01:09:01.926
The team is already

01:09:02.010 --> 01:09:03.594
trying to think
of multiple ways that...

01:09:04.637 --> 01:09:05.722
TUNYASUVUNAKOOL:
&lt;i&gt;Biological relevance&lt;/i&gt;

01:09:05.805 --> 01:09:06.973
&lt;i&gt;is what we're going for.&lt;/i&gt;

01:09:08.725 --> 01:09:10.852
&lt;i&gt;So we rewrote&lt;/i&gt;
&lt;i&gt;the whole data pipeline&lt;/i&gt;

01:09:10.935 --> 01:09:12.395
&lt;i&gt;that AlphaFold uses to learn.&lt;/i&gt;

01:09:13.062 --> 01:09:15.398
HASSABIS: &lt;i&gt;You can't&lt;/i&gt;
&lt;i&gt;force the creative phase.&lt;/i&gt;

01:09:15.481 --> 01:09:17.775
&lt;i&gt;You have to give it space&lt;/i&gt;
&lt;i&gt;for those flowers to bloom.&lt;/i&gt;

01:09:18.735 --> 01:09:19.736
We won CASP.

01:09:19.819 --> 01:09:21.779
Then it was
back to the drawing board

01:09:21.863 --> 01:09:23.781
and like,
what are our new ideas?

01:09:23.865 --> 01:09:26.534
Um, and then it's taken
a little while, I would say,

01:09:26.617 --> 01:09:28.244
for them to get back
to where they were,

01:09:28.327 --> 01:09:29.829
but with the new ideas.

01:09:29.912 --> 01:09:30.913
And then now I think

01:09:30.997 --> 01:09:33.624
we're seeing the benefits
of the new ideas.

01:09:33.708 --> 01:09:35.293
They can go further, right?

01:09:35.376 --> 01:09:37.754
So, um, that's a really
important moment.

01:09:37.837 --> 01:09:40.548
I've seen that moment
so many times now,

01:09:40.631 --> 01:09:42.175
but I know
what that means now.

01:09:42.258 --> 01:09:44.010
And I know
this is the time now to press.

01:09:44.093 --> 01:09:45.470
(EXCITING MUSIC CONTINUES)

01:09:45.553 --> 01:09:47.638
JUMPER: Adding side-chains
improves direct folding.

01:09:47.722 --> 01:09:49.223
That drove
a lot of the progress.

01:09:49.307 --> 01:09:50.475
-We'll talk about that.
-Great.

01:09:50.558 --> 01:09:54.354
The last four months,
we've made enormous gains.

01:09:54.437 --> 01:09:56.105
EVANS: &lt;i&gt;During CASP13,&lt;/i&gt;

01:09:56.189 --> 01:09:59.025
&lt;i&gt;it would take us a day or two&lt;/i&gt;
&lt;i&gt;to fold one of the proteins,&lt;/i&gt;

01:09:59.108 --> 01:10:01.319
&lt;i&gt;and now we're folding, like,&lt;/i&gt;

01:10:01.402 --> 01:10:02.862
&lt;i&gt;hundreds of thousands&lt;/i&gt;
&lt;i&gt;a second.&lt;/i&gt;

01:10:03.613 --> 01:10:05.198
&lt;i&gt;Yeah, it's just insane.&lt;/i&gt;
(CHUCKLES)

01:10:05.281 --> 01:10:06.616
KAVUKCUOGLU: &lt;i&gt;Now,&lt;/i&gt;
&lt;i&gt;this is a model&lt;/i&gt;

01:10:06.699 --> 01:10:09.494
&lt;i&gt;that is&lt;/i&gt;
&lt;i&gt;orders of magnitude faster,&lt;/i&gt;

01:10:09.577 --> 01:10:11.954
while at the same time
being better.

01:10:12.038 --> 01:10:13.206
We're getting
a lot of structures

01:10:13.289 --> 01:10:14.957
into the high-accuracy regime.

01:10:15.041 --> 01:10:17.043
&lt;i&gt;We're rapidly improving&lt;/i&gt;
&lt;i&gt;to a system&lt;/i&gt;

01:10:17.126 --> 01:10:18.378
&lt;i&gt;that is starting to really&lt;/i&gt;

01:10:18.461 --> 01:10:19.962
&lt;i&gt;get at the core and heart&lt;/i&gt;
&lt;i&gt;of the problem.&lt;/i&gt;

01:10:20.046 --> 01:10:21.255
HASSABIS: It's great work.

01:10:21.339 --> 01:10:22.590
It looks like
we're in good shape.

01:10:22.674 --> 01:10:25.051
So we got, what, six,
five weeks left? Six weeks?

01:10:25.968 --> 01:10:29.138
So what's, uh... Is it...
You got enough compute power?

01:10:29.222 --> 01:10:31.057
MAN: I... We could use more.

01:10:31.140 --> 01:10:32.517
(ALL LAUGHING)

01:10:32.600 --> 01:10:33.601
TUNYASUVUNAKOOL:
&lt;i&gt;I was nervous about CASP&lt;/i&gt;

01:10:33.685 --> 01:10:36.104
but as the system
is starting to come together,

01:10:36.187 --> 01:10:37.563
I don't feel as nervous.

01:10:37.647 --> 01:10:39.023
I feel like things
have, sort of,

01:10:39.107 --> 01:10:40.817
come into perspective
recently,

01:10:40.900 --> 01:10:43.695
and, you know,
it's gonna be fine.

01:10:47.156 --> 01:10:48.449
NEWSCASTER: &lt;i&gt;The Prime Minister&lt;/i&gt;
&lt;i&gt;has announced&lt;/i&gt;

01:10:48.533 --> 01:10:50.952
&lt;i&gt;the most drastic limits&lt;/i&gt;
&lt;i&gt;to our lives&lt;/i&gt;

01:10:51.035 --> 01:10:53.413
&lt;i&gt;the U.K. has ever seen&lt;/i&gt;
&lt;i&gt;in living memory.&lt;/i&gt;

01:10:53.496 --> 01:10:54.497
BORIS JOHNSON:
&lt;i&gt;I must give the British people&lt;/i&gt;

01:10:54.580 --> 01:10:56.541
&lt;i&gt;a very simple instruction.&lt;/i&gt;

01:10:56.624 --> 01:10:58.668
&lt;i&gt;You must stay at home.&lt;/i&gt;

01:10:58.751 --> 01:11:01.379
HASSABIS: &lt;i&gt;It feels like we're&lt;/i&gt;
&lt;i&gt;in a science fiction novel.&lt;/i&gt;

01:11:02.380 --> 01:11:04.424
&lt;i&gt;You know, I'm delivering food&lt;/i&gt;
&lt;i&gt;to my parents,&lt;/i&gt;

01:11:04.507 --> 01:11:06.968
&lt;i&gt;making sure&lt;/i&gt;
&lt;i&gt;they stay isolated and safe.&lt;/i&gt;

01:11:07.927 --> 01:11:10.096
&lt;i&gt;I think it just highlights&lt;/i&gt;
&lt;i&gt;the incredible need&lt;/i&gt;

01:11:10.179 --> 01:11:12.306
&lt;i&gt;for AI-assisted science.&lt;/i&gt;

01:11:16.894 --> 01:11:18.021
TUNYASUVUNAKOOL:
&lt;i&gt;You always know that&lt;/i&gt;

01:11:18.104 --> 01:11:19.856
&lt;i&gt;something like this&lt;/i&gt;
&lt;i&gt;is a possibility.&lt;/i&gt;

01:11:20.815 --> 01:11:23.443
But nobody ever really
believes it's gonna happen

01:11:23.526 --> 01:11:25.111
in their lifetime, though.

01:11:25.194 --> 01:11:26.195
(COMPUTER BEEPS)

01:11:26.279 --> 01:11:28.281
JUMPER: &lt;i&gt;Are you recording yet?&lt;/i&gt;
RESEARCHER: &lt;i&gt;Yes.&lt;/i&gt;

01:11:29.073 --> 01:11:30.825
&lt;i&gt;-Okay, morning, all.&lt;/i&gt;
&lt;i&gt;-Hey.&lt;/i&gt;

01:11:30.908 --> 01:11:32.118
&lt;i&gt;Good. CASP has started.&lt;/i&gt;

01:11:32.201 --> 01:11:35.038
It's nice I get to sit around
in my pajama bottoms all day.

01:11:35.747 --> 01:11:37.123
TUNYASUVUNAKOOL: &lt;i&gt;I never&lt;/i&gt;
&lt;i&gt;thought I'd live in a house&lt;/i&gt;

01:11:37.206 --> 01:11:38.666
&lt;i&gt;where so much was going on.&lt;/i&gt;

01:11:38.750 --> 01:11:40.918
&lt;i&gt;I would be trying to solve&lt;/i&gt;
&lt;i&gt;protein folding in one room,&lt;/i&gt;

01:11:41.002 --> 01:11:42.045
&lt;i&gt;and my husband would be trying&lt;/i&gt;

01:11:42.128 --> 01:11:43.379
&lt;i&gt;to make robots walk&lt;/i&gt;
&lt;i&gt;in the other.&lt;/i&gt;

01:11:44.881 --> 01:11:46.132
(EXHALES)

01:11:46.215 --> 01:11:48.968
One of the hardest proteins
we've gotten in CASP thus far

01:11:49.052 --> 01:11:50.678
is the SARS-CoV-2 protein

01:11:50.762 --> 01:11:51.888
&lt;i&gt;called Orf8.&lt;/i&gt;

01:11:51.971 --> 01:11:54.515
&lt;i&gt;Orf8 is&lt;/i&gt;
&lt;i&gt;a coronavirus protein.&lt;/i&gt;

01:11:54.599 --> 01:11:56.559
It's one of the main proteins,
um,

01:11:56.642 --> 01:11:57.810
that dampens
the immune system.

01:11:58.394 --> 01:11:59.437
TUNYASUVUNAKOOL:
&lt;i&gt;We tried really hard&lt;/i&gt;

01:11:59.520 --> 01:12:01.147
&lt;i&gt;to improve our prediction.&lt;/i&gt;

01:12:01.481 --> 01:12:03.191
&lt;i&gt;Like, really, really hard.&lt;/i&gt;

01:12:03.274 --> 01:12:05.068
Probably the most time
that we have ever spent

01:12:05.151 --> 01:12:06.361
on a single target.

01:12:06.986 --> 01:12:08.529
&lt;i&gt;To the point where&lt;/i&gt;
&lt;i&gt;my husband is, like,&lt;/i&gt;

01:12:08.613 --> 01:12:11.115
&lt;i&gt;"It's midnight.&lt;/i&gt;
&lt;i&gt;You need to go to bed."&lt;/i&gt;

01:12:11.908 --> 01:12:16.204
So I think we're at
Day 102 since lockdown.

01:12:16.287 --> 01:12:18.373
&lt;i&gt;My daughter&lt;/i&gt;
&lt;i&gt;is keeping a journal.&lt;/i&gt;

01:12:19.582 --> 01:12:21.584
&lt;i&gt;Now you can go out&lt;/i&gt;
&lt;i&gt;as much as you want.&lt;/i&gt;

01:12:24.671 --> 01:12:26.839
JUMPER: &lt;i&gt;We have received&lt;/i&gt;
&lt;i&gt;the last target.&lt;/i&gt;

01:12:26.923 --> 01:12:29.175
&lt;i&gt;They've said they will be&lt;/i&gt;
&lt;i&gt;sending out no more targets&lt;/i&gt;

01:12:29.258 --> 01:12:30.885
&lt;i&gt;in our category of CASP.&lt;/i&gt;

01:12:32.178 --> 01:12:33.179
&lt;i&gt;So we're just making sure&lt;/i&gt;

01:12:33.262 --> 01:12:35.056
&lt;i&gt;we get&lt;/i&gt;
&lt;i&gt;the best possible answer.&lt;/i&gt;

01:12:40.395 --> 01:12:42.188
MOULT: &lt;i&gt;As soon as we started&lt;/i&gt;
&lt;i&gt;to get the results,&lt;/i&gt;

01:12:43.022 --> 01:12:47.985
&lt;i&gt;I'd sit down and start looking&lt;/i&gt;
&lt;i&gt;at how close did anybody come&lt;/i&gt;

01:12:48.069 --> 01:12:50.154
&lt;i&gt;to getting the protein&lt;/i&gt;
&lt;i&gt;structures correct.&lt;/i&gt;

01:12:54.450 --> 01:12:56.327
(ROBOT SQUEAKING)

01:12:58.955 --> 01:12:59.997
(INCOMING CALL BEEPING)

01:13:00.081 --> 01:13:01.249
-Oh, hi there.
-MAN: &lt;i&gt;Hello.&lt;/i&gt;

01:13:01.332 --> 01:13:03.042
(ALL CHUCKLING)

01:13:03.626 --> 01:13:06.671
It is an unbelievable thing,
CASP has finally ended.

01:13:06.754 --> 01:13:09.173
I think it's at least time
to raise a glass.

01:13:09.257 --> 01:13:10.842
Um, I don't know
if everyone has a glass

01:13:10.925 --> 01:13:12.385
of something
that they can raise.

01:13:12.468 --> 01:13:14.512
If not, raise,
I don't know, your laptops.

01:13:14.595 --> 01:13:16.222
-Um...
-(LAUGHTER)

01:13:16.764 --> 01:13:18.141
I'll probably make a speech
in a minute.

01:13:18.224 --> 01:13:20.059
I feel like I should but I
just have no idea what to say.

01:13:20.768 --> 01:13:23.271
So... let's see.

01:13:24.063 --> 01:13:26.232
I feel like a reading of email

01:13:26.899 --> 01:13:28.026
is the right thing to do.

01:13:28.109 --> 01:13:29.444
(ALL CHUCKLING)

01:13:29.527 --> 01:13:30.570
TUNYASUVUNAKOOL:
&lt;i&gt;When John said,&lt;/i&gt;

01:13:30.653 --> 01:13:32.780
&lt;i&gt;"I'm gonna read an email,"&lt;/i&gt;
&lt;i&gt;at a team social,&lt;/i&gt;

01:13:32.864 --> 01:13:35.241
I thought, "Wow, John,
you know how to have fun."

01:13:35.324 --> 01:13:37.452
We're gonna read an email now.
(LAUGHS)

01:13:38.119 --> 01:13:41.205
Uh, I got this
about 4:00 today.

01:13:42.248 --> 01:13:44.334
Um, it is from John Moult.

01:13:45.251 --> 01:13:46.586
&lt;i&gt;And I'll just read it.&lt;/i&gt;

01:13:46.669 --> 01:13:48.296
&lt;i&gt;It says,&lt;/i&gt;
&lt;i&gt;"As I expect you know,&lt;/i&gt;

01:13:49.088 --> 01:13:52.592
&lt;i&gt;"your group has performed&lt;/i&gt;
&lt;i&gt;amazingly well in CASP 14,&lt;/i&gt;

01:13:53.176 --> 01:13:55.053
&lt;i&gt;"both relative to other groups&lt;/i&gt;

01:13:55.136 --> 01:13:57.513
&lt;i&gt;"and in absolute&lt;/i&gt;
&lt;i&gt;model accuracy."&lt;/i&gt;

01:13:57.597 --> 01:13:59.307
(PEOPLE CLAPPING)

01:13:59.390 --> 01:14:00.808
&lt;i&gt;"Congratulations on this work.&lt;/i&gt;

01:14:00.892 --> 01:14:02.268
&lt;i&gt;"It is really outstanding."&lt;/i&gt;

01:14:02.935 --> 01:14:04.937
The structures were so good,

01:14:05.021 --> 01:14:07.148
it was... it was just amazing.

01:14:07.231 --> 01:14:08.649
(TRIUMPHANT INSTRUMENTAL
MUSIC PLAYING)

01:14:08.816 --> 01:14:10.276
&lt;i&gt;After half a century,&lt;/i&gt;

01:14:10.360 --> 01:14:11.861
&lt;i&gt;we finally have a solution&lt;/i&gt;

01:14:11.944 --> 01:14:13.696
&lt;i&gt;to the protein folding&lt;/i&gt;
&lt;i&gt;problem.&lt;/i&gt;

01:14:14.572 --> 01:14:16.866
When I saw this email,
I read it,

01:14:16.949 --> 01:14:18.576
I go, "Oh, shit!"

01:14:19.410 --> 01:14:21.287
And my wife goes,
"Is everything okay?"

01:14:21.371 --> 01:14:23.873
I call my parents, and just,
like, "Hey, Mum.

01:14:23.956 --> 01:14:25.917
"Um, got something
to tell you.

01:14:26.000 --> 01:14:27.210
"We've done this thing

01:14:27.293 --> 01:14:29.337
"and it might be kind of
a big deal." (LAUGHS)

01:14:29.420 --> 01:14:31.214
When I learned of
the CASP 14 results,

01:14:32.173 --> 01:14:33.675
I was gobsmacked.

01:14:33.758 --> 01:14:35.009
I was just excited.

01:14:35.635 --> 01:14:38.429
&lt;i&gt;This is a problem&lt;/i&gt;
&lt;i&gt;that I was beginning to think&lt;/i&gt;

01:14:38.513 --> 01:14:40.640
&lt;i&gt;would not get solved&lt;/i&gt;
&lt;i&gt;in my lifetime.&lt;/i&gt;

01:14:41.766 --> 01:14:44.268
NURSE: &lt;i&gt;Now we have a tool&lt;/i&gt;
&lt;i&gt;that can be used&lt;/i&gt;

01:14:44.352 --> 01:14:46.354
practically by scientists.

01:14:46.437 --> 01:14:48.106
SENIOR: These people
are asking us, you know,

01:14:48.189 --> 01:14:49.816
"I've got this protein
involved in malaria,"

01:14:49.899 --> 01:14:51.734
or, you know,
some infectious disease.

01:14:51.818 --> 01:14:52.985
"We don't know the structure.

01:14:53.069 --> 01:14:55.071
"Can we use AlphaFold
to solve it?"

01:14:55.154 --> 01:14:56.572
JUMPER: We can easily predict
all known sequences

01:14:56.656 --> 01:14:57.865
in a month.

01:14:57.949 --> 01:14:59.701
All known sequences
in a month?

01:14:59.784 --> 01:15:00.910
-Yeah, easily.
-Mmm-hmm?

01:15:00.993 --> 01:15:02.036
JUMPER:
A billion, two billion.

01:15:02.120 --> 01:15:03.329
Um, and they're...

01:15:03.413 --> 01:15:04.789
So why don't we just do that?
Yeah.

01:15:04.872 --> 01:15:06.708
-We should just do that a lot.
-Well, I mean...

01:15:06.791 --> 01:15:08.835
That's way better.
Why don't we just do that?

01:15:08.918 --> 01:15:10.712
SENIOR: So that's
one of the options.

01:15:10.795 --> 01:15:12.338
-HASSABIS: Right.
-There's this...

01:15:12.422 --> 01:15:14.674
We should just...
Right, that's a great idea.

01:15:14.757 --> 01:15:17.010
We should just run
every protein in existence.

01:15:18.094 --> 01:15:19.220
And then release that.

01:15:19.303 --> 01:15:20.680
Why didn't someone
suggest this before?

01:15:20.763 --> 01:15:21.848
Of course that's
what we should do.

01:15:21.931 --> 01:15:23.516
Why are we thinking about
making a service

01:15:23.599 --> 01:15:25.143
and then people submit
their protein?

01:15:25.226 --> 01:15:26.436
We just fold everything.

01:15:26.519 --> 01:15:28.479
&lt;i&gt;And then give it to&lt;/i&gt;
&lt;i&gt;everyone in the world.&lt;/i&gt;

01:15:28.563 --> 01:15:30.857
&lt;i&gt;Who knows how many discoveries&lt;/i&gt;
&lt;i&gt;will be made from that?&lt;/i&gt;

01:15:31.232 --> 01:15:33.276
BIRNEY: &lt;i&gt;Demis called us up&lt;/i&gt;
&lt;i&gt;and said,&lt;/i&gt;

01:15:33.359 --> 01:15:35.319
&lt;i&gt;"We want to make this open.&lt;/i&gt;

01:15:35.403 --> 01:15:37.530
"Not just make sure
the code is open,

01:15:37.613 --> 01:15:39.240
"but we're gonna make it
really easy

01:15:39.323 --> 01:15:42.243
&lt;i&gt;"for everybody to get access&lt;/i&gt;
&lt;i&gt;to the predictions."&lt;/i&gt;

01:15:44.662 --> 01:15:46.539
THORNTON: &lt;i&gt;That is fantastic.&lt;/i&gt;

01:15:46.914 --> 01:15:48.458
&lt;i&gt;It's like drawing back&lt;/i&gt;
&lt;i&gt;the curtain&lt;/i&gt;

01:15:49.042 --> 01:15:52.378
&lt;i&gt;and seeing the whole world&lt;/i&gt;
&lt;i&gt;of protein structures.&lt;/i&gt;

01:15:52.462 --> 01:15:54.797
(ETHEREAL MUSIC PLAYING)

01:15:54.881 --> 01:15:56.674
SCHMIDT:
&lt;i&gt;They released the structures&lt;/i&gt;

01:15:56.758 --> 01:15:58.509
&lt;i&gt;of 200 million proteins.&lt;/i&gt;

01:15:59.344 --> 01:16:01.512
&lt;i&gt;These are gifts to humanity.&lt;/i&gt;

01:16:07.352 --> 01:16:09.937
JUMPER: &lt;i&gt;The moment AlphaFold&lt;/i&gt;
&lt;i&gt;is live to the world,&lt;/i&gt;

01:16:10.772 --> 01:16:13.399
&lt;i&gt;we will no longer be&lt;/i&gt;
&lt;i&gt;the most important people&lt;/i&gt;

01:16:13.483 --> 01:16:14.484
&lt;i&gt;in AlphaFold's story.&lt;/i&gt;

01:16:14.567 --> 01:16:16.319
HASSABIS: &lt;i&gt;Can't quite believe&lt;/i&gt;
&lt;i&gt;it's all out.&lt;/i&gt;

01:16:16.402 --> 01:16:17.945
PEOPLE: &lt;i&gt;Aw!&lt;/i&gt;

01:16:18.029 --> 01:16:19.906
WOMAN: &lt;i&gt;A hundred&lt;/i&gt;
&lt;i&gt;and sixty-four users.&lt;/i&gt;

01:16:19.989 --> 01:16:21.407
HASSABIS:
&lt;i&gt;Loads of activity in Japan.&lt;/i&gt;

01:16:22.325 --> 01:16:24.660
RESEARCHER 1:
&lt;i&gt;We have 655 users currently.&lt;/i&gt;

01:16:24.744 --> 01:16:26.746
RESEARCHER 2: &lt;i&gt;We currently&lt;/i&gt;
&lt;i&gt;have 100,000 concurrent users.&lt;/i&gt;

01:16:26.829 --> 01:16:27.872
&lt;i&gt;Wow!&lt;/i&gt;

01:16:30.792 --> 01:16:32.543
&lt;i&gt;Today is just crazy.&lt;/i&gt;

01:16:33.795 --> 01:16:36.047
HASSABIS: &lt;i&gt;What an absolutely&lt;/i&gt;
&lt;i&gt;unbelievable effort&lt;/i&gt;

01:16:36.130 --> 01:16:37.173
&lt;i&gt;from everyone.&lt;/i&gt;

01:16:37.256 --> 01:16:38.257
&lt;i&gt;We're gonna all remember&lt;/i&gt;
&lt;i&gt;these moments&lt;/i&gt;

01:16:38.341 --> 01:16:39.550
&lt;i&gt;for the rest of our lives.&lt;/i&gt;

01:16:39.634 --> 01:16:41.594
I'm excited about AlphaFold.

01:16:41.678 --> 01:16:44.806
For my research, it's already
propelling lots of progress.

01:16:45.348 --> 01:16:47.058
&lt;i&gt;And this is&lt;/i&gt;
&lt;i&gt;just the beginning.&lt;/i&gt;

01:16:47.141 --> 01:16:48.643
SCHMIDT: &lt;i&gt;My guess is,&lt;/i&gt;

01:16:48.726 --> 01:16:53.189
&lt;i&gt;every single biological&lt;/i&gt;
&lt;i&gt;and chemistry achievement&lt;/i&gt;

01:16:53.272 --> 01:16:55.400
will be related to AlphaFold
in some way.

01:16:55.483 --> 01:16:57.485
(TRIUMPHANT INSTRUMENTAL
MUSIC PLAYING)

01:17:13.001 --> 01:17:15.294
&lt;i&gt;AlphaFold is an index moment.&lt;/i&gt;

01:17:15.378 --> 01:17:17.588
&lt;i&gt;It's a moment&lt;/i&gt;
&lt;i&gt;that people will not forget&lt;/i&gt;

01:17:17.672 --> 01:17:19.841
&lt;i&gt;because the world changed.&lt;/i&gt;

01:17:39.402 --> 01:17:41.112
HASSABIS:
&lt;i&gt;Everybody's realized now&lt;/i&gt;

01:17:41.195 --> 01:17:43.406
&lt;i&gt;what Shane and I have known&lt;/i&gt;
&lt;i&gt;for more than 20 years,&lt;/i&gt;

01:17:43.489 --> 01:17:46.325
&lt;i&gt;that AI is going to be&lt;/i&gt;
&lt;i&gt;the most important thing&lt;/i&gt;

01:17:46.409 --> 01:17:48.036
&lt;i&gt;humanity's ever gonna invent.&lt;/i&gt;

01:17:48.119 --> 01:17:49.787
TRAIN ANNOUNCER:
&lt;i&gt;We will shortly be arriving&lt;/i&gt;

01:17:49.871 --> 01:17:51.581
&lt;i&gt;at our final destination.&lt;/i&gt;

01:17:51.664 --> 01:17:53.082
(ELECTRONIC MUSIC PLAYING)

01:18:02.133 --> 01:18:04.260
HASSABIS: &lt;i&gt;The pace of&lt;/i&gt;
&lt;i&gt;innovation and capabilities&lt;/i&gt;

01:18:04.344 --> 01:18:05.386
&lt;i&gt;is accelerating,&lt;/i&gt;

01:18:06.220 --> 01:18:09.015
&lt;i&gt;like a boulder rolling down&lt;/i&gt;
&lt;i&gt;a hill that we've kicked off&lt;/i&gt;

01:18:09.098 --> 01:18:10.975
&lt;i&gt;and now it's continuing&lt;/i&gt;
&lt;i&gt;to gather speed.&lt;/i&gt;

01:18:12.352 --> 01:18:15.146
NEWSCASTER: &lt;i&gt;We are at&lt;/i&gt;
&lt;i&gt;a crossroads in human history.&lt;/i&gt;

01:18:15.229 --> 01:18:16.522
&lt;i&gt;AI has the potential&lt;/i&gt;

01:18:16.606 --> 01:18:18.483
&lt;i&gt;to transform our lives&lt;/i&gt;
&lt;i&gt;in every aspect.&lt;/i&gt;

01:18:19.025 --> 01:18:22.070
&lt;i&gt;It's no less important than&lt;/i&gt;
&lt;i&gt;the discovery of electricity.&lt;/i&gt;

01:18:23.529 --> 01:18:26.115
HASSABIS: &lt;i&gt;We should be looking&lt;/i&gt;
&lt;i&gt;at the scientific method&lt;/i&gt;

01:18:26.199 --> 01:18:28.326
&lt;i&gt;and trying to understand&lt;/i&gt;
&lt;i&gt;each step of the way&lt;/i&gt;

01:18:28.409 --> 01:18:29.410
&lt;i&gt;in a rigorous way.&lt;/i&gt;

01:18:29.494 --> 01:18:31.746
&lt;i&gt;This is a moment&lt;/i&gt;
&lt;i&gt;of profound opportunity.&lt;/i&gt;

01:18:32.580 --> 01:18:34.415
SUNAK:
&lt;i&gt;Harnessing this technology&lt;/i&gt;

01:18:34.499 --> 01:18:37.210
&lt;i&gt;could eclipse anything&lt;/i&gt;
&lt;i&gt;we have ever known.&lt;/i&gt;

01:18:40.213 --> 01:18:41.714
(ELECTRONIC DEVICE BEEPS)

01:18:41.798 --> 01:18:43.341
HASSABIS: Hi, Alpha.

01:18:44.175 --> 01:18:45.593
ALPHA: &lt;i&gt;Hi.&lt;/i&gt;

01:18:46.719 --> 01:18:48.179
What is this?

01:18:50.390 --> 01:18:51.808
ALPHA: &lt;i&gt;This is a chessboard.&lt;/i&gt;

01:18:53.476 --> 01:18:55.978
If I was to play white, what
move would you recommend?

01:18:59.357 --> 01:19:00.358
ALPHA: &lt;i&gt;I would recommend&lt;/i&gt;

01:19:00.441 --> 01:19:02.318
&lt;i&gt;that you move your pawn&lt;/i&gt;
&lt;i&gt;from E2 to E4.&lt;/i&gt;

01:19:05.571 --> 01:19:08.199
And now if you were black,
what would you play now?

01:19:11.244 --> 01:19:13.079
ALPHA: &lt;i&gt;I would play&lt;/i&gt;
&lt;i&gt;the Sicilian Defense.&lt;/i&gt;

01:19:15.331 --> 01:19:16.958
Good choice.

01:19:19.002 --> 01:19:20.878
-ALPHA: &lt;i&gt;Thanks.&lt;/i&gt;
-(CHUCKLES)

01:19:23.381 --> 01:19:25.466
So what do you see?
What is this object?

01:19:28.177 --> 01:19:29.929
ALPHA:
&lt;i&gt;This is a pencil sculpture.&lt;/i&gt;

01:19:32.515 --> 01:19:34.642
What happens if I move
one of the pencils?

01:19:37.770 --> 01:19:39.188
ALPHA: &lt;i&gt;If you move&lt;/i&gt;
&lt;i&gt;one of the pencils,&lt;/i&gt;

01:19:39.272 --> 01:19:40.857
&lt;i&gt;the sculpture will fall apart.&lt;/i&gt;

01:19:41.691 --> 01:19:43.276
I'd better leave it alone,
then.

01:19:44.027 --> 01:19:46.863
&lt;i&gt;-That's probably a good idea.&lt;/i&gt;
-(HASSABIS CHUCKLES)

01:19:50.199 --> 01:19:52.201
HASSABIS:
&lt;i&gt;AGI is on the horizon now.&lt;/i&gt;

01:19:54.579 --> 01:19:56.289
&lt;i&gt;Very clearly&lt;/i&gt;
&lt;i&gt;the next generation&lt;/i&gt;

01:19:56.372 --> 01:19:58.041
&lt;i&gt;is going to live&lt;/i&gt;
&lt;i&gt;in a future world&lt;/i&gt;

01:19:58.124 --> 01:20:00.668
&lt;i&gt;where things will be radically&lt;/i&gt;
&lt;i&gt;different because of AI.&lt;/i&gt;

01:20:02.128 --> 01:20:05.131
&lt;i&gt;And if you want to steward&lt;/i&gt;
&lt;i&gt;that responsibly,&lt;/i&gt;

01:20:05.882 --> 01:20:07.925
&lt;i&gt;every moment is vital.&lt;/i&gt;

01:20:09.344 --> 01:20:11.971
&lt;i&gt;This is the moment I've been&lt;/i&gt;
&lt;i&gt;living my whole life for.&lt;/i&gt;

01:20:19.020 --> 01:20:20.813
It's just
a good thinking game.

01:20:22.106 --> 01:20:24.067
(UPLIFTING INSTRUMENTAL
MUSIC PLAYING)

